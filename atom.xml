<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>红色石头的机器学习之路</title>
  
  <subtitle>公众号ID：redstonewill</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://redstonewill.github.io/"/>
  <updated>2018-03-16T14:47:24.055Z</updated>
  <id>https://redstonewill.github.io/</id>
  
  <author>
    <name>红色石头</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记5 -- Training versus Testing</title>
    <link href="https://redstonewill.github.io/2018/03/16/5/"/>
    <id>https://redstonewill.github.io/2018/03/16/5/</id>
    <published>2018-03-16T14:45:12.000Z</published>
    <updated>2018-03-16T14:47:24.055Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170613075450559?imageView/2/w/400/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要介绍了机器学习的可行性。首先，由NFL定理可知，机器学习貌似是不可行的。但是，随后引入了统计学知识，如果样本数据足够大，且hypothesis个数有限，那么机器学习一般就是可行的。本节课将讨论机器学习的核心问题，严格证明为什么机器可以学习。从上节课最后的问题出发，即当hypothesis的个数是无限多的时候，机器学习的可行性是否仍然成立？</p><h3 id="Recap-and-Preview"><a href="#Recap-and-Preview" class="headerlink" title="Recap and Preview"></a>Recap and Preview</h3><p>我们先来看一下基于统计学的机器学习流程图：</p><p><img src="http://img.blog.csdn.net/20170613075450559?" alt="这里写图片描述"></p><p>该流程图中，训练样本D和最终测试h的样本都是来自同一个数据分布，这是机器能够学习的前提。另外，训练样本D应该足够大，且hypothesis set的个数是有限的，这样根据霍夫丁不等式，才不会出现Bad Data，保证$E_{in}\approx E_{out}$，即有很好的泛化能力。同时，通过训练，得到使$E_{in}$最小的h，作为模型最终的矩g，g接近于目标函数。</p><p>这里，我们总结一下前四节课的主要内容：第一节课，我们介绍了机器学习的定义，目标是找出最好的矩g，使$g\approx f$，保证$E_{out}(g)\approx 0$；第二节课，我们介绍了如何让$E_{in}\approx 0$，可以使用PLA、pocket等演算法来实现；第三节课，我们介绍了机器学习的分类，我们的训练样本是批量数据（batch），处理监督式（supervised）二元分类（binary classification）问题；第四节课，我们介绍了机器学习的可行性，通过统计学知识，把$E_{in}(g)$与$E_{out}(g)$联系起来，证明了在一些条件假设下，$E_{in}(g)\approx E_{out}(g)$成立。</p><p><img src="http://img.blog.csdn.net/20170613081415084?" alt="这里写图片描述"></p><p>这四节课总结下来，我们把机器学习的主要目标分成两个核心的问题：</p><ul><li><p>$E_{in}(g)\approx E_{out}(g)$</p></li><li><p>$E_{in}(g)$足够小</p></li></ul><p>上节课介绍的机器学习可行的一个条件是hypothesis set的个数M是有限的，那M跟上面这两个核心问题有什么联系呢？</p><p>我们先来看一下，当M很小的时候，由上节课介绍的霍夫丁不等式，得到$E_{in}(g)\approx E_{out}(g)$，即能保证第一个核心问题成立。但M很小时，演算法A可以选择的hypothesis有限，不一定能找到使$E_{in}(g)$足够小的hypothesis，即不能保证第二个核心问题成立。当M很大的时候，同样由霍夫丁不等式，$E_{in}(g)$与$E_{out}(g)$的差距可能比较大，第一个核心问题可能不成立。而M很大，使的演算法A的可以选择的hypothesis就很多，很有可能找到一个hypothesis，使$E_{in}(g)$足够小，第二个核心问题可能成立。</p><p><img src="http://img.blog.csdn.net/20170613083237643?" alt="这里写图片描述"></p><p>从上面的分析来看，M的选择直接影响机器学习两个核心问题是否满足，M不能太大也不能太小。那么如果M无限大的时候，是否机器就不可以学习了呢？例如PLA算法中直线是无数条的，但是PLA能够很好地进行机器学习，这又是为什么呢？如果我们能将无限大的M限定在一个有限的$m_H$内，问题似乎就解决了。</p><h3 id="Effective-Number-of-Line"><a href="#Effective-Number-of-Line" class="headerlink" title="Effective Number of Line"></a>Effective Number of Line</h3><p>我们先看一下上节课推导的霍夫丁不等式：</p><p>$$P[|E_{in}(g)-E_{out}(g)|&gt;\epsilon]\leq 2\cdot M\cdot exp(-2\epsilon^2N)$$</p><p>其中，M表示hypothesis的个数。每个hypothesis下的BAD events $B_m$级联的形式满足下列不等式：</p><p>$$P[B_1\ or\ B_2\ or\ \cdots B_M]\leq P[B_1]+P[B_2]+\cdots+P[B_M]$$</p><p>当$M=\infty$时，上面不等式右边值将会很大，似乎说明BAD events很大，$E_{in}(g)$与$E_{out}(g)$也并不接近。但是BAD events $B_m$级联的形式实际上是扩大了上界，union bound过大。这种做法假设各个hypothesis之间没有交集，这是最坏的情况，可是实际上往往不是如此，很多情况下，都是有交集的，也就是说M实际上没那么大，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170613092500157?" alt="这里写图片描述"></p><p>也就是说union bound被估计过高了（over-estimating）。所以，我们的目的是找出不同BAD events之间的重叠部分，也就是将无数个hypothesis分成有限个类别。</p><p>如何将无数个hypothesis分成有限类呢？我们先来看这样一个例子，假如平面上用直线将点分开，也就跟PLA一样。如果平面上只有一个点x1，那么直线的种类有两种：一种将x1划为+1，一种将x1划为-1：</p><p><img src="http://img.blog.csdn.net/20170613095709542?" alt="这里写图片描述"></p><p>如果平面上有两个点x1、x2，那么直线的种类共4种：x1、x2都为+1，x1、x2都为-1，x1为+1且x2为-1，x1为-1且x2为+1：</p><p><img src="http://img.blog.csdn.net/20170613103010274?" alt="这里写图片描述"></p><p>如果平面上有三个点x1、x2、x3，那么直线的种类共8种：</p><p><img src="http://img.blog.csdn.net/20170613103032665?" alt="这里写图片描述"></p><p>但是，在三个点的情况下，也会出现不能用一条直线划分的情况：</p><p><img src="http://img.blog.csdn.net/20170613102051356?" alt="这里写图片描述"></p><p>也就是说，对于平面上三个点，不能保证所有的8个类别都能被一条直线划分。那如果是四个点x1、x2、x3、x4，我们发现，平面上找不到一条直线能将四个点组成的16个类别完全分开，最多只能分开其中的14类，即直线最多只有14种：</p><p><img src="http://img.blog.csdn.net/20170613102539221?" alt="这里写图片描述"></p><p>经过分析，我们得到平面上线的种类是有限的，1个点最多有2种线，2个点最多有4种线，3个点最多有8种线，4个点最多有14（$&lt;2^4$）种线等等。我们发现，有效直线的数量总是满足$\leq 2^N$，其中，N是点的个数。所以，如果我们可以用effective(N)代替M，霍夫丁不等式可以写成：</p><p>$$P[|E_{in}(g)-E_{out}(g)|&gt;\epsilon]\leq 2\cdot effective(N)\cdot exp(-2\epsilon^2N)$$</p><p>已知effective(N)&lt;$2^N$，如果能够保证effective(N)&lt;&lt;$2^N$，即不等式右边接近于零，那么即使M无限大，直线的种类也很有限，机器学习也是可能的。</p><p><img src="http://img.blog.csdn.net/20170613110232379?" alt="这里写图片描述"></p><h3 id="Effective-Number-of-Hypotheses"><a href="#Effective-Number-of-Hypotheses" class="headerlink" title="Effective Number of Hypotheses"></a>Effective Number of Hypotheses</h3><p>接下来先介绍一个新名词：二分类（dichotomy）。dichotomy就是将空间中的点（例如二维平面）用一条直线分成正类（蓝色o）和负类（红色x）。令H是将平面上的点用直线分开的所有hypothesis h的集合，dichotomy H与hypotheses H的关系是：hypotheses H是平面上所有直线的集合，个数可能是无限个，而dichotomy H是平面上能将点完全用直线分开的直线种类，它的上界是$2^N$。接下来，我们要做的就是尝试用dichotomy代替M。</p><p><img src="http://img.blog.csdn.net/20170613112843268?" alt="这里写图片描述"></p><p>再介绍一个新的名词：成长函数（growth function），记为$m_H(H)$。成长函数的定义是：对于由N个点组成的不同集合中，某集合对应的dichotomy最大，那么这个dichotomy值就是$m_H(H)$，它的上界是$2^N$：</p><p><img src="http://img.blog.csdn.net/20170613113650318?" alt="这里写图片描述"></p><p>成长函数其实就是我们之前讲的effective lines的数量最大值。根据成长函数的定义，二维平面上，$m_H(H)$随N的变化关系是：</p><p><img src="http://img.blog.csdn.net/20170613113930381?" alt="这里写图片描述"></p><p>接下来，我们讨论如何计算成长函数。先看一个简单情况，一维的Positive Rays：</p><p><img src="http://img.blog.csdn.net/20170613115111665?" alt="这里写图片描述"></p><p>若有N个点，则整个区域可分为N+1段，很容易得到其成长函数$m_H(N)=N+1$。注意当N很大时，$(N+1)&lt;&lt;2^N$，这是我们希望看到的。</p><p>另一种情况是一维的Positive Intervals：</p><p><img src="http://img.blog.csdn.net/20170613133644825?" alt="这里写图片描述"></p><p>它的成长函数可以由下面推导得出：</p><p><img src="http://img.blog.csdn.net/20170613134246055?" alt="这里写图片描述"></p><p>这种情况下，$m_H(N)=\frac12N^2+\frac12N+1&lt;&lt;2^N$，在N很大的时候，仍然是满足的。</p><p>再来看这个例子，假设在二维空间里，如果hypothesis是凸多边形或类圆构成的封闭曲线，如下图所示，左边是convex的，右边不是convex的。那么，它的成长函数是多少呢？</p><p><img src="http://img.blog.csdn.net/20170613135200251?" alt="这里写图片描述"></p><p>当数据集D按照如下的凸分布时，我们很容易计算得到它的成长函数$m_H=2^N$。这种情况下，N个点所有可能的分类情况都能够被hypotheses set覆盖，我们把这种情形称为shattered。也就是说，如果能够找到一个数据分布集，hypotheses set对N个输入所有的分类情况都做得到，那么它的成长函数就是$2^N$。</p><p><img src="http://img.blog.csdn.net/20170613135918235?" alt="这里写图片描述"></p><h3 id="Break-Point"><a href="#Break-Point" class="headerlink" title="Break Point"></a>Break Point</h3><p>上一小节，我们介绍了四种不同的成长函数，分别是：</p><p><img src="http://img.blog.csdn.net/20170613140725974?" alt="这里写图片描述"></p><p>其中，positive rays和positive intervals的成长函数都是polynomial的，如果用$m_H$代替M的话，这两种情况是比较好的。而convex sets的成长函数是exponential的，即等于M，并不能保证机器学习的可行性。那么，对于2D perceptrons，它的成长函数究竟是polynomial的还是exponential的呢？</p><p>对于2D perceptrons，我们之前分析了3个点，可以做出8种所有的dichotomy，而4个点，就无法做出所有16个点的dichotomy了。所以，我们就把4称为2D perceptrons的break point（5、6、7等都是break point）。令有k个点，如果k大于等于break point时，它的成长函数一定小于2的k次方。</p><p>根据break point的定义，我们知道满足$m_H(k)\neq 2^k$的k的最小值就是break point。对于我们之前介绍的四种成长函数，他们的break point分别是：</p><p><img src="http://img.blog.csdn.net/20170613143127195?" alt="这里写图片描述"></p><p>通过观察，我们猜测成长函数可能与break point存在某种关系：对于convex sets，没有break point，它的成长函数是2的N次方；对于positive rays，break point k=2，它的成长函数是O(N)；对于positive intervals，break point k=3，它的成长函数是$O(N^2)$。则根据这种推论，我们猜测2D perceptrons，它的成长函数$m_H(N)=O(N^{k-1})$ 。如果成立，那么就可以用$m_H$代替M，就满足了机器能够学习的条件。关于上述猜测的证明，我们下节课再详细介绍。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课，我们更深入地探讨了机器学习的可行性。我们把机器学习拆分为两个核心问题：$E_{in}(g)\approx E_{out}(g)$和$E_{in}(g)\approx 0$。对于第一个问题，我们探讨了M个hypothesis到底可以划分为多少种，也就是成长函数$m_H$。并引入了break point的概念，给出了break point的计算方法。下节课，我们将详细论证对于2D perceptrons，它的成长函数与break point是否存在多项式的关系，如果是这样，那么机器学习就是可行的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170613075450559?imageView/2/w/400/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记4 -- Feasibility of Learning</title>
    <link href="https://redstonewill.github.io/2018/03/16/4/"/>
    <id>https://redstonewill.github.io/2018/03/16/4/</id>
    <published>2018-03-16T14:30:42.000Z</published>
    <updated>2018-03-16T14:34:55.760Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170612080753910?imageView/2/w/400/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要介绍了根据不同的设定，机器学习可以分为不同的类型。其中，监督式学习中的二元分类和回归分析是最常见的也是最重要的机器学习问题。本节课，我们将介绍机器学习的可行性，讨论问题是否可以使用机器学习来解决。</p><h3 id="Learning-is-Impossible"><a href="#Learning-is-Impossible" class="headerlink" title="Learning is Impossible"></a>Learning is Impossible</h3><p>首先，考虑这样一个例子，如下图所示，有3个label为-1的九宫格和3个label为+1的九宫格。根据这6个样本，提取相应label下的特征，预测右边九宫格是属于-1还是+1？结果是，如果依据对称性，我们会把它归为+1；如果依据九宫格左上角是否是黑色，我们会把它归为-1。除此之外，还有根据其它不同特征进行分类，得到不同结果的情况。而且，这些分类结果貌似都是正确合理的，因为对于6个训练样本来说，我们选择的模型都有很好的分类效果。</p><p><img src="http://img.blog.csdn.net/20170612080753910?" alt="这里写图片描述"></p><p>再来看一个比较数学化的二分类例子，输入特征x是二进制的、三维的，对应有8种输入，其中训练样本D有5个。那么，根据训练样本对应的输出y，假设有8个hypothesis，这8个hypothesis在D上，对5个训练样本的分类效果效果都完全正确。但是在另外3个测试数据上，不同的hypothesis表现有好有坏。在已知数据D上，$g\approx f$；但是在D以外的未知数据上，$g\approx f$不一定成立。而机器学习目的，恰恰是希望我们选择的模型能在未知数据上的预测与真实结果是一致的，而不是在已知的数据集D上寻求最佳效果。</p><p><img src="http://img.blog.csdn.net/20170612083115583?" alt="这里写图片描述"></p><p>这个例子告诉我们，我们想要在D以外的数据中更接近目标函数似乎是做不到的，只能保证对D有很好的分类结果。机器学习的这种特性被称为没有免费午餐（No Free Lunch）定理。NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。从这个例子来看，NFL说明了无法保证一个机器学习算法在D以外的数据集上一定能分类或预测正确，除非加上一些假设条件，我们以后会介绍。</p><h3 id="Probability-to-the-Rescue"><a href="#Probability-to-the-Rescue" class="headerlink" title="Probability to the Rescue"></a>Probability to the Rescue</h3><p>从上一节得出的结论是：在训练集D以外的样本上，机器学习的模型是很难，似乎做不到正确预测或分类的。那是否有一些工具或者方法能够对未知的目标函数f做一些推论，让我们的机器学习模型能够变得有用呢？</p><p>如果有一个装有很多（数量很大数不过来）橙色球和绿色球的罐子，我们能不能推断橙色球的比例u？统计学上的做法是，从罐子中随机取出N个球，作为样本，计算这N个球中橙色球的比例v，那么就估计出罐子中橙色球的比例约为v。</p><p><img src="http://img.blog.csdn.net/20170612094631233?" alt="这里写图片描述"></p><p>这种随机抽取的做法能否说明罐子里橙色球的比例一定是v呢？答案是否定的。但是从概率的角度来说，样本中的v很有可能接近我们未知的u。下面从数学推导的角度来看v与u是否相近。</p><p>已知u是罐子里橙色球的比例，v是N个抽取的样本中橙色球的比例。当N足够大的时候，v接近于u。这就是Hoeffding’s inequality：</p><p>$$P[|v-u|&gt;\epsilon]\leq 2exp(-2\epsilon^2N)$$</p><p>Hoeffding不等式说明当N很大的时候，v与u相差不会很大，它们之间的差值被限定在$\epsilon$之内。我们把结论v=u称为probably approximately correct(PAC)。</p><p><img src="http://img.blog.csdn.net/20170612100845352?" alt="这里写图片描述"></p><h3 id="Connection-to-Learning"><a href="#Connection-to-Learning" class="headerlink" title="Connection to Learning"></a>Connection to Learning</h3><p>下面，我们将罐子的内容对应到机器学习的概念上来。机器学习中hypothesis与目标函数相等的可能性，类比于罐子中橙色球的概率问题；罐子里的一颗颗弹珠类比于机器学习样本空间的x；橙色的弹珠类比于h(x)与f不相等；绿色的弹珠类比于h(x)与f相等；从罐子中抽取的N个球类比于机器学习的训练样本D，且这两种抽样的样本与总体样本之间都是独立同分布的。所以呢，如果样本N够大，且是独立同分布的，那么，从样本中$h(x)\neq f(x)$的概率就能推导在抽样样本外的所有样本中$h(x)\neq f(x)$的概率是多少。</p><p><img src="http://img.blog.csdn.net/20170612105733424?" alt="这里写图片描述"></p><p>映射中最关键的点是讲抽样中橙球的概率理解为样本数据集D上h(x)错误的概率，以此推算出在所有数据上h(x)错误的概率，这也是机器学习能够工作的本质，即我们为啥在采样数据上得到了一个假设，就可以推到全局呢？因为两者的错误率是PAC的，只要我们保证前者小，后者也就小了。</p><p> <img src="http://img.blog.csdn.net/20170612110350990?" alt="这里写图片描述"></p><p>这里我们引入两个值$E_{in}(h)$和$E_{out}(h)$。$E_{in}(h)$表示在抽样样本中，h(x)与$y_n$不相等的概率；$E_{out}(h)$表示实际所有样本中，h(x)与f(x)不相等的概率是多少。</p><p><img src="http://img.blog.csdn.net/20170612110744008?" alt="这里写图片描述"></p><p>同样，它的Hoeffding’s inequality可以表示为：</p><p>$$P[|E_{in}(h)-E_{out}(h)|&gt;\epsilon]\leq 2exp(-2\epsilon^2N)$$</p><p>该不等式表明，$E_{in}(h)=E_{out}(h)$也是PAC的。如果$E_{in}(h)\approx E_{out}(h)$，$E_{in}(h)$很小，那么就能推断出$E_{out}(h)$很小，也就是说在该数据分布P下，h与f非常接近，机器学习的模型比较准确。</p><p>一般地，h如果是固定的，N很大的时候，$E_{in}(h)\approx E_{out}(h)$，但是并不意味着$g\approx f$。因为h是固定的，不能保证$E_{in}(h)$足够小，即使$E_{in}(h)\approx E_{out}(h)$，也可能使$E_{out}(h)$偏大。所以，一般会通过演算法A，选择最好的h，使$E_{in}(h)$足够小，从而保证$E_{out}(h)$很小。固定的h，使用新数据进行测试，验证其错误率是多少。</p><p><img src="http://img.blog.csdn.net/20170612112902807?" alt="这里写图片描述"></p><h3 id="Connection-to-Real-Learning"><a href="#Connection-to-Real-Learning" class="headerlink" title="Connection to Real Learning"></a>Connection to Real Learning</h3><p><img src="http://img.blog.csdn.net/20170612135223361?" alt="这里写图片描述"></p><p>假设现在有很多罐子M个（即有M个hypothesis），如果其中某个罐子抽样的球全是绿色，那是不是应该选择这个罐子呢？我们先来看这样一个例子：150个人抛硬币，那么其中至少有一个人连续5次硬币都是正面朝上的概率是</p><p>$$1-(\frac{31}{32})^{150}&gt;99\%$$</p><p>可见这个概率是很大的，但是能否说明5次正面朝上的这个硬币具有代表性呢？答案是否定的！并不能说明该硬币单次正面朝上的概率很大，其实都是0.5。一样的道理，抽到全是绿色求的时候也不能一定说明那个罐子就全是绿色球。当罐子数目很多或者抛硬币的人数很多的时候，可能引发Bad Sample，Bad Sample就是$E_{in}$和$E_{out}$差别很大，即选择过多带来的负面影响，选择过多会恶化不好的情形。</p><p>根据许多次抽样的到的不同的数据集D，Hoeffding’s inequality保证了大多数的D都是比较好的情形（即对于某个h，保证$E_{in}\approx E_{out}$），但是也有可能出现Bad Data，即$E_{in}$和$E_{out}$差别很大的数据集D，这是小概率事件。</p><p><img src="http://img.blog.csdn.net/20170612140418003?" alt="这里写图片描述"></p><p>也就是说，不同的数据集$D_n$，对于不同的hypothesis，有可能成为Bad Data。只要$D_n$在某个hypothesis上是Bad Data，那么$D_n$就是Bad Data。只有当$D_n$在所有的hypothesis上都是好的数据，才说明$D_n$不是Bad Data，可以自由选择演算法A进行建模。那么，根据Hoeffding’s inequality，Bad Data的上界可以表示为连级（union bound）的形式：</p><p><img src="http://img.blog.csdn.net/20170612141520550?" alt="这里写图片描述"></p><p>其中，M是hypothesis的个数，N是样本D的数量，$\epsilon$是参数。该union bound表明，当M有限，且N足够大的时候，Bad Data出现的概率就更低了，即能保证D对于所有的h都有$E_{in}\approx E_{out}$，满足PAC，演算法A的选择不受限制。那么满足这种union bound的情况，我们就可以和之前一样，选取一个合理的演算法（PLA/pocket），选择使$E_{in}$最小的$h_m$作为矩g，一般能够保证$g\approx f$，即有不错的泛化能力。</p><p>所以，如果hypothesis的个数M是有限的，N足够大，那么通过演算法A任意选择一个矩g，都有$E_{in}\approx E_{out}$成立；同时，如果找到一个矩g，使$E_{in}\approx 0$，PAC就能保证$E_{out}\approx 0$。至此，就证明了机器学习是可行的。</p><p><img src="http://img.blog.csdn.net/20170612143915944?" alt="这里写图片描述"></p><p>但是，如上面的学习流程图右下角所示，如果M是无数个，例如之前介绍的PLA直线有无数条，是否这些推论就不成立了呢？是否机器就不能进行学习呢？这些内容和问题，我们下节课再介绍。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了机器学习的可行性。首先引入NFL定理，说明机器学习无法找到一个矩g能够完全和目标函数f一样。接着介绍了可以采用一些统计上的假设，例如Hoeffding不等式，建立$E_{in}$和$E_{out}$的联系，证明对于某个h，当N足够大的时候，$E_{in}$和$E_{out}$是PAC的。最后，对于h个数很多的情况，只要有h个数M是有限的，且N足够大，就能保证$E_{in}\approx E_{out}$，证明机器学习是可行的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170612080753910?imageView/2/w/400/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记3 -- Types of Learning</title>
    <link href="https://redstonewill.github.io/2018/03/16/3/"/>
    <id>https://redstonewill.github.io/2018/03/16/3/</id>
    <published>2018-03-16T13:52:20.000Z</published>
    <updated>2018-03-16T14:16:49.462Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170609080354964?imageView/2/w/400/q/100" alt="这里写图片描述"></p><a id="more"></a><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了解决线性分类问题的一个简单的方法：PLA。PLA能够在平面中选择一条直线将样本数据完全正确分类。而对于线性不可分的情况，可以使用Pocket Algorithm来处理。本节课将主要介绍一下机器学习有哪些种类，并进行归纳。</p><h3 id="Learning-with-Different-Output-Space-Y"><a href="#Learning-with-Different-Output-Space-Y" class="headerlink" title="Learning with Different Output Space Y"></a>Learning with Different Output Space Y</h3><p>我们在上节课引入的银行根据用户个人情况判断是否给他发信用卡的例子，这是一个典型的二元分类（binary classification）问题。也就是说输出只有两个，一般y={-1, +1}，-1代表不发信用卡（负类），+1代表发信用卡（正类）。</p><p>二元分类的问题很常见，包括信用卡发放、垃圾邮件判别、患者疾病诊断、答案正确性估计等等。二元分类是机器学习领域非常核心和基本的问题。二元分类有线性模型也有非线性模型，根据实际问题情况，选择不同的模型。</p><p><img src="http://img.blog.csdn.net/20170609080354964?" alt="这里写图片描述"></p><p>除了二元分类，也有多元分类（Multiclass Classification）问题。顾名思义，多元分类的输出多于两个，y={1, 2, … , K}, K&gt;2. 一般多元分类的应用有数字识别、图片内容识别等等。</p><p><img src="http://img.blog.csdn.net/20170609081211320?" alt="这里写图片描述"></p><p>二元分类和多元分类都属于分类问题，它们的输出都是离散值。二对于另外一种情况，比如训练模型，预测房屋价格、股票收益多少等，这类问题的输出y=R，即范围在整个实数空间，是连续的。这类问题，我们把它叫做回归（Regression）。最简单的线性回归是一种典型的回归模型。</p><p>除了分类和回归问题，在自然语言处理等领域中，还会用到一种机器学习问题：结构化学习（Structured Learning）。结构化学习的输出空间包含了某种结构在里面，它的一些解法通常是从多分类问题延伸而来的，比较复杂。本系列课程不会详细介绍Structured Learning，有兴趣的读者可以自行对它进行更深入的研究。</p><p>简单总结一下，机器学习按照输出空间划分的话，包括二元分类、多元分类、回归、结构化学习等不同的类型。其中二元分类和回归是最基础、最核心的两个类型，也是我们课程主要介绍的部分。</p><p><img src="http://img.blog.csdn.net/20170609083208763?" alt="这里写图片描述"></p><h3 id="Learning-with-Different-Data-Label-yn"><a href="#Learning-with-Different-Data-Label-yn" class="headerlink" title="Learning with Different Data Label yn"></a>Learning with Different Data Label yn</h3><p>如果我们拿到的训练样本D既有输入特征x，也有输出yn，那么我们把这种类型的学习称为监督式学习（Supervised Learning）。监督式学习可以是二元分类、多元分类或者是回归，最重要的是知道输出标签yn。与监督式学习相对立的另一种类型是非监督式学习（Unsupervised learning）。非监督式学习是没有输出标签yn的，典型的非监督式学习包括：聚类（clustering）问题，比如对网页上新闻的自动分类；密度估计，比如交通路况分析；异常检测，比如用户网络流量监测。通常情况下，非监督式学习更复杂一些，而且非监督的问题很多都可以使用监督式学习的一些算法思想来实现。</p><p><img src="http://img.blog.csdn.net/20170609084807897?" alt="这里写图片描述"></p><p>介于监督式和非监督式学习之间的叫做半监督式学习（Semi-supervised Learning）。顾名思义，半监督式学习就是说一部分数据有输出标签yn，而另一部分数据没有输出标签yn。在实际应用中，半监督式学习有时候是必须的，比如医药公司对某些药物进行检测，考虑到成本和实验人群限制等问题，只有一部分数据有输出标签yn。</p><p>监督式、非监督式、半监督式学习是机器学习领域三个主要类型。除此之外，还有一种非常重要的类型：增强学习（Reinforcement Learning）。增强学习中，我们给模型或系统一些输入，但是给不了我们希望的真实的输出y，根据模型的输出反馈，如果反馈结果良好，更接近真实输出，就给其正向激励，如果反馈结果不好，偏离真实输出，就给其反向激励。不断通过“反馈-修正”这种形式，一步一步让模型学习的更好，这就是增强学习的核心所在。增强学习可以类比成训练宠物的过程，比如我们要训练狗狗坐下，但是狗狗无法直接听懂我们的指令“sit down”。在训练过程中，我们给狗狗示意，如果它表现得好，我们就给他奖励，如果它做跟sit down完全无关的动作，我们就给它小小的惩罚。这样不断修正狗狗的动作，最终能让它按照我们的指令来行动。实际生活中，增强学习的例子也很多，比如根据用户点击、选择而不断改进的广告系统</p><p>简单总结一下，机器学习按照数据输出标签yn划分的话，包括监督式学习、非监督式学习、半监督式学习和增强学习等。其中，监督式学习应用最为广泛。</p><p><img src="http://img.blog.csdn.net/20170609093112388?" alt="这里写图片描述"></p><h3 id="Learning-with-Different-Protocol-f-xn-yn"><a href="#Learning-with-Different-Protocol-f-xn-yn" class="headerlink" title="Learning with Different Protocol f(xn,yn)"></a>Learning with Different Protocol f(xn,yn)</h3><p>按照不同的协议，机器学习可以分为三种类型：</p><ul><li><p>Batch Learning</p></li><li><p>Online</p></li><li><p>Active Learning</p></li></ul><p>batch learning是一种常见的类型。batch learning获得的训练数据D是一批的，即一次性拿到整个D，对其进行学习建模，得到我们最终的机器学习模型。batch learning在实际应用中最为广泛。</p><p>online是一种在线学习模型，数据是实时更新的，根据数据一个个进来，同步更新我们的算法。比如在线邮件过滤系统，根据一封一封邮件的内容，根据当前算法判断是否为垃圾邮件，再根据用户反馈，及时更新当前算法。这是一个动态的过程。之前我们介绍的PLA和增强学习都可以使用online模型。</p><p>active learning是近些年来新出现的一种机器学习类型，即让机器具备主动问问题的能力，例如手写数字识别，机器自己生成一个数字或者对它不确定的手写字主动提问。active learning优势之一是在获取样本label比较困难的时候，可以节约时间和成本，只对一些重要的label提出需求。</p><p>简单总结一下，按照不同的协议，机器学习可以分为batch, online, active。这三种学习类型分别可以类比为：填鸭式，老师教学以及主动问问题。</p><p><img src="http://img.blog.csdn.net/20170609101642114?" alt="这里写图片描述"></p><h3 id="Learning-with-Different-Input-Space-X"><a href="#Learning-with-Different-Input-Space-X" class="headerlink" title="Learning with Different Input Space X"></a>Learning with Different Input Space X</h3><p>上面几部分介绍的机器学习分类都是根据输出来分类的，比如根据输出空间进行分类，根据输出y的标记进行分类，根据取得数据和标记的方法进行分类。这部分，我们将谈谈输入X有哪些类型。</p><p>输入X的第一种类型就是concrete features。比如说硬币分类问题中硬币的尺寸、重量等；比如疾病诊断中的病人信息等具体特征。concrete features对机器学习来说最容易理解和使用。</p><p>第二种类型是raw features。比如说手写数字识别中每个数字所在图片的mxn维像素值；比如语音信号的频谱等。raw features一般比较抽象，经常需要人或者机器来转换为其对应的concrete features，这个转换的过程就是Feature Transform。</p><p>第三种类型是abstract features。比如某购物网站做购买预测时，提供给参赛者的是抽象加密过的资料编号或者ID，这些特征X完全是抽象的，没有实际的物理含义。所以对于机器学习来说是比较困难的，需要对特征进行更多的转换和提取。</p><p>简单总结一下，根据输入X类型不同，可以分为concetet, raw, abstract。将一些抽象的特征转换为具体的特征，是机器学习过程中非常重要的一个环节。在《机器学习技法》课程中，我们再详细介绍。</p><p><img src="http://img.blog.csdn.net/20170609104530535?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了机器学习的类型，包括Out Space、Data Label、Protocol、Input Space四种类型。</p><p><img src="http://img.blog.csdn.net/20170609104941159?" alt="这里写图片描述"></p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p><p><strong>更多AI资源请关注公众号：红色石头的机器学习之路（ID：redstonewill）</strong><br><img src="http://img.blog.csdn.net/20180305133756771?" alt="这里写图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170609080354964?imageView/2/w/400/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记2 -- Learning to Answer Yes or No</title>
    <link href="https://redstonewill.github.io/2018/03/14/2/"/>
    <id>https://redstonewill.github.io/2018/03/14/2/</id>
    <published>2018-03-14T01:39:54.000Z</published>
    <updated>2018-03-16T14:16:44.490Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170608082007683?imageView/2/w/400/q/100" alt="这里写图片描述"></p><a id="more"></a><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要简述了机器学习的定义及其重要性，并用流程图的形式介绍了机器学习的整个过程：根据模型H，使用演算法A，在训练样本D上进行训练，得到最好的h，其对应的g就是我们最后需要的机器学习的模型函数，一般g接近于目标函数f。本节课将继续深入探讨机器学习问题，介绍感知机Perceptron模型，并推导课程的第一个机器学习算法：Perceptron Learning Algorithm（PLA）。</p><h3 id="Perceptron-Hypothesis-Set"><a href="#Perceptron-Hypothesis-Set" class="headerlink" title="Perceptron Hypothesis Set"></a>Perceptron Hypothesis Set</h3><p>引入这样一个例子：某银行要根据用户的年龄、性别、年收入等情况来判断是否给该用户发信用卡。现在有训练样本D，即之前用户的信息和是否发了信用卡。这是一个典型的机器学习问题，我们要根据D，通过A，在H中选择最好的h，得到g，接近目标函数f，也就是根据先验知识建立是否给用户发信用卡的模型。银行用这个模型对以后用户进行判断：发信用卡（+1），不发信用卡（-1）。</p><p>在这个机器学习的整个流程中，有一个部分非常重要：就是模型选择，即Hypothesis Set。选择什么样的模型，很大程度上会影响机器学习的效果和表现。下面介绍一个简单常用的Hypothesis Set：感知机（Perceptron）。</p><p>还是刚才银行是否给用户发信用卡的例子，我们把用户的个人信息作为特征向量x，令总共有d个特征，每个特征赋予不同的权重w，表示该特征对输出（是否发信用卡）的影响有多大。那所有特征的加权和的值与一个设定的阈值threshold进行比较：大于这个阈值，输出为+1，即发信用卡；小于这个阈值，输出为-1，即不发信用卡。感知机模型，就是当特征加权和与阈值的差大于或等于0，则输出h(x)=1；当特征加权和与阈值的差小于0，则输出h(x)=-1，而我们的目的就是计算出所有权值w和阈值threshold。</p><p><img src="http://img.blog.csdn.net/20170608082007683?" alt="这里写图片描述"></p><p>为了计算方便，通常我们将阈值threshold当做$w_0$，引入一个$x_0=1$的量与$w_0$相乘，这样就把threshold也转变成了权值$w_0$，简化了计算。h(x)的表达式做如下变换：</p><p><img src="http://img.blog.csdn.net/20170608083119699?" alt="这里写图片描述"></p><p>为了更清晰地说明感知机模型，我们假设Perceptrons在二维平面上，即$h(x)=sign(w_0+w_1x_1+w_2x_2)$。其中，$w_0+w_1x_1+w_2x_2=0$是平面上一条分类直线，直线一侧是正类（+1），直线另一侧是负类（-1）。权重w不同，对应于平面上不同的直线。</p><p><img src="http://img.blog.csdn.net/20170608084125366?" alt="这里写图片描述"></p><p>那么，我们所说的Perceptron，在这个模型上就是一条直线，称之为linear(binary) classifiers。注意一下，感知器线性分类不限定在二维空间中，在3D中，线性分类用平面表示，在更高维度中，线性分类用超平面表示，即只要是形如$w^Tx$的线性模型就都属于linear(binary) classifiers。</p><p>同时，需要注意的是，这里所说的linear(binary) classifiers是用简单的感知器模型建立的，线性分类问题还可以使用logistic regression来解决，后面将会介绍。</p><h3 id="Perceptron-Learning-Algorithm-PLA"><a href="#Perceptron-Learning-Algorithm-PLA" class="headerlink" title="Perceptron Learning Algorithm(PLA)"></a>Perceptron Learning Algorithm(PLA)</h3><p>根据上一部分的介绍，我们已经知道了hypothesis set由许多条直线构成。接下来，我们的目的就是如何设计一个演算法A，来选择一个最好的直线，能将平面上所有的正类和负类完全分开，也就是找到最好的g，使$g\approx f$。</p><p>如何找到这样一条最好的直线呢？我们可以使用逐点修正的思想，首先在平面上随意取一条直线，看看哪些点分类错误。然后开始对第一个错误点就行修正，即变换直线的位置，使这个错误点变成分类正确的点。接着，再对第二个、第三个等所有的错误分类点就行直线纠正，直到所有的点都完全分类正确了，就得到了最好的直线。这种“逐步修正”，就是PLA思想所在。</p><p><img src="http://img.blog.csdn.net/20170608095000165?" alt="这里写图片描述"></p><p>下面介绍一下PLA是怎么做的。首先随机选择一条直线进行分类。然后找到第一个分类错误的点，如果这个点表示正类，被误分为负类，即$w_t^Tx_{n(t)}&lt;0$，那表示w和x夹角大于90度，其中w是直线的法向量。所以，x被误分在直线的下侧（相对于法向量，法向量的方向即为正类所在的一侧），修正的方法就是使w和x夹角小于90度。通常做法是$w\leftarrow w+yx,\ y=1$，如图右上角所示，一次或多次更新后的$w+yx$与x夹角小于90度，能保证x位于直线的上侧，则对误分为负类的错误点完成了直线修正。</p><p>同理，如果是误分为正类的点，即$w_t^Tx_{n(t)}&gt;0$，那表示w和x夹角小于90度，其中w是直线的法向量。所以，x被误分在直线的上侧，修正的方法就是使w和x夹角大于90度。通常做法是$w\leftarrow w+yx,\ y=-1$，如图右下角所示，一次或多次更新后的$w+yx$与x夹角大于90度，能保证x位于直线的下侧，则对误分为正类的错误点也完成了直线修正。</p><p>按照这种思想，遇到个错误点就进行修正，不断迭代。要注意一点：每次修正直线，可能使之前分类正确的点变成错误点，这是可能发生的。但是没关系，不断迭代，不断修正，最终会将所有点完全正确分类（PLA前提是线性可分的）。这种做法的思想是“知错能改”，有句话形容它：“A fault confessed is half redressed.”</p><p>实际操作中，可以一个点一个点地遍历，发现分类错误的点就进行修正，直到所有点全部分类正确。这种被称为Cyclic PLA。</p><p><img src="http://img.blog.csdn.net/20170608102847562?" alt="这里写图片描述"></p><p>下面用图解的形式来介绍PLA的修正过程：</p><p><img src="http://img.blog.csdn.net/20170608104910590?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608104952086?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105013685?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105029404?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105044842?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105100764?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105122249?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105214946?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105230634?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105243181?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105257829?" alt="这里写图片描述"></p><p>对PLA，我们需要考虑以下两个问题：</p><ul><li><p>PLA迭代一定会停下来吗？如果线性不可分怎么办？</p></li><li><p>PLA停下来的时候，是否能保证$f\approx g$？如果没有停下来，是否有$f\approx g$？</p></li></ul><h3 id="Guarantee-of-PLA"><a href="#Guarantee-of-PLA" class="headerlink" title="Guarantee of PLA"></a>Guarantee of PLA</h3><p>PLA什么时候会停下来呢？根据PLA的定义，当找到一条直线，能将所有平面上的点都分类正确，那么PLA就停止了。要达到这个终止条件，就必须保证D是线性可分（linear separable）。如果是非线性可分的，那么，PLA就不会停止。</p><p><img src="http://img.blog.csdn.net/20170608111542131?" alt="这里写图片描述"></p><p>对于线性可分的情况，如果有这样一条直线，能够将正类和负类完全分开，令这时候的目标权重为$w_f$，则对每个点，必然满足$y_n=sign(w_f^Tx_n)$，即对任一点：</p><p><img src="http://img.blog.csdn.net/20170608134312092?" alt="这里写图片描述"></p><p>PLA会对每次错误的点进行修正，更新权重$w_{t+1}$的值，如果$w_{t+1}$与$w_f$越来越接近，数学运算上就是内积越大，那表示$w_{t+1}$是在接近目标权重$w_f$，证明PLA是有学习效果的。所以，我们来计算$w_{t+1}$与$w_f$的内积：</p><p><img src="http://img.blog.csdn.net/20170608134340499?" alt="这里写图片描述"></p><p>从推导可以看出，$w_{t+1}$与$w_f$的内积跟$w_t$与$w_f$的内积相比更大了。似乎说明了$w_{t+1}$更接近$w_f$，但是内积更大，可能是向量长度更大了，不一定是向量间角度更小。所以，下一步，我们还需要证明$w_{t+1}$与$w_t$向量长度的关系：</p><p><img src="http://img.blog.csdn.net/20170608140302480?" alt="这里写图片描述"></p><p>$w_t$只会在分类错误的情况下更新，最终得到的$||w_{t+1}^2||$相比$||w_{t}^2||$的增量值不超过$max||x_n^2||$。也就是说，$w_t$的增长被限制了，$w_{t+1}$与$w_t$向量长度不会差别太大！</p><p>如果令初始权值$w_0=0$，那么经过T次错误修正后，有如下结论：</p><p>$$\frac{w_f^T}{||w_f||}\frac{w_T}{w_T}\geq \sqrt T\cdot constant$$</p><p>下面贴出来该结论的具体推导过程：</p><p><img src="http://img.blog.csdn.net/20170608143421951?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608143438779?" alt="这里写图片描述"></p><p>上述不等式左边其实是$w_T$与$w_f$夹角的余弦值，随着T增大，该余弦值越来越接近1，即$w_T$与$w_f$越来越接近。同时，需要注意的是，$\sqrt T\cdot constant\leq 1$，也就是说，迭代次数T是有上界的。根据以上证明，我们最终得到的结论是：$w_{t+1}$与$w_f$的是随着迭代次数增加，逐渐接近的。而且，PLA最终会停下来（因为T有上界），实现对线性可分的数据集完全分类。</p><h3 id="Non-Separable-Data"><a href="#Non-Separable-Data" class="headerlink" title="Non-Separable Data"></a>Non-Separable Data</h3><p>上一部分，我们证明了线性可分的情况下，PLA是可以停下来并正确分类的，但对于非线性可分的情况，$w_f$实际上并不存在，那么之前的推导并不成立，PLA不一定会停下来。所以，PLA虽然实现简单，但也有缺点：</p><p><img src="http://img.blog.csdn.net/20170608145844603?" alt="这里写图片描述"></p><p>对于非线性可分的情况，我们可以把它当成是数据集D中掺杂了一下noise，事实上，大多数情况下我们遇到的D，都或多或少地掺杂了noise。这时，机器学习流程是这样的：</p><p><img src="http://img.blog.csdn.net/20170608150716294?" alt="这里写图片描述"></p><p>在非线性情况下，我们可以把条件放松，即不苛求每个点都分类正确，而是容忍有错误点，取错误点的个数最少时的权重w：</p><p><img src="http://img.blog.csdn.net/20170608151418751?" alt="这里写图片描述"></p><p>事实证明，上面的解是NP-hard问题，难以求解。然而，我们可以对在线性可分类型中表现很好的PLA做个修改，把它应用到非线性可分类型中，获得近似最好的g。</p><p>修改后的PLA称为Packet Algorithm。它的算法流程与PLA基本类似，首先初始化权重$w_0$，计算出在这条初始化的直线中，分类错误点的个数。然后对错误点进行修正，更新w，得到一条新的直线，在计算其对应的分类错误的点的个数，并与之前错误点个数比较，取个数较小的直线作为我们当前选择的分类直线。之后，再经过n次迭代，不断比较当前分类错误点个数与之前最少的错误点个数比较，选择最小的值保存。直到迭代次数完成后，选取个数最少的直线对应的w，即为我们最终想要得到的权重值。</p><p><img src="http://img.blog.csdn.net/20170608155259223?" alt="这里写图片描述"></p><p>如何判断数据集D是不是线性可分？对于二维数据来说，通常还是通过肉眼观察来判断的。一般情况下，Pocket Algorithm要比PLA速度慢一些。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了线性感知机模型，以及解决这类感知机分类问题的简单算法：PLA。我们详细证明了对于线性可分问题，PLA可以停下来并实现完全正确分类。对于不是线性可分的问题，可以使用PLA的修正算法Pocket Algorithm来解决。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170608082007683?imageView/2/w/400/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记1 -- The Learning Problem</title>
    <link href="https://redstonewill.github.io/2018/03/13/1/"/>
    <id>https://redstonewill.github.io/2018/03/13/1/</id>
    <published>2018-03-13T08:59:11.000Z</published>
    <updated>2018-03-16T14:16:15.793Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170607145430382?imageView/2/w/400/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>最近在看NTU林轩田的《机器学习基石》课程，个人感觉讲的非常好。整个基石课程分成四个部分：</p><ul><li><p>When Can Machine Learn? </p></li><li><p>Why Can Machine Learn? </p></li><li><p>How Can Machine Learn?</p></li><li><p>How Can Machine Learn Better?</p></li></ul><p>每个部分由四节课组成，总共有16节课。那么，从这篇开始，我们将连续对这门课做课程笔记，共16篇，希望能对正在看这们课的童鞋有所帮助。下面开始第一节课的笔记：The Learning Problem。</p><h3 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning"></a>What is Machine Learning</h3><p>什么是“学习”？学习就是人类通过观察、积累经验，掌握某项技能或能力。就好像我们从小学习识别字母、认识汉字，就是学习的过程。而机器学习（Machine Learning），顾名思义，就是让机器（计算机）也能向人类一样，通过观察大量的数据和训练，发现事物规律，获得某种分析问题、解决问题的能力。</p><p><img src="http://img.blog.csdn.net/20170607145430382?" alt="这里写图片描述"></p><p>机器学习可以被定义为：Improving some performance measure with experence computed from data. 也就是机器从数据中总结经验，从数据中找出某种规律或者模型，并用它来解决实际问题。</p><p><img src="http://img.blog.csdn.net/20170607145937180?" alt="这里写图片描述"></p><p>什么情况下会使用机器学习来解决问题呢？其实，目前机器学习的应用非常广泛，基本上任何场合都能够看到它的身影。其应用场合大致可归纳为三个条件：</p><ul><li><p>事物本身存在某种潜在规律</p></li><li><p>某些问题难以使用普通编程解决</p></li><li><p>有大量的数据样本可供使用</p></li></ul><p><img src="http://img.blog.csdn.net/20170607151033657?" alt="这里写图片描述"></p><h3 id="Applications-of-Machine-Learning"><a href="#Applications-of-Machine-Learning" class="headerlink" title="Applications of Machine Learning"></a>Applications of Machine Learning</h3><p>机器学习在我们的衣、食、住、行、教育、娱乐等各个方面都有着广泛的应用，我们的生活处处都离不开机器学习。比如，打开购物网站，网站就会给我们自动推荐我们可能会喜欢的商品；电影频道会根据用户的浏览记录和观影记录，向不同用户推荐他们可能喜欢的电影等等，到处都有机器学习的影子。</p><h3 id="Components-of-Machine-Learning"><a href="#Components-of-Machine-Learning" class="headerlink" title="Components of Machine Learning"></a>Components of Machine Learning</h3><p>本系列的课程对机器学习问题有一些基本的术语需要注意一下：</p><ul><li><p>输入x</p></li><li><p>输出y</p></li><li><p>目标函数f，即最接近实际样本分布的规律</p></li><li><p>训练样本data</p></li><li><p>假设hypothesis，一个机器学习模型对应了很多不同的hypothesis，通过演算法A，选择一个最佳的hypothesis对应的函数称为矩g，g能最好地表示事物的内在规律，也是我们最终想要得到的模型表达式。</p></li></ul><p><img src="http://img.blog.csdn.net/20170607153054321?" alt="这里写图片描述"></p><p>实际中，机器学习的流程图可以表示为：</p><p><img src="http://img.blog.csdn.net/20170607153730795?" alt="这里写图片描述"></p><p>对于理想的目标函数f，我们是不知道的，我们手上拿到的是一些训练样本D，假设是监督式学习，其中有输入x，也有输出y。机器学习的过程，就是根据先验知识选择模型，该模型对应的hypothesis set（用H表示），H中包含了许多不同的hypothesis，通过演算法A，在训练样本D上进行训练，选择出一个最好的hypothes，对应的函数表达式g就是我们最终要求的。一般情况下，g能最接近目标函数f，这样，机器学习的整个流程就完成了。</p><h3 id="Machine-Learning-and-Other-Fields"><a href="#Machine-Learning-and-Other-Fields" class="headerlink" title="Machine Learning and Other Fields"></a>Machine Learning and Other Fields</h3><p>与机器学习相关的领域有：</p><ul><li><p>数据挖掘（Data Mining）</p></li><li><p>人工智能（Artificial Intelligence）</p></li><li><p>统计（Statistics）</p></li></ul><p>其实，机器学习与这三个领域是相通的，基本类似，但也不完全一样。机器学习是这三个领域中的有力工具，而同时，这三个领域也是机器学习可以广泛应用的领域，总得来说，他们之间没有十分明确的界线。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了什么是机器学习，什么样的场合下可以使用机器学习解决问题，然后用流程图的形式展示了机器学习的整个过程，最后把机器学习和数据挖掘、人工智能、统计这三个领域做个比较。本节课的内容主要是概述性的东西，比较简单，所以笔记也相对比较简略。</p><p>这里附上林轩田（Hsuan-Tien Lin）关于这门课的主页：<br><a href="http://www.csie.ntu.edu.tw/~htlin/" target="_blank" rel="noopener">http://www.csie.ntu.edu.tw/~htlin/</a></p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p><p><strong>关注公众号并回复jishi1获得本节课笔记的pdf文件哦～</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170607145430382?imageView/2/w/400/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
