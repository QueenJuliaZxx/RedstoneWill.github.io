<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>红色石头的机器学习之路</title>
  
  <subtitle>公众号ID：redstonewill</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://redstonewill.github.io/"/>
  <updated>2018-03-18T03:44:31.196Z</updated>
  <id>https://redstonewill.github.io/</id>
  
  <author>
    <name>红色石头</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记6 -- Support Vector Regression</title>
    <link href="https://redstonewill.github.io/2018/03/18/23/"/>
    <id>https://redstonewill.github.io/2018/03/18/23/</id>
    <published>2018-03-18T03:23:36.000Z</published>
    <updated>2018-03-18T03:44:31.196Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170708142619499?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Kernel Logistic Regression，讨论如何把SVM的技巧应用在soft-binary classification上。方法是使用2-level learning，先利用SVM得到参数b和w，然后再用通用的logistic regression优化算法，通过迭代优化，对参数b和w进行微调，得到最佳解。然后，也介绍了可以通过Representer Theorem，在z空间中，引入SVM的kernel技巧，直接对logistic regression进行求解。本节课将延伸上节课的内容，讨论如何将SVM的kernel技巧应用到regression问题上。</p><h3 id="Kernel-Ridge-Regression"><a href="#Kernel-Ridge-Regression" class="headerlink" title="Kernel Ridge Regression"></a>Kernel Ridge Regression</h3><p>首先回顾一下上节课介绍的Representer Theorem，对于任何包含正则项的L2-regularized linear model，它的最佳化解w都可以写成是z的线性组合形式，因此，也就能引入kernel技巧，将模型kernelized化。</p><p><img src="http://img.blog.csdn.net/20170708142619499?" alt="这里写图片描述"></p><p>那么如何将regression模型变成kernel的形式呢？我们之前介绍的linear/ridge regression最常用的错误估计是squared error，即$err(y,w^Tz)=(y-w^Tz)^2$。这种形式对应的解是analytic solution，即可以使用线性最小二乘法，通过向量运算，直接得到最优化解。那么接下来我们就要研究如何将kernel引入到ridge regression中去，得到与之对应的analytic solution。</p><p>我们先把Kernel Ridge Regression问题写下来：</p><p><img src="http://img.blog.csdn.net/20170708143552496?" alt="这里写图片描述"></p><p>其中，最佳解$w_<em>$必然是z的线性组合。那么我们就把$w_</em>=\sum_{n=1}^N\beta_nz_n$代入到ridge regression中，将z的内积用kernel替换，把求$w_*$的问题转化成求$\beta_n$的问题，得到：</p><p><img src="http://img.blog.csdn.net/20170708144926721?" alt="这里写图片描述"></p><p>ridge regression可以写成矩阵的形式，其中第一项可以看成是$\beta_n$的正则项，而第二项可以看成是$\beta_n$的error function。这样，我们的目的就是求解该式最小化对应的$\beta_n$值，这样就解决了kernel ridge regression问题。</p><p>求解$\beta_n$的问题可以写成如下形式：</p><p><img src="http://img.blog.csdn.net/20170708145512253?" alt="这里写图片描述"></p><p>$E_{aug}(\beta)$是关于$\beta$的二次多项式，要对$E_{aug}(\beta)$求最小化解，这种凸二次最优化问题，只需要先计算其梯度，再令梯度为零即可。$\nabla E_{aug}(\beta)$已经在上式中写出来了，令其等于零，即可得到一种可能的$\beta$的解析解为：</p><p>$$\beta=(\lambda I+K)^{-1}y$$</p><p>这里需要关心的问题是$(\lambda I+K)$的逆矩阵是否存在？答案是肯定的。因为我们之前介绍过，核函数K满足Mercer’s condition，它是半正定的，而且$\lambda&gt;0$，所以$(\lambda I+K)$一定是可逆的。从计算的时间复杂上来说，由于$(\lambda I+K)$是NxN大小的，所以时间复杂度是$O(N^3)$。还有一点，$\nabla E_{aug}(\beta)$是由两项乘积构成的，另一项是K，会不会出现K=0的情况呢？其实，由于核函数K表征的是z空间的内积，一般而言，除非两个向量互相垂直，内积才为零，否则，一般情况下K不等于零。这个原因也决定了$(\lambda I+K)$是dense matrix，即$\beta$的解大部分都是非零值。这个性质，我们之后还会说明。</p><p>所以说，我们可以通过kernel来解决non-linear regression的问题。下面比较一下linear ridge regression和kernel ridge regression的关系。</p><p><img src="http://img.blog.csdn.net/20170708155331930?" alt="这里写图片描述"></p><p>如上图所示，左边是linear ridge regression，是一条直线；右边是kernel ridge regression，是一条曲线。大致比较一下，右边的曲线拟合的效果更好一些。这两种regression有什么样的优点和缺点呢？对于linear ridge regression来说，它是线性模型，只能拟合直线；其次，它的训练复杂度是$O(d^3+d^2N)$，预测的复杂度是$O(d)$，如果N比d大很多时，这种模型就更有效率。而对于kernel ridge regression来说，它转换到z空间，使用kernel技巧，得到的是非线性模型，所以更加灵活；其次，它的训练复杂度是$O(N^3)$，预测的复杂度是$O(N)$，均只与N有关。当N很大的时候，计算量就很大，所以，kernel ridge regression适合N不是很大的场合。比较下来，可以说linear和kernel实际上是效率（efficiency）和灵活（flexibility）之间的权衡。</p><p><img src="http://img.blog.csdn.net/20170708160254603?" alt="这里写图片描述"></p><h3 id="Support-Vector-Regression-Primal"><a href="#Support-Vector-Regression-Primal" class="headerlink" title="Support Vector Regression Primal"></a>Support Vector Regression Primal</h3><p>我们在机器学习基石课程中介绍过linear regression可以用来做classification，那么上一部分介绍的kernel ridge regression同样可以来做classification。我们把kernel ridge regression应用在classification上取个新的名字，叫做least-squares SVM（LSSVM）。</p><p>先来看一下对于某个问题，soft-margin Gaussian SVM和Gaussian LSSVM结果有哪些不一样的地方。</p><p><img src="http://img.blog.csdn.net/20170708165152565?" alt="这里写图片描述"></p><p>如上图所示，如果只看分类边界的话，soft-margin Gaussian SVM和Gaussian LSSVM差别不是很大，即的到的分类线是几乎相同的。但是如果看Support Vector的话（图中方框标注的点），左边soft-margin Gaussian SVM的SV不多，而右边Gaussian LSSVM中基本上每个点都是SV。这是因为soft-margin Gaussian SVM中的$\alpha_n$大部分是等于零，$\alpha_n&gt;0$的点只占少数，所以SV少。而对于LSSVM，我们上一部分介绍了$\beta$的解大部分都是非零值，所以对应的每个点基本上都是SV。SV太多会带来一个问题，就是做预测的矩$g(x)=\sum_{n=1}^N\beta_nK(x_n,x)$，如果$\beta_n$非零值较多，那么g的计算量也比较大，降低计算速度。基于这个原因，soft-margin Gaussian SVM更有优势。</p><p><img src="http://img.blog.csdn.net/20170708171711889?" alt="这里写图片描述"></p><p>那么，针对LSSVM中dense $\beta$的缺点，我们能不能使用一些方法来的得到sparse $\beta$，使得SV不会太多，从而得到和soft-margin SVM同样的分类效果呢？下面我们将尝试解决这个问题。</p><p>方法是引入一个叫做Tube Regression的做法，即在分类线上下分别划定一个区域（中立区），如果数据点分布在这个区域内，则不算分类错误，只有误分在中立区域之外的地方才算error。</p><p><img src="http://img.blog.csdn.net/20170708174337480?" alt="这里写图片描述"></p><p>假定中立区的宽度为$2\epsilon$，$\epsilon&gt;0$,那么error measure就可以写成：$err(y,s)=max(0,|s-y|-\epsilon)$，对应上图中红色标注的距离。</p><p><img src="http://img.blog.csdn.net/20170708174952966?" alt="这里写图片描述"></p><p>通常把这个error叫做$\epsilon$-insensitive error，这种max的形式跟我们上节课中介绍的hinge error measure形式其实是类似的。所以，我们接下来要做的事情就是将L2-regularized tube regression做类似于soft-margin SVM的推导，从而得到sparse $\beta$。</p><p>首先，我们把tube regression中的error与squared error做个比较：</p><p><img src="http://img.blog.csdn.net/20170708180829090?" alt="这里写图片描述"></p><p>然后，将err(y,s)与s的关系曲线分别画出来：</p><p><img src="http://img.blog.csdn.net/20170708181020406?" alt="这里写图片描述"></p><p>上图中，红色的线表示squared error，蓝色的线表示tube error。我们发现，当|s-y|比较小即s比较接近y的时候，squared error与tube error是差不多大小的。而在|s-y|比较大的区域，squared error的增长幅度要比tube error大很多。error的增长幅度越大，表示越容易受到noise的影响，不利于最优化问题的求解。所以，从这个方面来看，tube regression的这种error function要更好一些。</p><p>现在，我们把L2-Regularized Tube Regression写下来：</p><p><img src="http://img.blog.csdn.net/20170708205719827?" alt="这里写图片描述"></p><p>这个最优化问题，由于其中包含max项，并不是处处可微分的，所以不适合用GD/SGD来求解。而且，虽然满足representer theorem，有可能通过引入kernel来求解，但是也并不能保证得到sparsity $\beta$。从另一方面考虑，我们可以把这个问题转换为带条件的QP问题，仿照dual SVM的推导方法，引入kernel，得到KKT条件，从而保证解$\beta$是sparse的。</p><p><img src="http://img.blog.csdn.net/20170708210614893?" alt="这里写图片描述"></p><p>所以，我们就可以把L2-Regularized Tube Regression写成跟SVM类似的形式：</p><p><img src="http://img.blog.csdn.net/20170708210828256?" alt="这里写图片描述"></p><p>值得一提的是，系数$\lambda$和C是反比例相关的，$\lambda$越大对应C越小，$\lambda$越小对应C越大。而且该式也把$w_0$即b单独拿了出来，这跟我们之前推导SVM的解的方法是一致的。</p><p>现在我们已经有了Standard Support Vector Regression的初始形式，这还是不是一个标准的QP问题。我们继续对该表达式做一些转化和推导：</p><p><img src="http://img.blog.csdn.net/20170708211909816?" alt="这里写图片描述"></p><p>如上图右边所示，即为标准的QP问题，其中$\xi_n^{\bigvee}$和$\xi_n^{\bigwedge}$分别表示upper tube violations和lower tube violations。这种形式叫做Support Vector Regression（SVR） primal。</p><p><img src="http://img.blog.csdn.net/20170708212831107?" alt="这里写图片描述"></p><p>SVR的标准QP形式包含几个重要的参数：C和$\epsilon$。C表示的是regularization和tube violation之间的权衡。large C倾向于tube violation，small C则倾向于regularization。$\epsilon$表征了tube的区域宽度，即对错误点的容忍程度。$\epsilon$越大，则表示对错误的容忍度越大。$\epsilon$是可设置的常数，是SVR问题中独有的，SVM中没有这个参数。另外，SVR的QP形式共有$\hat{d}+1+2N$个参数，2N+2N个条件。</p><p><img src="http://img.blog.csdn.net/20170708213803381?" alt="这里写图片描述"></p><h3 id="Support-Vector-Regression-Dual"><a href="#Support-Vector-Regression-Dual" class="headerlink" title="Support Vector Regression Dual"></a>Support Vector Regression Dual</h3><p>现在我们已经得到了SVR的primal形式，接下来将推导SVR的Dual形式。首先，与SVM对偶形式一样，先令拉格朗日因子$\alpha^{\bigvee}$和$\alpha^{\bigwedge}$，分别是与$\xi_n^{\bigvee}$和$\xi_n^{\bigwedge}$不等式相对应。这里忽略了与$\xi_n^{\bigvee}\geq0$和$\xi_n^{\bigwedge}\geq0$对应的拉格朗日因子。</p><p><img src="http://img.blog.csdn.net/20170709102015594?" alt="这里写图片描述"></p><p>然后，与SVM一样做同样的推导和化简，拉格朗日函数对相关参数偏微分为零，得到相应的KKT条件：</p><p><img src="http://img.blog.csdn.net/20170709102506198?" alt="这里写图片描述"></p><p>接下来，通过观察SVM primal与SVM dual的参数对应关系，直接从SVR primal推导出SVR dual的形式。（具体数学推导，此处忽略！）</p><p><img src="http://img.blog.csdn.net/20170709103540709?" alt="这里写图片描述"></p><p>最后，我们就要来讨论一下SVR的解是否真的是sparse的。前面已经推导了SVR dual形式下推导的解w为：</p><p>$$w=\sum_{n=1}^N(\alpha_n^{\bigwedge}-\alpha_n^{\bigvee})z_n$$</p><p>相应的complementary slackness为：</p><p><img src="http://img.blog.csdn.net/20170709104346150?" alt="这里写图片描述"></p><p>对于分布在tube中心区域内的点，满足$|w^Tz_n+b-y_n|&lt;\epsilon$，此时忽略错误，$\xi_n^{\bigvee}$和$\xi_n^{\bigwedge}$都等于零。则complementary slackness两个等式的第二项均不为零，必然得到$\alpha_n^{\bigwedge}=0$和$\alpha_n^{\bigvee}=0$，即$\beta_n=\alpha_n^{\bigwedge}-\alpha_n^{\bigvee}=0$。</p><p>所以，对于分布在tube内的点，得到的解$\beta_n=0$，是sparse的。而分布在tube之外的点，$\beta_n\neq0$。至此，我们就得到了SVR的sparse解。</p><h3 id="Summary-of-Kernel-Models"><a href="#Summary-of-Kernel-Models" class="headerlink" title="Summary of Kernel Models"></a>Summary of Kernel Models</h3><p>这部分将对我们介绍过的所有的kernel模型做个概括和总结。我们总共介绍过三种线性模型，分别是PLA/pocket，regularized logistic regression和linear ridge regression。这三种模型都可以使用国立台湾大学的Chih-Jen Lin博士开发的Liblinear库函数来解决。</p><p>另外，我们介绍了linear soft-margin SVM，其中的error function是$\hat{err}_{svm}$，可以通过标准的QP问题来求解。linear soft-margin SVM和PLA/pocket一样都是解决同样的问题。然后，还介绍了linear SVR问题，它与linear ridge regression一样都是解决同样的问题，从SVM的角度，使用$err_{tube}$，转换为QP问题进行求解，这也是我们本节课的主要内容。</p><p><img src="http://img.blog.csdn.net/20170709132228041?" alt="这里写图片描述"></p><p>上图中相应的模型也可以转化为dual形式，引入kernel，整体的框图如下：</p><p><img src="http://img.blog.csdn.net/20170709132519188?" alt="这里写图片描述"></p><p>其中SVM，SVR和probabilistic SVM都可以使用国立台湾大学的Chih-Jen Lin博士开发的LLibsvm库函数来解决。通常来说，这些模型中SVR和probabilistic SVM最为常用。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了SVR，我们先通过representer theorem理论，将ridge regression转化为kernel的形式，即kernel ridge regression，并推导了SVR的解。但是得到的解是dense的，大部分为非零值。所以，我们定义新的tube regression，使用SVM的推导方法，来最小化regularized tube errors，转化为对偶形式，得到了sparse的解。最后，我们对介绍过的所有kernel模型做个总结，简单概述了各自的特点。在实际应用中，我们要根据不同的问题进行合适的模型选择。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170708142619499?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记5 -- Kernel Logistic Regression</title>
    <link href="https://redstonewill.github.io/2018/03/18/22/"/>
    <id>https://redstonewill.github.io/2018/03/18/22/</id>
    <published>2018-03-18T03:15:40.000Z</published>
    <updated>2018-03-18T03:41:38.506Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170705230810204?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Soft-Margin SVM，即如果允许有分类错误的点存在，那么在原来的Hard-Margin SVM中添加新的惩罚因子C，修正原来的公式，得到新的$\alpha_n$值。最终的到的$\alpha_n$有个上界，上界就是C。Soft-Margin SVM权衡了large-margin和error point之前的关系，目的是在尽可能犯更少错误的前提下，得到最大分类边界。本节课将把Soft-Margin SVM和我们之前介绍的Logistic Regression联系起来，研究如何使用kernel技巧来解决更多的问题。</p><h3 id="Soft-Margin-SVM-as-Regularized-Model"><a href="#Soft-Margin-SVM-as-Regularized-Model" class="headerlink" title="Soft-Margin SVM as Regularized Model"></a>Soft-Margin SVM as Regularized Model</h3><p>先复习一下我们已经介绍过的内容，我们最早开始讲了Hard-Margin Primal的数学表达式，然后推导了Hard-Margin Dual形式。后来，为了允许有错误点的存在（或者noise），也为了避免模型过于复杂化，造成过拟合，我们建立了Soft-Margin Primal的数学表达式，并引入了新的参数C作为权衡因子，然后也推导了其Soft-Margin Dual形式。因为Soft-Margin Dual SVM更加灵活、便于调整参数，所以在实际应用中，使用Soft-Margin Dual SVM来解决分类问题的情况更多一些。</p><p><img src="http://img.blog.csdn.net/20170705230810204?" alt="这里写图片描述"></p><p>Soft-Margin Dual SVM有两个应用非常广泛的工具包，分别是Libsvm和Liblinear。 Libsvm和Liblinear都是国立台湾大学的Chih-Jen Lin博士开发的，Chih-Jen Lin的个人网站为：<a href="http://www.csie.ntu.edu.tw/~cjlin/index.html" target="_blank" rel="noopener">Welcome to Chih-Jen Lin’s Home Page</a></p><p>下面我们再来回顾一下Soft-Margin SVM的主要内容。我们的出发点是用$\xi_n$来表示margin violation，即犯错值的大小，没有犯错对应的$\xi_n=0$。然后将有条件问题转化为对偶dual形式，使用QP来得到最佳化的解。</p><p>从另外一个角度来看，$\xi_n$描述的是点$(x_n,y_n)$ 距离$y_n(w^Tz_n+b)=1$的边界有多远。第一种情况是violating margin，即不满足$y_n(w^Tz_n+b)\geq1$。那么$\xi_n$可表示为：$\xi_n=1-y_n(w^Tz_n+b)&gt;0$。第二种情况是not violating margin，即点$(x_n,y_n)$ 在边界之外，满足$y_n(w^Tz_n+b)\geq1$的条件，此时$\xi_n=0$。我们可以将两种情况整合到一个表达式中，对任意点：</p><p>$$\xi_n=max(1-y_n(w^Tz_n+b),0)$$</p><p>上式表明，如果有voilating margin，则$1-y_n(w^Tz_n+b)&gt;0$，$\xi_n=1-y_n(w^Tz_n+b)$；如果not violating margin，则$1-y_n(w^Tz_n+b)&lt;0$，$\xi_n=0$。整合之后，我们可以把Soft-Margin SVM的最小化问题写成如下形式：</p><p>$$\frac12w^Tw+C\sum_{n=1}^Nmax(1-y_n(w^Tz_n+b),0)$$</p><p>经过这种转换之后，表征犯错误值大小的变量$\xi_n$就被消去了，转而由一个max操作代替。</p><p><img src="http://img.blog.csdn.net/20170706081744914?" alt="这里写图片描述"></p><p>为什么要将把Soft-Margin SVM转换为这种unconstrained form呢？我们再来看一下转换后的形式，其中包含两项，第一项是w的内积，第二项关于y和w，b，z的表达式，似乎有点像一种错误估计$\hat{err}$，则类似这样的形式：</p><p>$$min\ \frac12w^Tw+C\sum\hat{err}$$</p><p>看到这样的形式我们应该很熟悉，因为之前介绍的L2 Regularization中最优化问题的表达式跟这个是类似的：</p><p>$$min\ \frac{\lambda}{N}w^Tw+\frac1N\sum err$$</p><p><img src="http://img.blog.csdn.net/20170706083142170?" alt="这里写图片描述"></p><p>这里提一下，既然unconstrained form SVM与L2 Regularization的形式是一致的，而且L2 Regularization的解法我们之前也介绍过，那么为什么不直接利用这种方法来解决unconstrained form SVM的问题呢？有两个原因。一个是这种无条件的最优化问题无法通过QP解决，即对偶推导和kernel都无法使用；另一个是这种形式中包含的max()项可能造成函数并不是处处可导，这种情况难以用微分方法解决。</p><p>我们在第一节课中就介绍过Hard-Margin SVM与Regularization Model是有关系的。Regularization的目标是最小化$E_{in}$，条件是$w^Tw\leq C$，而Hard-Margin SVM的目标是最小化$w^Tw$，条件是$E_{in}=0$，即它们的最小化目标和限制条件是相互对调的。对于L2 Regularization来说，条件和最优化问题结合起来，整体形式写成：</p><p>$$\frac{\lambda}{N}w^Tw+E_{in}$$</p><p>而对于Soft-Margin SVM来说，条件和最优化问题结合起来，整体形式写成：</p><p>$$\frac12w^Tw+CN\hat{E_{in}}$$</p><p><img src="http://img.blog.csdn.net/20170706085330431?" alt="这里写图片描述"></p><p>通过对比，我们发现L2 Regularization和Soft-Margin SVM的形式是相同的，两个式子分别包含了参数$\lambda$和C。Soft-Margin SVM中的large margin对应着L2 Regularization中的short w，也就是都让hyperplanes更简单一些。我们使用特别的$\hat{err}$来代表可以容忍犯错误的程度，即soft margin。L2 Regularization中的$\lambda$和Soft-Margin SVM中的C也是相互对应的，$\lambda$越大，w会越小，Regularization的程度就越大；C越小，$\hat{E_{in}}$会越大，相应的margin就越大。所以说增大C，或者减小$\lambda$，效果是一致的，Large-Margin等同于Regularization，都起到了防止过拟合的作用。</p><p><img src="http://img.blog.csdn.net/20170706101351607?" alt="这里写图片描述"></p><p>建立了Regularization和Soft-Margin SVM的关系，接下来我们将尝试看看是否能把SVM作为一个regularized的模型进行扩展，来解决其它一些问题。</p><h3 id="SVM-versus-Logistic-Regression"><a href="#SVM-versus-Logistic-Regression" class="headerlink" title="SVM versus Logistic Regression"></a>SVM versus Logistic Regression</h3><p>上一小节，我们已经把Soft-Margin SVM转换成无条件的形式：</p><p><img src="http://img.blog.csdn.net/20170706112629915?" alt="这里写图片描述"></p><p>上式中第二项的$max(1-y_n(w^Tz_n+b),0)$倍设置为$\hat{err}$。下面我们来看看$\hat{err}$与之前再二元分类中介绍过的$err_{0/1}$有什么关系。</p><p>对于$err_{0/1}$，它的linear score $s=w^Tz_n+b$，当$ys\geq0$时，$err_{0/1}=0$；当$ys&lt;0$时，$err_{0/1}=1$，呈阶梯状，如下图所示。而对于$\hat{err}$，当$ys\geq0$时，$err_{0/1}=0$；当$ys&lt;0$时，$err_{0/1}=1-ys$，呈折线状，如下图所示，通常把$\hat{err}_{svm}$称为hinge error measure。比较两条error曲线，我们发现$\hat{err}_{svm}$始终在$err_{0/1}$的上面，则$\hat{err}_{svm}$可作为$err_{0/1}$的上界。所以，可以使用$\hat{err}_{svm}$来代替$err_{0/1}$，解决二元线性分类问题，而且$\hat{err}_{svm}$是一个凸函数，使它在最佳化问题中有更好的性质。</p><p><img src="http://img.blog.csdn.net/20170706140323646?" alt="这里写图片描述"></p><p>紧接着，我们再来看一下logistic regression中的error function。逻辑回归中，$err_{sce}=log_2(1+exp(-ys))$，当ys=0时，$err_{sce}=1$。它的err曲线如下所示。</p><p><img src="http://img.blog.csdn.net/20170706141204821?" alt="这里写图片描述"></p><p>很明显，$err_{sce}$也是$err_{0/1}$的上界，而$err_{sce}$与$\hat{err}<em>{svm}$也是比较相近的。因为当ys趋向正无穷大的时候，$err</em>{sce}$和$\hat{err}<em>{svm}$都趋向于零；当ys趋向负无穷大的时候，$err</em>{sce}$和$\hat{err}_{svm}$都趋向于正无穷大。正因为二者的这种相似性，我们可以把SVM看成是L2-regularized logistic regression。</p><p>总结一下，我们已经介绍过几种Binary Classification的Linear Models，包括PLA，Logistic Regression和Soft-Margin SVM。PLA是相对简单的一个模型，对应的是$err_{0/1}$，通过不断修正错误的点来获得最佳分类线。它的优点是简单快速，缺点是只对线性可分的情况有用，线性不可分的情况需要用到pocket算法。Logistic Regression对应的是$err_{sce}$，通常使用GD/SGD算法求解最佳分类线。它的优点是凸函数$err_{sce}$便于最优化求解，而且有regularization作为避免过拟合的保证；缺点是$err_{sce}$作为$err_{0/1}$的上界，当ys很小（负值）时，上界变得更宽松，不利于最优化求解。Soft-Margin SVM对应的是$\hat{err}_{svm}$，通常使用QP求解最佳分类线。它的优点和Logistic Regression一样，凸优化问题计算简单而且分类线比较“粗壮”一些；缺点也和Logistic Regression一样，当ys很小（负值）时，上界变得过于宽松。其实，Logistic Regression和Soft-Margin SVM都是在最佳化$err_{0/1}$的上界而已。</p><p><img src="http://img.blog.csdn.net/20170706144406136?" alt="这里写图片描述"></p><p>至此，可以看出，求解regularized logistic regression的问题等同于求解soft-margin SVM的问题。反过来，如果我们求解了一个soft-margin SVM的问题，那这个解能否直接为regularized logistic regression所用？来预测结果是正类的几率是多少，就像regularized logistic regression做的一样。我们下一小节将来解答这个问题。</p><h3 id="SVM-for-Soft-Binary-Classification"><a href="#SVM-for-Soft-Binary-Classification" class="headerlink" title="SVM for Soft Binary Classification"></a>SVM for Soft Binary Classification</h3><p>接下来，我们探讨如何将SVM的结果应用在Soft Binary Classification中，得到是正类的概率值。</p><p>第一种简单的方法是先得到SVM的解$(b_{svm},w_{svm})$，然后直接代入到logistic regression中，得到$g(x)=\theta(w_{svm}^Tx+b_{svm})$。这种方法直接使用了SVM和logistic regression的相似性，一般情况下表现还不错。但是，这种形式过于简单，与logistic regression的关联不大，没有使用到logistic regression中好的性质和方法。</p><p>第二种简单的方法是同样先得到SVM的解$(b_{svm},w_{svm})$，然后把$(b_{svm},w_{svm})$作为logistic regression的初始值，再进行迭代训练修正，速度比较快，最后，将得到的b和w代入到g(x)中。这种做法有点显得多此一举，因为并没有比直接使用logistic regression快捷多少。</p><p><img src="http://img.blog.csdn.net/20170706153919029?" alt="这里写图片描述"></p><p>这两种方法都没有融合SVM和logistic regression各自的优势，下面构造一个模型，融合了二者的优势。构造的模型g(x)表达式为：</p><p>$$g(x)=\theta(A\cdot(w_{svm}^T\Phi(x)+b_{svm})+B)$$</p><p>与上述第一种简单方法不同，我们额外增加了放缩因子A和平移因子B。首先利用SVM的解$(b_{svm},w_{svm})$来构造这个模型，放缩因子A和平移因子B是待定系数。然后再用通用的logistic regression优化算法，通过迭代优化，得到最终的A和B。一般来说，如果$(b_{svm},w_{svm})$较为合理的话，满足A&gt;0且$B\approx0$。</p><p><img src="http://img.blog.csdn.net/20170706155120610?" alt="这里写图片描述"></p><p>那么，新的logistic regression表达式为：</p><p><img src="http://img.blog.csdn.net/20170706160545395?" alt="这里写图片描述"></p><p>这个表达式看上去很复杂，其实其中的$(b_{svm},w_{svm})$已经在SVM中解出来了，实际上的未知参数只有A和B两个。归纳一下，这种Probabilistic SVM的做法分为三个步骤：</p><p><img src="http://img.blog.csdn.net/20170706161137869?" alt="这里写图片描述"></p><p>这种soft binary classifier方法得到的结果跟直接使用SVM classifier得到的结果可能不一样，这是因为我们引入了系数A和B。一般来说，soft binary classifier效果更好。至于logistic regression的解法，可以选择GD、SGD等等。</p><h3 id="Kernel-Logistic-Regression"><a href="#Kernel-Logistic-Regression" class="headerlink" title="Kernel Logistic Regression"></a>Kernel Logistic Regression</h3><p>上一小节我们介绍的是通过kernel SVM在z空间中求得logistic regression的近似解。如果我们希望直接在z空间中直接求解logistic regression，通过引入kernel，来解决最优化问题，又该怎么做呢？SVM中使用kernel，转化为QP问题，进行求解，但是logistic regression却不是个QP问题，看似好像没有办法利用kernel来解决。</p><p>我们先来看看之前介绍的kernel trick为什么会work，kernel trick就是把z空间的内积转换到x空间中比较容易计算的函数。如果w可以表示为z的线性组合，即$w_*=\sum_{n=1}^N\beta_nz_n$的形式，那么乘积项$w_*^Tz=\sum_{n=1}^N\beta_nz_n^Tz=\sum_{n=1}^N\beta_nK(x_n,x)$，即其中包含了z的内积。也就是w可以表示为z的线性组合是kernel trick可以work的关键。</p><p>我们之前介绍过SVM、PLA包扩logistic regression都可以表示成z的线性组合，这也提供了一种可能，就是将kernel应用到这些问题中去，简化z空间的计算难度。</p><p><img src="http://img.blog.csdn.net/20170706230717900?" alt="这里写图片描述"></p><p>有这样一个理论，对于L2-regularized linear model，如果它的最小化问题形式为如下的话，那么最优解$w_*=\sum_{n=1}^N\beta_nz_n$。</p><p><img src="http://img.blog.csdn.net/20170706231305238?" alt="这里写图片描述"></p><p>下面给出简单的证明，假如最优解$w_*=w_{||}+w_{\bot}$。其中，$w_{||}$和$w_{\bot}$分别是平行z空间和垂直z空间的部分。我们需要证明的是$w_{\bot}=0$。利用反证法，假如$w_{\bot}\neq0$，考虑$w_*$与$w_{||}$的比较。第一步先比较最小化问题的第二项：$err(y,w_*^Tz_n)=err(y_n,(w_{||}+w_{\bot})^Tz_n=err(y_n,w_{||}^Tz_n)$，即第二项是相等的。然后第二步比较第一项：$w_*^Tw_*=w_{||}^Tw_{||}+2w_{||}^Tw_{\bot}+w_{\bot}^Tw_{\bot}&gt;w_{||}^Tw_{||}$，即$w_*$对应的L2-regularized linear model值要比$w_{||}$大，这就说明$w_*$并不是最优解，从而证明$w_{\bot}$必然等于零，即$w_*=\sum_{n=1}^N\beta_nz_n$一定成立，$w_*$一定可以写成z的线性组合形式。</p><p><img src="http://img.blog.csdn.net/20170706233401559?" alt="这里写图片描述"></p><p>经过证明和分析，我们得到了结论是任何L2-regularized linear model都可以使用kernel来解决。</p><p>现在，我们来看看如何把kernel应用在L2-regularized logistic regression上。上面我们已经证明了$w_*$一定可以写成z的线性组合形式，即$w_*=\sum_{n=1}^N\beta_nz_n$。那么我们就无需一定求出$w_*$，而只要求出其中的$\beta_n$就行了。怎么求呢？直接将$w_*=\sum_{n=1}^N\beta_nz_n$代入到L2-regularized logistic regression最小化问题中，得到：</p><p><img src="http://img.blog.csdn.net/20170707075257770?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170707080638752?" alt="这里写图片描述"></p><p>上式中，所有的w项都换成$\beta_n$来表示了，变成了没有条件限制的最优化问题。我们把这种问题称为kernel logistic regression，即引入kernel，将求w的问题转换为求$\beta_n$的问题。</p><p>从另外一个角度来看Kernel Logistic Regression（KLR）：</p><p><img src="http://img.blog.csdn.net/20170707081255169?" alt="这里写图片描述"></p><p>上式中log项里的$\sum_{m=1}^N\beta_mK(x_m,x_n)$可以看成是变量$\beta$和$K(x_m,x_n)$的内积。上式第一项中的$\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(x_n,x_m)$可以看成是关于$\beta$的正则化项$\beta^TK\beta$。所以，KLR是$\beta$的线性组合，其中包含了kernel内积项和kernel regularizer。这与SVM是相似的形式。</p><p>但值得一提的是，KLR中的$\beta_n$与SVM中的$\alpha_n$是有区别的。SVM中的$\alpha_n$大部分为零，SV的个数通常是比较少的；而KLR中的$\beta_n$通常都是非零值。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Kernel Logistic Regression。首先把Soft-Margin SVM解释成Regularized Model，建立二者之间的联系，其实Soft-Margin SVM就是一个L2-regularization，对应着hinge error messure。然后利用它们之间的相似性，讨论了如何利用SVM的解来得到Soft Binary Classification。方法是先得到SVM的解，再在logistic regression中引入参数A和B，迭代训练，得到最佳解。最后介绍了Kernel Logistic Regression，证明L2-regularized logistic regression中，最佳解$w_*$一定可以写成z的线性组合形式，从而可以将kernel引入logistic regression中，使用kernel思想在z空间直接求解L2-regularized logistic regression问题。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170705230810204?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记4 -- Soft-Margin Support Vector Machine</title>
    <link href="https://redstonewill.github.io/2018/03/18/21/"/>
    <id>https://redstonewill.github.io/2018/03/18/21/</id>
    <published>2018-03-18T02:58:08.000Z</published>
    <updated>2018-03-18T03:00:55.902Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170704082607300?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Kernel SVM。先将特征转换和计算内积这两个步骤合并起来，简化计算、提高计算速度，再用Dual SVM的求解方法来解决。Kernel SVM不仅能解决简单的线性分类问题，也可以求解非常复杂甚至是无限多维的分类问题，关键在于核函数的选择，例如线性核函数、多项式核函数和高斯核函数等等。但是，我们之前讲的这些方法都是Hard-Margin SVM，即必须将所有的样本都分类正确才行。这往往需要更多更复杂的特征转换，甚至造成过拟合。本节课将介绍一种Soft-Margin SVM，目的是让分类错误的点越少越好，而不是必须将所有点分类正确，也就是允许有noise存在。这种做法很大程度上不会使模型过于复杂，不会造成过拟合，而且分类效果是令人满意的。</p><h3 id="Motivation-and-Primal-Problem"><a href="#Motivation-and-Primal-Problem" class="headerlink" title="Motivation and Primal Problem"></a>Motivation and Primal Problem</h3><p>上节课我们说明了一点，就是SVM同样可能会造成overfit。原因有两个，一个是由于我们的SVM模型（即kernel）过于复杂，转换的维度太多，过于powerful了；另外一个是由于我们坚持要将所有的样本都分类正确，即不允许错误存在，造成模型过于复杂。如下图所示，左边的图$\Phi_1$是线性的，虽然有几个点分类错误，但是大部分都能完全分开。右边的图$\Phi_4$是四次多项式，所有点都分类正确了，但是模型比较复杂，可能造成过拟合。直观上来说，左边的图是更合理的模型。</p><p><img src="http://img.blog.csdn.net/20170704082607300?" alt="这里写图片描述"></p><p>如何避免过拟合？方法是允许有分类错误的点，即把某些点当作是noise，放弃这些noise点，但是尽量让这些noise个数越少越好。回顾一下我们在机器学习基石笔记中介绍的pocket算法，pocket的思想不是将所有点完全分开，而是找到一条分类线能让分类错误的点最少。而Hard-Margin SVM的目标是将所有点都完全分开，不允许有错误点存在。为了防止过拟合，我们可以借鉴pocket的思想，即允许有犯错误的点，目标是让这些点越少越好。</p><p><img src="http://img.blog.csdn.net/20170704083643796?" alt="这里写图片描述"></p><p>为了引入允许犯错误的点，我们将Hard-Margin SVM的目标和条件做一些结合和修正，转换为如下形式：</p><p><img src="http://img.blog.csdn.net/20170704083927515?" alt="这里写图片描述"></p><p>修正后的条件中，对于分类正确的点，仍需满足$y_n(w^Tz_n+b)\geq 1$，而对于noise点，满足$y_n(w^Tz_n+b)\geq -\infty$，即没有限制。修正后的目标除了$\frac12w^Tw$项，还添加了$y_n\neq sign(w^Tz_n+b)$，即noise点的个数。参数C的引入是为了权衡目标第一项和第二项的关系，即权衡large margin和noise tolerance的关系。</p><p>我们再对上述的条件做修正，将两个条件合并，得到：</p><p><img src="http://img.blog.csdn.net/20170704090202170?" alt="这里写图片描述"></p><p>这个式子存在两个不足的地方。首先，最小化目标中第二项是非线性的，不满足QP的条件，所以无法使用dual或者kernel SVM来计算。然后，对于犯错误的点，有的离边界很近，即error小，而有的离边界很远，error很大，上式的条件和目标没有区分small error和large error。这种分类效果是不完美的。</p><p><img src="http://img.blog.csdn.net/20170704091211117?" alt="这里写图片描述"></p><p>为了改正这些不足，我们继续做如下修正：</p><p><img src="http://img.blog.csdn.net/20170704091355472?" alt="这里写图片描述"></p><p>修正后的表达式中，我们引入了新的参数$\xi_n$来表示每个点犯错误的程度值，$\xi_n\geq0$。通过使用error值的大小代替是否有error，让问题变得易于求解，满足QP形式要求。这种方法类似于我们在机器学习基石笔记中介绍的0/1 error和squared error。这种soft-margin SVM引入新的参数$\xi$。</p><p>至此，最终的Soft-Margin SVM的目标为：</p><p>$$min(b,w,\xi)\ \frac12w^Tw+C\cdot\sum_{n=1}^N\xi_n$$</p><p>条件是：</p><p>$$y_n(w^Tz_n+b)\geq 1-\xi_n$$</p><p>$$\xi_n\geq0$$</p><p>其中，$\xi_n$表示每个点犯错误的程度，$\xi_n=0$，表示没有错误，$\xi_n$越大，表示错误越大，即点距离边界（负的）越大。参数C表示尽可能选择宽边界和尽可能不要犯错两者之间的权衡，因为边界宽了，往往犯错误的点会增加。large C表示希望得到更少的分类错误，即不惜选择窄边界也要尽可能把更多点正确分类；small C表示希望得到更宽的边界，即不惜增加错误点个数也要选择更宽的分类边界。</p><p>与之对应的QP问题中，由于新的参数$\xi_n$的引入，总共参数个数为$\hat d+1+N$，限制条件添加了$\xi_n\geq0$，则总条件个数为2N。</p><p><img src="http://img.blog.csdn.net/20170704100607061?" alt="这里写图片描述"></p><h3 id="Dual-Problem"><a href="#Dual-Problem" class="headerlink" title="Dual Problem"></a>Dual Problem</h3><p>接下来，我们将推导Soft-Margin SVM的对偶dual形式，从而让QP计算更加简单，并便于引入kernel算法。首先，我们把Soft-Margin SVM的原始形式写出来：</p><p><img src="http://img.blog.csdn.net/20170704143333672?" alt="这里写图片描述"></p><p>然后，跟我们在第二节课中介绍的Hard-Margin SVM做法一样，构造一个拉格朗日函数。因为引入了$\xi_n$，原始问题有两类条件，所以包含了两个拉格朗日因子$\alpha_n$和$\beta_n$。拉格朗日函数可表示为如下形式：</p><p><img src="http://img.blog.csdn.net/20170704144256286?" alt="这里写图片描述"></p><p>接下来，我们跟第二节课中的做法一样，利用Lagrange dual problem，将Soft-Margin SVM问题转换为如下形式：</p><p><img src="http://img.blog.csdn.net/20170704144748340?" alt="这里写图片描述"></p><p>根据之前介绍的KKT条件，我们对上式进行简化。上式括号里面的是对拉格朗日函数$L(b,w,\xi,\alpha,\beta)$计算最小值。那么根据梯度下降算法思想：最小值位置满足梯度为零。</p><p>我们先对$\xi_n$做偏微分：</p><p>$$\frac{\partial L}{\partial \xi_n}=0=C-\alpha_n-\beta_n$$</p><p>根据上式，得到$\beta_n=C-\alpha_n$，因为有$\beta_n\geq0$，所以限制$0\leq\alpha_n\leq C$。将$\beta_n=C-\alpha_n$代入到dual形式中并化简，我们发现$\beta_n$和$\xi_n$都被消去了：</p><p><img src="http://img.blog.csdn.net/20170704150122322?" alt="这里写图片描述"></p><p>这个形式跟Hard-Margin SVM中的dual形式是基本一致的，只是条件不同。那么，我们分别令拉个朗日函数L对b和w的偏导数为零，分别得到：</p><p>$$\sum_{n=1}^N\alpha_ny_n=0$$</p><p>$$w=\sum_{n=1}^N\alpha_ny_nz_n$$</p><p>经过化简和推导，最终标准的Soft-Margin SVM的Dual形式如下图所示：</p><p><img src="http://img.blog.csdn.net/20170704151156715?" alt="这里写图片描述"></p><p>Soft-Margin SVM Dual与Hard-Margin SVM Dual基本一致，只有一些条件不同。Hard-Margin SVM Dual中$\alpha_n\geq0$，而Soft-Margin SVM Dual中$0\leq\alpha_n\leq C$，且新的拉格朗日因子$\beta_n=C-\alpha_n$。在QP问题中，Soft-Margin SVM Dual的参数$\alpha_n$同样是N个，但是，条件由Hard-Margin SVM Dual中的N+1个变成2N+1个，这是因为多了N个$\alpha_n$的上界条件。</p><p>对于Soft-Margin SVM Dual这部分推导不太清楚的同学，可以看下第二节课的笔记：<a href="http://blog.csdn.net/red_stone1/article/details/73822768" target="_blank" rel="noopener">台湾大学林轩田机器学习技法课程学习笔记2 – Dual Support Vector Machine</a></p><h3 id="Messages-behind-Soft-Margin-SVM"><a href="#Messages-behind-Soft-Margin-SVM" class="headerlink" title="Messages behind Soft-Margin SVM"></a>Messages behind Soft-Margin SVM</h3><p>推导完Soft-Margin SVM Dual的简化形式后，就可以利用QP，找到Q，p，A，c对应的值，用软件工具包得到$\alpha_n$的值。或者利用核函数的方式，同样可以简化计算，优化分类效果。Soft-Margin SVM Dual计算$\alpha_n$的方法过程与Hard-Margin SVM Dual的过程是相同的。</p><p><img src="http://img.blog.csdn.net/20170704153246056?" alt="这里写图片描述"></p><p>但是如何根据$\alpha_n$的值计算b呢？在Hard-Margin SVM Dual中，有complementary slackness条件：$\alpha_n(1-y_n(w^Tz_n+b))=0$，找到SV，即$\alpha_s&gt;0$的点，计算得到$b=y_s-w^Tz_s$。</p><p>那么，在Soft-Margin SVM Dual中，相应的complementary slackness条件有两个（因为两个拉格朗日因子$\alpha_n$和$\beta_n$）：</p><p>$$\alpha_n(1-\xi_n-y_n(w^Tz_n+b))=0$$</p><p>$$\beta_n\xi_n=(C-\alpha_n)\xi=0$$</p><p>找到SV，即$\alpha_s&gt;0$的点，由于参数$\xi_n$的存在，还不能完全计算出b的值。根据第二个complementary slackness条件，如果令$C-\alpha_n\neq0$，即$\alpha_n\neq C$，则一定有$\xi_n=0$，代入到第一个complementary slackness条件，即可计算得到$b=y_s-w^Tz_s$。我们把$0&lt;\alpha_s&lt;C$的点称为free SV。引入核函数后，b的表达式为：</p><p>$$b=y_s-\sum_{SV}\alpha_ny_nK(x_n,x_s)$$</p><p>上面求解b提到的一个假设是$\alpha_s&lt;C$，这个假设是否一定满足呢？如果没有free SV，所有$\alpha_s$大于零的点都满足$\alpha_s=C$怎么办？一般情况下，至少存在一组SV使$\alpha_s&lt;C$的概率是很大的。如果出现没有free SV的情况，那么b通常会由许多不等式条件限制取值范围，值是不确定的，只要能找到其中满足KKT条件的任意一个b值就可以了。这部分细节比较复杂，不再赘述。</p><p><img src="http://img.blog.csdn.net/20170704161945487?" alt="这里写图片描述"></p><p>接下来，我们看看C取不同的值对margin的影响。例如，对于Soft-Margin Gaussian SVM，C分别取1，10，100时，相应的margin如下图所示：</p><p><img src="http://img.blog.csdn.net/20170704162711810?" alt="这里写图片描述"></p><p>从上图可以看出，C=1时，margin比较粗，但是分类错误的点也比较多，当C越来越大的时候，margin越来越细，分类错误的点也在减少。正如前面介绍的，C值反映了margin和分类正确的一个权衡。C越小，越倾向于得到粗的margin，宁可增加分类错误的点；C越大，越倾向于得到高的分类正确率，宁可margin很细。我们发现，当C值很大的时候，虽然分类正确率提高，但很可能把noise也进行了处理，从而可能造成过拟合。也就是说Soft-Margin Gaussian SVM同样可能会出现过拟合现象，所以参数$(\gamma,C)$的选择非常重要。</p><p>我们再来看看$\alpha_n$取不同值是对应的物理意义。已知$0\leq\alpha_n\leq C$满足两个complementary slackness条件：</p><p>$$\alpha_n(1-\xi_n-y_n(w^Tz_n+b))=0$$</p><p>$$\beta_n\xi_n=(C-\alpha_n)\xi=0$$</p><p>若$\alpha_n=0$，得$\xi_n=0$。$\xi_n=0$表示该点没有犯错，$\alpha_n=0$表示该点不是SV。所以对应的点在margin之外（或者在margin上），且均分类正确。</p><p>若$0&lt;\alpha_n&lt;C$，得$\xi_n=0$，且$y_n(w^Tz_n+b)=1$。$\xi_n=0$表示该点没有犯错，$y_n(w^Tz_n+b)=1$表示该点在margin上。这些点即free SV，确定了b的值。</p><p>若$\alpha_n=C$，不能确定$\xi_n$是否为零，且得到$1-y_n(w^Tz_n+b)=\xi_n$，这个式表示该点偏离margin的程度，$\xi_n$越大，偏离margin的程度越大。只有当$\xi_n=0$时，该点落在margin上。所以这种情况对应的点在margin之内负方向（或者在margin上），有分类正确也有分类错误的。这些点称为bounded SV。</p><p>所以，在Soft-Margin SVM Dual中，根据$\alpha_n$的取值，就可以推断数据点在空间的分布情况。</p><p><img src="http://img.blog.csdn.net/20170704170804624?" alt="这里写图片描述"></p><h3 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h3><p>在Soft-Margin SVM Dual中，kernel的选择、C等参数的选择都非常重要，直接影响分类效果。例如，对于Gaussian SVM，不同的参数$(C,\gamma)$，会得到不同的margin，如下图所示。</p><p><img src="http://img.blog.csdn.net/20170704200115449?" alt="这里写图片描述"></p><p>其中横坐标是C逐渐增大的情况，纵坐标是$\gamma$逐渐增大的情况。不同的$(C,\gamma)$组合，margin的差别很大。那么如何选择最好的$(C,\gamma)$等参数呢？最简单最好用的工具就是validation。</p><p>validation我们在机器学习基石课程中已经介绍过，只需要将由不同$(C,\gamma)$等参数得到的模型在验证集上进行cross validation，选取$E_{cv}$最小的对应的模型就可以了。例如上图中各种$(C,\gamma)$组合得到的$E_{cv}$如下图所示：</p><p><img src="http://img.blog.csdn.net/20170704202325241?" alt="这里写图片描述"></p><p>因为左下角的$E_{cv}(C,\gamma)$最小，所以就选择该$(C,\gamma)$对应的模型。通常来说，$E_{cv}(C,\gamma)$并不是$(C,\gamma)$的连续函数，很难使用最优化选择（例如梯度下降）。一般做法是选取不同的离散的$(C,\gamma)$值进行组合，得到最小的$E_{cv}(C,\gamma)$，其对应的模型即为最佳模型。这种算法就是我们之前在机器学习基石中介绍过的V-Fold cross validation，在SVM中使用非常广泛。</p><p>V-Fold cross validation的一种极限就是Leave-One-Out CV，也就是验证集只有一个样本。对于SVM问题，它的验证集Error满足：</p><p>$$E_{loocv}\leq \frac{SV}{N}$$</p><p>也就是说留一法验证集Error大小不超过支持向量SV占所有样本的比例。下面做简单的证明。令样本总数为N，对这N个点进行SVM分类后得到margin，假设第N个点$(x_N,y_N)$的$\alpha_N=0$，不是SV，即远离margin（正距离）。这时候，如果我们只使用剩下的N-1个点来进行SVM分类，那么第N个点$(x_N,y_N)$必然是分类正确的点，所得的SVM margin跟使用N个点的到的是完全一致的。这是因为我们假设第N个点是non-SV，对SV没有贡献，不影响margin的位置和形状。所以前N-1个点和N个点得到的margin是一样的。</p><p>那么，对于non-SV的点，它的$g^-=g$，即对第N个点，它的Error必然为零：</p><p>$$e_{non-SV}=err(g^-,non-SV)=err(g,non-SV)=0$$</p><p>另一方面，假设第N个点$\alpha_N\neq0$，即对于SV的点，它的Error可能是0，也可能是1，必然有：</p><p>$$e_{SV}\leq1$$</p><p>综上所述，即证明了$E_{loocv}\leq \frac{SV}{N}$。这符合我们之前得到的结论，即只有SV影响margin，non-SV对margin没有任何影响，可以舍弃。</p><p>SV的数量在SVM模型选择中也是很重要的。一般来说，SV越多，表示模型可能越复杂，越有可能会造成过拟合。所以，通常选择SV数量较少的模型，然后在剩下的模型中使用cross-validation，比较选择最佳模型。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Soft-Margin SVM。我们的出发点是与Hard-Margin SVM不同，不一定要将所有的样本点都完全分开，允许有分类错误的点，而使margin比较宽。然后，我们增加了$\xi_n$作为分类错误的惩罚项，根据之前介绍的Dual SVM，推导出了Soft-Margin SVM的QP形式。得到的$\alpha_n$除了要满足大于零，还有一个上界C。接着介绍了通过$\alpha_n$值的大小，可以将数据点分为三种：non-SVs，free SVs，bounded SVs，这种更清晰的物理解释便于数据分析。最后介绍了如何选择合适的SVM模型，通常的办法是cross-validation和利用SV的数量进行筛选。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170704082607300?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记3 -- Kernel Support Vector Machine</title>
    <link href="https://redstonewill.github.io/2018/03/18/20/"/>
    <id>https://redstonewill.github.io/2018/03/18/20/</id>
    <published>2018-03-18T02:54:13.000Z</published>
    <updated>2018-03-18T02:56:48.751Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170630081046212?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了SVM的对偶形式，即dual SVM。Dual SVM也是一个二次规划问题，可以用QP来进行求解。之所以要推导SVM的对偶形式是因为：首先，它展示了SVM的几何意义；然后，从计算上，求解过程“好像”与所在维度$\hat d$无关，规避了$\hat d$很大时难以求解的情况。但是，上节课的最后，我们也提到dual SVM的计算过程其实跟$\hat d$还是有关系的。那么，能不能完全摆脱对$\hat d$的依赖，从而减少SVM计算量呢？这就是我们本节课所要讲的主要内容。</p><h3 id="Kernel-Trick"><a href="#Kernel-Trick" class="headerlink" title="Kernel Trick"></a>Kernel Trick</h3><p>我们上节课推导的dual SVM是如下形式：</p><p><img src="http://img.blog.csdn.net/20170630081046212?" alt="这里写图片描述"></p><p>其中$\alpha$是拉格朗日因子，共N个，这是我们要求解的，而条件共有N+1个。我们来看向量$Q_D$中的$q_{n,m}=y_ny_mz_n^Tz_m$，看似这个计算与$\hat d$无关，但是$z_n^Tz_m$的内积中不得不引入$\hat d$。也就是说，如果$\hat d$很大，计算$z_n^Tz_m$的复杂度也会很高，同样会影响QP问题的计算效率。可以说，$q_{n,m}=y_ny_mz_n^Tz_m$这一步是计算的瓶颈所在。</p><p>其实问题的关键在于$z_n^Tz_m$内积求解上。我们知道，z是由x经过特征转换而来：</p><p>$$z_n^Tz_m=\Phi(x_n)\Phi(x_m)$$</p><p>如果从x空间来看的话，$z_n^Tz_m$分为两个步骤：1. 进行特征转换$\Phi(x_n)$和$\Phi(x_m)$；2. 计算$\Phi(x_n)$与$\Phi(x_m)$的内积。这种先转换再计算内积的方式，必然会引入$\hat d$参数，从而在$\hat d$很大的时候影响计算速度。那么，若把这两个步骤联合起来，是否可以有效地减小计算量，提高计算速度呢？</p><p>我们先来看一个简单的例子，对于二阶多项式转换，各种排列组合为：</p><p><img src="http://img.blog.csdn.net/20170630091408601?" alt="这里写图片描述"></p><p>这里提一下，为了简单起见，我们把$x_0=1$包含进来，同时将二次项$x_1x_2$和$x_2x_1$也包含进来。转换之后再做内积并进行推导，得到：</p><p><img src="http://img.blog.csdn.net/20170630092526343?" alt="这里写图片描述"></p><p>其中$x^Tx’$是x空间中特征向量的内积。所以，$\Phi_2(x)$与$\Phi_2(x’)$的内积的复杂度由原来的$O(d^2)$变成$O(d)$，只与x空间的维度d有关，而与z空间的维度$\hat d$无关，这正是我们想要的！</p><p>至此，我们发现如果把特征转换和z空间计算内积这两个步骤合并起来，有可能会简化计算。因为我们只是推导了二阶多项式会提高运算速度，这个特例并不具有一般推论性。但是，我们还是看到了希望。</p><p>我们把合并特征转换和计算内积这两个步骤的操作叫做Kernel Function，用大写字母K表示。例如刚刚讲的二阶多项式例子，它的kernel function为：</p><p>$$K_{\Phi}(x,x’)=\Phi(x)^T\Phi(x’)$$</p><p>$$K_{\Phi_2}(x,x’)=1+(x^Tx’)+(x^Tx’)^2$$</p><p>有了kernel function之后，我们来看看它在SVM里面如何使用。在dual SVM中，二次项系数$q_{n,m}$中有z的内积计算，就可以用kernel function替换：</p><p>$$q_{n,m}=y_ny_mz_n^Tz_m=y_ny_mK(x_n,x_m)$$</p><p>所以，直接计算出$K(x_n,x_m)$，再代入上式，就能得到$q_{n,m}$的值。</p><p>$q_{n,m}$值计算之后，就能通过QP得到拉格朗日因子$\alpha_n$。然后，下一步就是计算b（取$\alpha_n$&gt;0的点，即SV），b的表达式中包含z，可以作如下推导：</p><p>$$b=y_s-w^Tz_s=y_s-(\sum_{n=1}^N\alpha_ny_nz_n)^Tz_s=y_s-\sum_{n=1}^N\alpha_ny_n(K(x_n,x_s))$$</p><p>这样得到的b就可以用kernel function表示，而与z空间无关。</p><p>最终我们要求的矩$g_{SVM}$可以作如下推导：</p><p>$$g_{SVM}(x)=sign(w^T\Phi(x)+b)=sign((\sum_{n=1}^N\alpha_ny_nz_n)^Tz+b)=sign(\sum_{n=1}^N\alpha_ny_n(K(x_n,x))+b)$$</p><p>至此，dual SVM中我们所有需要求解的参数都已经得到了，而且整个计算过程中都没有在z空间作内积，即与z无关。我们把这个过程称为kernel trick，也就是把特征转换和计算内积两个步骤结合起来，用kernel function来避免计算过程中受$\hat d$的影响，从而提高运算速度。</p><p><img src="http://img.blog.csdn.net/20170630105432632?" alt="这里写图片描述"></p><p>那么总结一下，引入kernel funtion后，SVM算法变成：</p><p><img src="http://img.blog.csdn.net/20170630105722080?" alt="这里写图片描述"></p><p>分析每个步骤的时间复杂度为：</p><p><img src="http://img.blog.csdn.net/20170630110019354?" alt="这里写图片描述"></p><p>我们把这种引入kernel function的SVM称为kernel SVM，它是基于dual SVM推导而来的。kernel SVM同样只用SV（$\alpha_n$&gt;0）就能得到最佳分类面，而且整个计算过程中摆脱了$\hat d$的影响，大大提高了计算速度。</p><h3 id="Polynomial-Kernel"><a href="#Polynomial-Kernel" class="headerlink" title="Polynomial Kernel"></a>Polynomial Kernel</h3><p>我们刚刚通过一个特殊的二次多项式导出了相对应的kernel，其实二次多项式的kernel形式是多种的。例如，相应系数的放缩构成完全平方公式等。下面列举了几种常用的二次多项式kernel形式：</p><p><img src="http://img.blog.csdn.net/20170703080852707?" alt="这里写图片描述"></p><p>比较一下，第一种$\Phi_2(x)$（蓝色标记）和第三种$\Phi_2(x)$（绿色标记）从某种角度来说是一样的，因为都是二次转换，对应到同一个z空间。但是，它们系数不同，内积就会有差异，那么就代表有不同的距离，最终可能会得到不同的SVM margin。所以，系数不同，可能会得到不同的SVM分界线。通常情况下，第三种$\Phi_2(x)$（绿色标记）简单一些，更加常用。</p><p><img src="http://img.blog.csdn.net/20170703081851356?" alt="这里写图片描述"></p><p>不同的转换，对应到不同的几何距离，得到不同的距离，这是什么意思呢？举个例子，对于我们之前介绍的一般的二次多项式kernel，它的SVM margin和对应的SV如下图（中）所示。对于上面介绍的完全平方公式形式，自由度$\gamma=0.001$，它的SVM margin和对应的SV如下图（左）所示。比较发现，这种SVM margin比较简单一些。对于自由度$\gamma=1000$，它的SVM margin和对应的SV如下图（右）所示。与前两种比较，margin和SV都有所不同。</p><p><img src="http://img.blog.csdn.net/20170703082844954?" alt="这里写图片描述"></p><p>通过改变不同的系数，得到不同的SVM margin和SV，如何选择正确的kernel，非常重要。</p><p>归纳一下，引入$\zeta\geq 0$和$\gamma&gt;0$，对于Q次多项式一般的kernel形式可表示为：</p><p><img src="http://img.blog.csdn.net/20170703083722140?" alt="这里写图片描述"></p><p>所以，使用高阶的多项式kernel有两个优点：</p><ul><li><p><strong>得到最大SVM margin，SV数量不会太多，分类面不会太复杂，防止过拟合，减少复杂度</strong></p></li><li><p><strong>计算过程避免了对$\hat d$的依赖，大大简化了计算量。</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170703084625483?" alt="这里写图片描述"></p><p>顺便提一下，当多项式阶数Q=1时，那么对应的kernel就是线性的，即本系列课程第一节课所介绍的内容。对于linear kernel，计算方法是简单的，而且也是我们解决SVM问题的首选。还记得机器学习基石课程中介绍的奥卡姆剃刀定律（Occam’s Razor）吗？</p><h3 id="Gaussian-Kernel"><a href="#Gaussian-Kernel" class="headerlink" title="Gaussian Kernel"></a>Gaussian Kernel</h3><p>刚刚我们介绍的Q阶多项式kernel的阶数是有限的，即特征转换的$\hat d$是有限的。但是，如果是无限多维的转换$\Phi(x)$，是否还能通过kernel的思想，来简化SVM的计算呢？答案是肯定的。</p><p>先举个例子，简单起见，假设原空间是一维的，只有一个特征x，我们构造一个kernel function为高斯函数：</p><p>$$K(x,x’)=e^{-(x-x’)^2}$$</p><p>构造的过程正好与二次多项式kernel的相反，利用反推法，先将上式分解并做泰勒展开：</p><p><img src="http://img.blog.csdn.net/20170703093947389?" alt="这里写图片描述"></p><p>将构造的K(x,x’)推导展开为两个$\Phi(x)$和$\Phi(x’)$的乘积，其中：</p><p>$$\Phi(x)=e^{-x^2}\cdot (1,\sqrt \frac{2}{1!}x,\sqrt \frac{2^2}{2!}x^2,\cdots)$$</p><p>通过反推，我们得到了$\Phi(x)$，$\Phi(x)$是无限多维的，它就可以当成特征转换的函数，且$\hat d$是无限的。这种$\Phi(x)$得到的核函数即为Gaussian kernel。</p><p>更一般地，对于原空间不止一维的情况（d&gt;1），引入缩放因子$\gamma&gt;0$，它对应的Gaussian kernel表达式为：</p><p>$$K(x,x’)=e^{-\gamma||x-x’||^2}$$</p><p>那么引入了高斯核函数，将有限维度的特征转换拓展到无限的特征转换中。根据本节课上一小节的内容，由K，计算得到$\alpha_n$和b，进而得到矩$g_{SVM}$。将其中的核函数K用高斯核函数代替，得到：</p><p>$$g_{SVM}(x)=sign(\sum_{SV}\alpha_ny_nK(x_n,x)+b)=sign(\sum_{SV}\alpha_ny_ne^{(-\gamma||x-x_n||^2)}+b)$$</p><p>通过上式可以看出，$g_{SVM}$有n个高斯函数线性组合而成，其中n是SV的个数。而且，每个高斯函数的中心都是对应的SV。通常我们也把高斯核函数称为径向基函数（Radial Basis Function, RBF）。</p><p><img src="http://img.blog.csdn.net/20170703101252712?" alt="这里写图片描述"></p><p>总结一下，kernel SVM可以获得large-margin的hyperplanes，并且可以通过高阶的特征转换使$E_{in}$尽可能地小。kernel的引入大大简化了dual SVM的计算量。而且，Gaussian kernel能将特征转换扩展到无限维，并使用有限个SV数量的高斯函数构造出矩$g_{SVM}$。</p><p><img src="http://img.blog.csdn.net/20170703102031293?" alt="这里写图片描述"></p><p>值得注意的是，缩放因子$\gamma$取值不同，会得到不同的高斯核函数，hyperplanes不同，分类效果也有很大的差异。举个例子，$\gamma$分别取1, 10, 100时对应的分类效果如下：</p><p><img src="http://img.blog.csdn.net/20170703103142607?" alt="这里写图片描述"></p><p>从图中可以看出，当$\gamma$比较小的时候，分类线比较光滑，当$\gamma$越来越大的时候，分类线变得越来越复杂和扭曲，直到最后，分类线变成一个个独立的小区域，像小岛一样将每个样本单独包起来了。为什么会出现这种区别呢？这是因为$\gamma$越大，其对应的高斯核函数越尖瘦，那么有限个高斯核函数的线性组合就比较离散，分类效果并不好。所以，SVM也会出现过拟合现象，$\gamma$的正确选择尤为重要，不能太大。</p><h3 id="Comparison-of-Kernels"><a href="#Comparison-of-Kernels" class="headerlink" title="Comparison of Kernels"></a>Comparison of Kernels</h3><p>目前为止，我们已经介绍了几种kernel，下面来对几种kernel进行比较。</p><p>首先，Linear Kernel是最简单最基本的核，平面上对应一条直线，三维空间里对应一个平面。Linear Kernel可以使用上一节课介绍的Dual SVM中的QP直接计算得到。</p><p><img src="http://img.blog.csdn.net/20170703112425643?" alt="这里写图片描述"></p><p>Linear Kernel的优点是计算简单、快速，可以直接使用QP快速得到参数值，而且从视觉上分类效果非常直观，便于理解；缺点是如果数据不是线性可分的情况，Linear Kernel就不能使用了。</p><p><img src="http://img.blog.csdn.net/20170703113330995?" alt="这里写图片描述"></p><p>然后，Polynomial Kernel的hyperplanes是由多项式曲线构成。</p><p><img src="http://img.blog.csdn.net/20170703131711028?" alt="这里写图片描述"></p><p>Polynomial Kernel的优点是阶数Q可以灵活设置，相比linear kernel限制更少，更贴近实际样本分布；缺点是当Q很大时，K的数值范围波动很大，而且参数个数较多，难以选择合适的值。</p><p><img src="http://img.blog.csdn.net/20170703132120216?" alt="这里写图片描述"></p><p>对于Gaussian Kernel，表示为高斯函数形式。</p><p><img src="http://img.blog.csdn.net/20170703133450677?" alt="这里写图片描述"></p><p>Gaussian Kernel的优点是边界更加复杂多样，能最准确地区分数据样本，数值计算K值波动较小，而且只有一个参数，容易选择；缺点是由于特征转换到无限维度中，w没有求解出来，计算速度要低于linear kernel，而且可能会发生过拟合。</p><p><img src="http://img.blog.csdn.net/20170703134358134?" alt="这里写图片描述"></p><p>除了这三种kernel之外，我们还可以使用其它形式的kernel。首先，我们考虑kernel是什么？实际上kernel代表的是两笔资料x和x’，特征变换后的相似性即内积。但是不能说任何计算相似性的函数都可以是kernel。有效的kernel还需满足几个条件：</p><ul><li><p><strong>K是对称的</strong></p></li><li><p><strong>K是半正定的</strong></p></li></ul><p>这两个条件不仅是必要条件，同时也是充分条件。所以，只要我们构造的K同时满足这两个条件，那它就是一个有效的kernel。这被称为Mercer 定理。事实上，构造一个有效的kernel是比较困难的。</p><p><img src="http://img.blog.csdn.net/20170703135827622?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Kernel Support Vector Machine。首先，我们将特征转换和计算内积的操作合并到一起，消除了$\hat d$的影响，提高了计算速度。然后，分别推导了Polynomial Kernel和Gaussian Kernel，并列举了各自的优缺点并做了比较。对于不同的问题，应该选择合适的核函数进行求解，以达到最佳的分类效果。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170630081046212?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记2 -- Dual Support Vector Machine</title>
    <link href="https://redstonewill.github.io/2018/03/18/19/"/>
    <id>https://redstonewill.github.io/2018/03/18/19/</id>
    <published>2018-03-18T02:50:33.000Z</published>
    <updated>2018-03-18T02:52:29.417Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170627195157734?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了线性支持向量机（Linear Support Vector Machine）。Linear SVM的目标是找出最“胖”的分割线进行正负类的分离，方法是使用二次规划来求出分类线。本节课将从另一个方面入手，研究对偶支持向量机（Dual Support Vector Machine），尝试从新的角度计算得出分类线，推广SVM的应用范围。</p><h3 id="Motivation-of-Dual-SVM"><a href="#Motivation-of-Dual-SVM" class="headerlink" title="Motivation of Dual SVM"></a>Motivation of Dual SVM</h3><p>首先，我们回顾一下，对于非线性SVM，我们通常可以使用非线性变换将变量从x域转换到z域中。然后，在z域中，根据上一节课的内容，使用线性SVM解决问题即可。上一节课我们说过，使用SVM得到large-margin，减少了有效的VC Dimension，限制了模型复杂度；另一方面，使用特征转换，目的是让模型更复杂，减小$E_{in}$。所以说，非线性SVM是把这两者目的结合起来，平衡这两者的关系。那么，特征转换下，求解QP问题在z域中的维度设为$\hat d +1$，如果模型越复杂，则$\hat d +1$越大，相应求解这个QP问题也变得很困难。当$\hat d$无限大的时候，问题将会变得难以求解，那么有没有什么办法可以解决这个问题呢？一种方法就是使SVM的求解过程不依赖$\hat d$，这就是我们本节课所要讨论的主要内容。</p><p><img src="http://img.blog.csdn.net/20170627195157734?" alt="这里写图片描述"></p><p>比较一下，我们上一节课所讲的Original SVM二次规划问题的变量个数是$\hat d +1$，有N个限制条件；而本节课，我们把问题转化为对偶问题（’Equivalent’ SVM），同样是二次规划，只不过变量个数变成N个，有N+1个限制条件。这种对偶SVM的好处就是问题只跟N有关，与$\hat d$无关，这样就不存在上文提到的当$\hat d$无限大时难以求解的情况。</p><p><img src="http://img.blog.csdn.net/20170627195918132?" alt="这里写图片描述"></p><p>如何把问题转化为对偶问题（’Equivalent’ SVM），其中的数学推导非常复杂，本文不做详细数学论证，但是会从概念和原理上进行简单的推导。</p><p>还记得我们在《机器学习基石》课程中介绍的Regularization中，在最小化$E_{in}$的过程中，也添加了限制条件：$w^Tw\leq C$。我们的求解方法是引入拉格朗日因子$\lambda$，将有条件的最小化问题转换为无条件的最小化问题：$min\ E_{aug}(w)=E_{in}(w)+\frac{\lambda}{N}w^Tw$，最终得到的w的最优化解为：</p><p>$$\nabla E_{in}(w)+\frac{2\lambda}{N}w=0$$</p><p>所以，在regularization问题中，$\lambda$是已知常量，求解过程变得容易。那么，对于dual SVM问题，同样可以引入$\lambda$，将条件问题转换为非条件问题，只不过$\lambda$是未知参数，且个数是N，需要对其进行求解。</p><p><img src="http://img.blog.csdn.net/20170627202803919?" alt="这里写图片描述"></p><p>如何将条件问题转换为非条件问题？上一节课我们介绍的SVM中，目标是：$min\ \frac12w^Tw$，条件是：$y_n(w^Tz_n+b)\geq 1,\ for\ n=1,2,\cdots,N$。首先，我们令拉格朗日因子为$\alpha_n$（区别于regularization），构造一个函数：</p><p>$$L(b,w,\alpha)=\frac12w^Tw+\sum_{n=1}^N\alpha_n(1-y_n(w^Tz_n+b))$$</p><p>这个函数右边第一项是SVM的目标，第二项是SVM的条件和拉格朗日因子$\alpha_n$的乘积。我们把这个函数称为拉格朗日函数，其中包含三个参数：b，w，$\alpha_n$。</p><p><img src="http://img.blog.csdn.net/20170627215728198?" alt="这里写图片描述"></p><p>下面，我们利用拉格朗日函数，把SVM构造成一个非条件问题：</p><p><img src="http://img.blog.csdn.net/20170627225749786?" alt="这里写图片描述"></p><p>该最小化问题中包含了最大化问题，怎么解释呢？首先我们规定拉格朗日因子$\alpha_n\geq0$，根据SVM的限定条件可得：$(1-y_n(w^Tz_n+b))\leq0$，如果没有达到最优解，即有不满足$(1-y_n(w^Tz_n+b))\leq0$的情况，因为$\alpha_n\geq0$，那么必然有$\sum_n\alpha_n(1-y_n(w^Tz_n+b))\geq0$。对于这种大于零的情况，其最大值是无解的。如果对于所有的点，均满足$(1-y_n(w^Tz_n+b))\leq0$，那么必然有$\sum_n\alpha_n(1-y_n(w^Tz_n+b))\leq0$，则当$\sum_n\alpha_n(1-y_n(w^Tz_n+b))=0$时，其有最大值，最大值就是我们SVM的目标：$\frac12w^Tw$。因此，这种转化为非条件的SVM构造函数的形式是可行的。</p><h3 id="Lagrange-Dual-SVM"><a href="#Lagrange-Dual-SVM" class="headerlink" title="Lagrange Dual SVM"></a>Lagrange Dual SVM</h3><p>现在，我们已经将SVM问题转化为与拉格朗日因子$\alpha_n$有关的最大最小值形式。已知$\alpha_n\geq0$，那么对于任何固定的$\alpha’$，且$\alpha_n’\geq0$，一定有如下不等式成立：</p><p><img src="http://img.blog.csdn.net/20170628075805823?" alt="这里写图片描述"></p><p>对上述不等式右边取最大值，不等式同样成立：</p><p><img src="http://img.blog.csdn.net/20170628080019645?" alt="这里写图片描述"></p><p>上述不等式表明，我们对SVM的min和max做了对调，满足这样的关系，这叫做Lagrange dual problem。不等式右边是SVM问题的下界，我们接下来的目的就是求出这个下界。</p><p>已知$\geq$是一种弱对偶关系，在二次规划QP问题中，如果满足以下三个条件：</p><ul><li><p><strong>函数是凸的（convex primal）</strong></p></li><li><p><strong>函数有解（feasible primal）</strong></p></li><li><p><strong>条件是线性的（linear constraints）</strong></p></li></ul><p>那么，上述不等式关系就变成强对偶关系，$\geq$变成=，即一定存在满足条件的解$(b,w,\alpha)$，使等式左边和右边都成立，SVM的解就转化为右边的形式。</p><p>经过推导，SVM对偶问题的解已经转化为无条件形式：</p><p><img src="http://img.blog.csdn.net/20170628083339835?" alt="这里写图片描述"></p><p>其中，上式括号里面的是对拉格朗日函数$L(b,w,\alpha)$计算最小值。那么根据梯度下降算法思想：最小值位置满足梯度为零。首先，令$L(b,w,\alpha)$对参数b的梯度为零：</p><p>$$\frac{\partial L(b,w,\alpha)}{\partial b}=0=-\sum_{n=1}^N\alpha_ny_n$$</p><p>也就是说，最优解一定满足$\sum_{n=1}^N\alpha_ny_n=0$。那么，我们把这个条件代入计算max条件中（与$\alpha_n\geq0$同为条件），并进行化简：</p><p><img src="http://img.blog.csdn.net/20170628084913755?" alt="这里写图片描述"></p><p>这样，SVM表达式消去了b，问题化简了一些。然后，再根据最小值思想，令$L(b,w,\alpha)$对参数w的梯度为零：</p><p>$$\frac{\partial L(b,w,\alpha}{\partial w}=0=w-\sum_{n=1}^N\alpha_ny_nz_n$$</p><p>即得到：</p><p>$$w=\sum_{n=1}^N\alpha_ny_nz_n$$</p><p>也就是说，最优解一定满足$w=\sum_{n=1}^N\alpha_ny_nz_n$。那么，同样我们把这个条件代入并进行化简：</p><p><img src="http://img.blog.csdn.net/20170628090217878?" alt="这里写图片描述"></p><p>这样，SVM表达式消去了w，问题更加简化了。这时候的条件有3个：</p><ul><li><p>all $\alpha_n\geq0$</p></li><li><p>$\sum_{n=1}^N\alpha_ny_n=0$</p></li><li><p>$w=\sum_{n=1}^N\alpha_ny_nz_n$</p></li></ul><p>SVM简化为只有$\alpha_n$的最佳化问题，即计算满足上述三个条件下，函数$-\frac12||\sum_{n=1}^N\alpha_ny_nz_n||^2+\sum_{n=1}^N\alpha_n$最小值时对应的$\alpha_n$是多少。</p><p>总结一下，SVM最佳化形式转化为只与$\alpha_n$有关：</p><p><img src="http://img.blog.csdn.net/20170628091730046?" alt="这里写图片描述"></p><p>其中，满足最佳化的条件称之为Karush-Kuhn-Tucker(KKT)：</p><p><img src="http://img.blog.csdn.net/20170628091859925?" alt="这里写图片描述"></p><p>在下一部分中，我们将利用KKT条件来计算最优化问题中的$\alpha$，进而得到b和w。</p><h3 id="Solving-Dual-SVM"><a href="#Solving-Dual-SVM" class="headerlink" title="Solving Dual SVM"></a>Solving Dual SVM</h3><p>上面我们已经得到了dual SVM的简化版了，接下来，我们继续对它进行一些优化。首先，将max问题转化为min问题，再做一些条件整理和推导，得到：</p><p><img src="http://img.blog.csdn.net/20170628125859622?" alt="这里写图片描述"></p><p>显然，这是一个convex的QP问题，且有N个变量$\alpha_n$，限制条件有N+1个。则根据上一节课讲的QP解法，找到Q，p，A，c对应的值，用软件工具包进行求解即可。</p><p><img src="http://img.blog.csdn.net/20170628131919549?" alt="这里写图片描述"></p><p>求解过程很清晰，但是值得注意的是，$q_{n,m}=y_ny_mz^T_nz_m$，大部分值是非零的，称为dense。当N很大的时候，例如N=30000，那么对应的$Q_D$的计算量将会很大，存储空间也很大。所以一般情况下，对dual SVM问题的矩阵$Q_D$，需要使用一些特殊的方法，这部分内容就不再赘述了。</p><p><img src="http://img.blog.csdn.net/20170628140938851?" alt="这里写图片描述"></p><p>得到$\alpha_n$之后，再根据之前的KKT条件，就可以计算出w和b了。首先利用条件$w=\sum\alpha_ny_nz_n$得到w，然后利用条件$\alpha_n(1-y_n(w^Tz_n+b))=0$，取任一$\alpha_n\neq0$即$\alpha_n$&gt;0的点，得到$1-y_n(w^Tz_n+b)=0$，进而求得$b=y_n-w^Tz_n$。</p><p><img src="http://img.blog.csdn.net/20170628144714622?" alt="这里写图片描述"></p><p>值得注意的是，计算b值，$\alpha_n$&gt;0时，有$y_n(w^Tz_n+b)=1$成立。$y_n(w^Tz_n+b)=1$正好表示的是该点在SVM分类线上，即fat boundary。也就是说，满足$\alpha_n$&gt;0的点一定落在fat boundary上，这些点就是Support Vector。这是一个非常有趣的特性。</p><h3 id="Messages-behind-Dual-SVM"><a href="#Messages-behind-Dual-SVM" class="headerlink" title="Messages behind Dual SVM"></a>Messages behind Dual SVM</h3><p>回忆一下，上一节课中，我们把位于分类线边界上的点称为support vector（candidates）。本节课前面介绍了$\alpha_n$&gt;0的点一定落在分类线边界上，这些点称之为support vector（注意没有candidates）。也就是说分类线上的点不一定都是支持向量，但是满足$\alpha_n$&gt;0的点，一定是支持向量。</p><p><img src="http://img.blog.csdn.net/20170628153020643?" alt="这里写图片描述"></p><p>SV只由$\alpha_n$&gt;0的点决定，根据上一部分推导的w和b的计算公式，我们发现，w和b仅由SV即$\alpha_n$&gt;0的点决定，简化了计算量。这跟我们上一节课介绍的分类线只由“胖”边界上的点所决定是一个道理。也就是说，样本点可以分成两类：一类是support vectors，通过support vectors可以求得fattest hyperplane；另一类不是support vectors，对我们求得fattest hyperplane没有影响。</p><p><img src="http://img.blog.csdn.net/20170628153642180?" alt="这里写图片描述"></p><p>回过头来，我们来比较一下SVM和PLA的w公式：</p><p><img src="http://img.blog.csdn.net/20170628154103081?" alt="这里写图片描述"></p><p>我们发现，二者在形式上是相似的。$w_{SVM}$由fattest hyperplane边界上所有的SV决定，$w_{PLA}$由所有当前分类错误的点决定。$w_{SVM}$和$w_{PLA}$都是原始数据点$y_nz_n$的线性组合形式，是原始数据的代表。</p><p><img src="http://img.blog.csdn.net/20170628154618376?" alt="这里写图片描述"></p><p>总结一下，本节课和上节课主要介绍了两种形式的SVM，一种是Primal Hard-Margin SVM，另一种是Dual Hard_Margin SVM。Primal Hard-Margin SVM有$\hat d+1$个参数，有N个限制条件。当$\hat d+1$很大时，求解困难。而Dual Hard_Margin SVM有N个参数，有N+1个限制条件。当数据量N很大时，也同样会增大计算难度。两种形式都能得到w和b，求得fattest hyperplane。通常情况下，如果N不是很大，一般使用Dual SVM来解决问题。</p><p><img src="http://img.blog.csdn.net/20170628155410408?" alt="这里写图片描述"></p><p>这节课提出的Dual SVM的目的是为了避免计算过程中对$\hat d$的依赖，而只与N有关。但是，Dual SVM是否真的消除了对$\hat d$的依赖呢？其实并没有。因为在计算$q_{n,m}=y_ny_mz_n^Tz_m$的过程中，由z向量引入了$\hat d$，实际上复杂度已经隐藏在计算过程中了。所以，我们的目标并没有实现。下一节课我们将继续研究探讨如何消除对$\hat d$的依赖。</p><p><img src="http://img.blog.csdn.net/20170628160120576?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了SVM的另一种形式：Dual SVM。我们这样做的出发点是为了移除计算过程对$\hat d$的依赖。Dual SVM的推导过程是通过引入拉格朗日因子$\alpha$，将SVM转化为新的非条件形式。然后，利用QP，得到最佳解的拉格朗日因子$\alpha$。再通过KKT条件，计算得到对应的w和b。最终求得fattest hyperplane。下一节课，我们将解决Dual SVM计算过程中对$\hat d$的依赖问题。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170627195157734?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记1 -- Linear Support Vector Machine</title>
    <link href="https://redstonewill.github.io/2018/03/18/18/"/>
    <id>https://redstonewill.github.io/2018/03/18/18/</id>
    <published>2018-03-18T02:25:58.000Z</published>
    <updated>2018-03-18T02:52:42.883Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170621075532782?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>关于台湾大学林轩田老师的《机器学习基石》课程，我们已经总结了16节课的笔记。这里附上基石第一节课的博客地址：</p><p><a href="http://blog.csdn.net/red_stone1/article/details/72899485" target="_blank" rel="noopener">台湾大学林轩田机器学习基石课程学习笔记1 – The Learning Problem</a></p><p>本系列同样分成16节课，将会介绍《机器学习基石》的进阶版《机器学习技法》，更深入地探讨机器学习一些高级算法和技巧。</p><h3 id="Large-Margin-Separating-Hyperplane"><a href="#Large-Margin-Separating-Hyperplane" class="headerlink" title="Large-Margin Separating Hyperplane"></a>Large-Margin Separating Hyperplane</h3><p>回顾一下我们之前介绍了linear classification，对于线性可分的情况，我们可以使用PLA/pocket算法在平面或者超平面上把正负类分开。</p><p><img src="http://img.blog.csdn.net/20170621075532782?" alt="这里写图片描述"></p><p>例如对平面2D这种情况，我们可以找到一条直线，能将正类和负类完全分开。但是，这样的直线通常不止一条，如下图所示。那么，下图中的三条分类线都能将数据分开，但是哪条线更好呢？</p><p><img src="http://img.blog.csdn.net/20170621080123771?" alt="这里写图片描述"></p><p>这三条直线都是由PLA/pocket算法不断修正错误点而最终产生的，整个确定直线形状的过程是随机的。单从分类效果上看，这三条直线都满足要求，而且都满足VC bound要求，模型复杂度$\Omega(H)$是一样的，即具有一定的泛化能力。但是，如果要选择的话，凭第一感觉，我们还是会选择第三条直线，感觉它的分类效果更好一些。那这又是为什么呢？</p><p>先给个简单解释，一般情况下，训练样本外的测量数据应该分布在训练样本附近，但与训练样本的位置有一些偏差。若要保证对未知的测量数据也能进行正确分类，最好让分类直线距离正类负类的点都有一定的距离。这样能让每个样本点附近的圆形区域是“安全”的。圆形区域越大，表示分类直线对测量数据误差的容忍性越高，越“安全”。</p><p><img src="http://img.blog.csdn.net/20170621082437517?" alt="这里写图片描述"></p><p>如上图所示，左边的点距离分类直线的最小距离很小，它的圆形区域很小。那么，这种情况下，分类线对测量数据误差的容忍性就很差，测量数据与样本数据稍有偏差，很有可能就被误分。而右边的点距离分类直线的最小距离更大一些，其圆形区域也比较大。这种情况下，分类线对测量数据误差的容忍性就相对来说大很多，不容易误分。也就是说，左边分类线和右边分类线的最大区别是对这类测量误差的容忍度不同。</p><p>那么，如果每一笔训练资料距离分类线越远的话，就表示分类型可以忍受更多的测量误差（noise）。我们之前在《机器学习基石》中介绍过，noise是造成overfitting的主要原因，而测量误差也是一种noise。所以，如果分类线对测量误差的容忍性越好的话，表示这是一条不错的分类线。那么，我们的目标就是找到这样一条最“健壮”的线，即距离数据点越远越好。</p><p><img src="http://img.blog.csdn.net/20170621092003010?" alt="这里写图片描述"></p><p>上面我们用圆形区域表示分类线能够容忍多少误差，也就相当于计算点到直线的距离。距离越大，表示直线越“胖”，越能容忍误差；距离越小，表示直线越“瘦”，越不能容忍误差。越胖越好（像杨贵妃那样的哦~）。</p><p><img src="http://img.blog.csdn.net/20170621092840092?" alt="这里写图片描述"></p><p>如何定义分类线有多胖，就是看距离分类线最近的点与分类线的距离，我们把它用margin表示。分类线由权重w决定，目的就是找到使margin最大时对应的w值。整体来说，我们的目标就是找到这样的分类线并满足下列条件：</p><ul><li><p><strong>分类正确，即$y_nw^Tx_n&gt;0$</strong></p></li><li><p><strong>margin最大化</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170621094248085?" alt="这里写图片描述"></p><h3 id="Standard-Large-Margin-Problem"><a href="#Standard-Large-Margin-Problem" class="headerlink" title="Standard Large-Margin Problem"></a>Standard Large-Margin Problem</h3><p>要让margin最大，即让离分类线最近的点到分类线距离最大，我们先来看一下如何计算点到分类线的距离。</p><p>首先，我们将权重$w(w_0,w_1,\cdots,w_d)$中的$w_0$拿出来，用b表示。同时省去$x_0$项。这样，hypothesis就变成了$h(x)=sign(w^Tx+b)$。</p><p><img src="http://img.blog.csdn.net/20170621100716605?" alt="这里写图片描述"></p><p>下面，利用图解的方式，详细推导如何计算点到分类平面的距离：</p><p><img src="http://img.blog.csdn.net/20170621101004571?" alt="这里写图片描述"></p><p>如上图所示，平面上有两个点：x’和x’’。因为这两个点都在分类平面上，所以它们都满足：</p><p>$$w^Tx’+b=0$$</p><p>$$w^Tx’’+b=0$$</p><p>同时可以得到：$w^Tx’=-b$，$w^Tx’’=-b$，则有：</p><p>$$w^T(x’’-x’)=w^Tx’’-w^Tx’=-b-(-b)=0$$</p><p>(x’’-x’)是平面上的任一向量，(x’’-x’)与w内积为0，表示(x’’-x’)垂直于w，那么w就是平面的法向量。</p><p>现在，若要计算平面外一点x到该平面的距离，做法是只要将向量(x-x’)投影到垂直于该平面的方向（即w方向）上就可以了。那么，令(x’’-x’)与w的夹角为$\theta$，距离就可以表示为：</p><p>$$distance(x,b,w)=|(x-x’)cos(\theta)|=|\ ||x-x’||\cdot \frac{(x-x’)w}{||x-x’||\cdot ||w||}|=\frac1{||w||}|w^Tx-w^Tx’|$$</p><p>代入$w^Tx’=-b$，可得：</p><p>$$distance(x,b,w)=\frac1{||w||}|w^Tx+b|$$</p><p>点到分类面（Separating Hyperplane）的距离已经算出来了。基于这个分类面，所有的点均满足：$y_n(w^Tx_n+b)&gt;0$，表示所有点都分类正确，则distance公式就可以变换成：</p><p>$$distance(x,b,w)=\frac1{||w||}y_n(w^Tx_n+b)$$</p><p>那么，我们的目标形式就转换为：</p><p><img src="http://img.blog.csdn.net/20170621112518701?" alt="这里写图片描述"></p><p>对上面的式子还不容易求解，我们继续对它进行简化。我们知道分类面$w^Tx+b=0$和$3w^Tx+3b=0$其实是一样的。也就是说，对w和b进行同样的缩放还会得到同一分类面。所以，为了简化计算，我们令距离分类满最近的点满足$y_n(w^Tx_n+b)=1$。那我们所要求的margin就变成了:</p><p>$$margin(b,w)=\frac1{||w||}$$</p><p>这样，目标形式就简化为：</p><p><img src="http://img.blog.csdn.net/20170621134411889?" alt="这里写图片描述"></p><p>这里可以省略条件：$y_n(w^Tx_n+b)&gt;0$，因为满足条件$y_n(w^Tx_n+b)=1$必然满足大于零的条件。我们的目标就是根据这个条件，计算$\frac1{||w||}$的最大值。</p><p>刚刚我们讲的距离分类满最近的点满足$y_n(w^Tx_n+b)=1$，也就是说对所有的点满足$y_n(w^Tx_n+b)\geq 1$。另外，因为最小化问题我们最熟悉也最好解，所以可以把目标$\frac1{||w||}$最大化转化为计算$\frac12w^Tw$的最小化问题。</p><p><img src="http://img.blog.csdn.net/20170621140211481?" alt="这里写图片描述"></p><p>如上图所示，最终的条件就是$y_n(w^Tx_n+b)\geq 1$，而我们的目标就是最小化$\frac12w^Tw$值。</p><h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><p>现在，条件和目标变成：</p><p><img src="http://img.blog.csdn.net/20170621143428452?" alt="这里写图片描述"></p><p>现在，举个例子，假如平面上有四个点，两个正类，两个负类：</p><p><img src="http://img.blog.csdn.net/20170621143607559?" alt="这里写图片描述"></p><p>不同点的坐标加上条件$y_n(w^Tx_n+b)\geq 1$，可以得到：</p><p><img src="http://img.blog.csdn.net/20170621143829925?" alt="这里写图片描述"></p><p>最终，我们得到的条件是：</p><p>$$w_1\geq +1$$</p><p>$$w_2\leq -1$$</p><p>而我们的目标是：</p><p>$$min\ \frac12w^Tw=\frac12(w_1^2+w_2^2)\geq 1$$</p><p>目标最小值为1，即$w_1=1,\ w_2=-1,\ b=-1$，那么这个例子就得到了最佳分类面的解，如图所示，且$margin(b,w)=\frac1{||w||}=\frac1{\sqrt2}$。分类面的表达式为：</p><p>$$x_1-x_2-1=0$$</p><p>最终我们得到的矩的表达式为：</p><p>$$g_{SVM}(x)=sign(x_1-x_2-1)$$</p><p>Support Vector Machine(SVM)这个名字从何而来？为什么把这种分类面解法称为支持向量机呢？这是因为分类面仅仅由分类面的两边距离它最近的几个点决定的，其它点对分类面没有影响。决定分类面的几个点称之为支持向量（Support Vector），好比这些点“支撑”着分类面。而利用Support Vector得到最佳分类面的方法，称之为支持向量机（Support Vector Machine）。</p><p>下面介绍SVM的一般求解方法。先写下我们的条件和目标：</p><p><img src="http://img.blog.csdn.net/20170621151733194?" alt="这里写图片描述"></p><p>这是一个典型的二次规划问题，即Quadratic Programming（QP）。因为SVM的目标是关于w的二次函数，条件是关于w和b的一次函数，所以，它的求解过程还是比较容易的，可以使用一些软件（例如Matlab）自带的二次规划的库函数来求解。下图给出SVM与标准二次规划问题的参数对应关系：</p><p><img src="http://img.blog.csdn.net/20170621153830381?" alt="这里写图片描述"></p><p>那么，线性SVM算法可以总结为三步：</p><ul><li><p><strong>计算对应的二次规划参数Q，p，A，c</strong></p></li><li><p><strong>根据二次规划库函数，计算b，w</strong></p></li><li><p><strong>将b和w代入$g_{SVM}$，得到最佳分类面</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170621154841557?" alt="这里写图片描述"></p><p>这种方法称为Linear Hard-Margin SVM Algorithm。如果是非线性的，例如包含x的高阶项，那么可以使用我们之前在《机器学习基石》课程中介绍的特征转换的方法，先作$z_n=\Phi(x_n)$的特征变换，从非线性的x域映射到线性的z域空间，再利用Linear Hard-Margin SVM Algorithm求解即可。</p><h3 id="Reasons-behind-Large-Margin-Hyperplane"><a href="#Reasons-behind-Large-Margin-Hyperplane" class="headerlink" title="Reasons behind Large-Margin Hyperplane"></a>Reasons behind Large-Margin Hyperplane</h3><p>从视觉和直觉的角度，我们认为Large-Margin Hyperplane的分类效果更好。SVM的这种思想其实与我们之前介绍的机器学习非常重要的正则化regularization思想很类似。regularization的目标是将$E_{in}$最小化，条件是$w^Tw\leq C$；SVM的目标是$w^Tw$最小化，条件是$y_n(w^Tx_n+b)\geq1$，即保证了$E_{in}=0$。有趣的是，regularization与SVM的目标和限制条件分别对调了。其实，考虑的内容是类似的，效果也是相近的。SVM也可以说是一种weight-decay regularization，限制条件是$E_{in}=0$。</p><p><img src="http://img.blog.csdn.net/20170621161842672?" alt="这里写图片描述"></p><p>从另一方面来看，Large-Margin会限制Dichotomies的个数。这从视觉上也很好理解，假如一条分类面越“胖”，即对应Large-Margin，那么它可能shtter的点的个数就可能越少：</p><p><img src="http://img.blog.csdn.net/20170621162739818?" alt="这里写图片描述"></p><p>之前的《机器学习基石》课程中介绍过，Dichotomies与VC Dimension是紧密联系的。也就是说如果Dichotomies越少，那么复杂度就越低，即有效的VC Dimension就越小，得到$E_{out}\approx E_{in}$，泛化能力强。</p><p>下面我们从概念的角度推导一下为什么dichotomies越少，VC Dimension就越少。首先我们考虑一下Large-Margin演算法的VC Dimension，记为$d_{vc}(A_{\rho})$。$d_{vc}(A_{\rho})$与数据有关，而我们之前介绍的$d_{vc}$与数据无关。</p><p>假如平面上有3个点分布在单位圆上，如果Margin为0，即$\rho=0$，这条细细的直线可以很容易将圆上任意三点分开（shatter），就能得到它的$d_{vc}=3$。如果$\rho&gt;\frac{\sqrt3}{2}$，这条粗粗的线无论如何都不能将圆上的任一三点全完分开（no shatter），因为圆上必然至少存在两个点的距离小于$\sqrt3$，那么其对应d的$d_{vc}&lt;3$。</p><p><img src="http://img.blog.csdn.net/20170621164552819?" alt="这里写图片描述"></p><p>那么，一般地，在d维空间，当数据点分布在半径为R的超球体内时，得到的$d_{vc}(A_{\rho})$满足下列不等式：</p><p><img src="http://img.blog.csdn.net/20170621165105577?" alt="这里写图片描述"></p><p>之前介绍的Perceptrons的VC Dimension为d+1，这里得到的结果是Large-Margin演算法的$d_{vc}(A_{\rho})\leq d+1$。所以，由于Large-Margin，得到的dichotomies个数减少，从而VC Dimension也减少了。VC Dimension减少降低了模型复杂度，提高了泛化能力。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了线性支持向量机（Linear Support Vector Machine）。我们先从视觉角度出发，希望得到一个比较“胖”的分类面，即满足所有的点距离分类面都尽可能远。然后，我们通过一步步推导和简化，最终把这个问题转换为标准的二次规划（QP）问题。二次规划问题可以使用Matlab等软件来进行求解，得到我们要求的w和b，确定分类面。这种方法背后的原理其实就是减少了dichotomies的种类，减少了有效的VC Dimension数量，从而让机器学习的模型具有更好的泛化能力。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170621075532782?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中的维度灾难</title>
    <link href="https://redstonewill.github.io/2018/03/18/17/"/>
    <id>https://redstonewill.github.io/2018/03/18/17/</id>
    <published>2018-03-18T02:16:34.000Z</published>
    <updated>2018-03-18T02:23:31.789Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170514233502869?" alt="这里写图片描述"><br><a id="more"></a></p><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>本篇文章，我们将讨论所谓的“维度灾难”，并解释在设计一个分类器时它为何如此重要。在下面几节中我将对这个概念进行直观的解释，并通过一个由于维度灾难导致的过拟合的例子来讲解。</p><p>考虑这样一个例子，我们有一些图片，每张图片描绘的是小猫或者小狗。我们试图构建一个分类器来自动识别图片中是猫还是狗。要做到这一点，我们首先需要考虑猫、狗的量化特征，这样分类器算法才能利用这些特征对图片进行分类。例如我们可以通过毛皮颜色特征对猫狗进行识别，即通过图片的红色程度、绿色程度、蓝色程度不同，设计一个简单的线性分类器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">If 0.5*red+0.3*green+0.2*blue&gt;0.6: </span><br><span class="line">return cat;</span><br><span class="line">else:</span><br><span class="line">return dog;</span><br></pre></td></tr></table></figure><p>红、绿、蓝三种颜色我们称之为特征Features，但仅仅利用这三个特征，还不能得到一个完美的分类器。因此，我们可以增加更多的特征来描述图片。例如计算图片X和Y方向的平均边缘或者梯度密度。现在总共有5个特征来构建我们的分类器了。</p><p>为了得到更好的分类效果，我们可以增加更多特征，例如颜色、纹理分布和统计信息等。也许我们能得到上百个特征，但是分类器的效果会变得更好吗？答案有些令人沮丧：并不能！事实上，特征数量超过一定值的时候，分类器的效果反而下降。图1显示了这种变化趋势，这就是“维度灾难”。</p><center><br><img src="http://img.blog.csdn.net/20170514233502869?" alt="这里写图片描述"><br>图1. 随着维度增加，分类器性能提升；维度增加到某值后，分类器性能下降<br></center> <p>下一节我们将解释为什么产生这条曲线并讨论如何避免这种情况发生。</p><h3 id="维度灾难与过拟合"><a href="#维度灾难与过拟合" class="headerlink" title="维度灾难与过拟合"></a>维度灾难与过拟合</h3><p>在之前引入的猫和狗的例子中，我们假设有无穷多的猫和狗的图片，然而，由于时间和处理能力限制，我们只得到10张图片（猫的图片或者狗的图片）。我们的最终目标是基于这10张图片构建一个分类器，能够正确对10个样本之外的无限多的图片进行正确分类。</p><p>现在，让我们使用一个简单的线性分类器来尝试得到一个好的分类器。如果只使用一个特征，例如使用图片的平均红色程度red。</p><center><br><img src="http://img.blog.csdn.net/20170511231757618?" alt="这里写图片描述"><br>图2. 单个特征对训练样本分类效果不佳<br></center><p>图2展示了只使用一个特征并不能得到一个最佳的分类结果。因此，我们觉得增加第二个特征：图片的平均绿色程度green。</p><center><br><img src="http://img.blog.csdn.net/20170511232142217?" alt="这里写图片描述"><br>图3. 增加第二个特征仍然不能线性分割，即不存在一条直线能够将猫和狗完全分开。<br></center><br>最后，我们决定再增加第三个特征：图片的平均蓝色程度，得到了三维特征空间：<br><center><br><img src="http://img.blog.csdn.net/20170511232607160?" alt="这里写图片描述"><br>图4. 增加第三个特征实现了线性可分，即存在一个平面完全将猫和狗分开来。<br></center><p>在三维特征空间，我们可以找到一个平面将猫和狗完全分开。这意味着三个特征的线性组合可以对10个训练样本进行最佳的分类。</p><center><br><img src="http://img.blog.csdn.net/20170511233703642?" alt="这里写图片描述"><br>图5. 特征越多，越有可能实现正确分类<br></center><p>以上的例子似乎证明了不断增加特征数量，直到获得最佳分类效果，是构建一个分类器的最好方法。但是，之前图1中，我们认为情况并非如此。我们需要注意一个问题：随着特征维度的增加，训练样本的在特征空间的密度是如何呈指数型下降的？</p><p>在1D空间中（图2所示），10个训练样本完全覆盖了1D特征空间，特征空间宽度为5。因此，1D下的样本密度是10/2=5。而在2D空间中（图3所示），同样是10个训练样本，它构成的2D特征空间面积为5x5=25.因此，2D下的样本密度是10/25=0.4。最后在3D空间中，10个训练样本构成的特征空间大小为5x5x5=125，因此，3D下的样本密度为10/125=0.08。</p><p>如果我们继续增加特征，整个特征空间维度增加，并变得越来越稀疏。由于稀疏性，我们更加容易找到一个超平面来实现分类。这是因为随着特征数量变得无限大，训练样本在最佳超平面的错误侧的可能性将会变得无限小。然而，如果我们将高维的分类结果投影到低维空间中，将会出现一个严重的问题：</p><center><br><img src="http://img.blog.csdn.net/20170512102455332?" alt="这里写图片描述"><br>图6. 使用太多特征导致过拟合。分类器学习了过多样本数据的异常特征（噪声），而对新数据的泛化能力不好。<br></center><p>图6展示了3D的分类结果投影到2D特征空间的样子。样本数据在3D是线性可分的，但是在2D却并非如此。事实上，增加第三个维度来获得最佳的线性分类效果，等同于在低维特征空间中使用非线性分类器。其结果是，分类器学习了训练数据的噪声和异常，而对样本外的数据拟合效果并不理想，甚至很差。</p><p>这个概念称为过拟合，是维度灾难的一个直接后果。图7展示了一个只用2个特征进行分类的线性分类器的二维平面图。</p><center><br><img src="http://img.blog.csdn.net/20170512104505138?" alt="这里写图片描述"><br>图7. 尽管训练样本不能全都分类正确，但这个分类器的泛化能力比图5要好。<br></center><br>尽管图7中的简单的线性分类器比图5中的非线性分类器的效果差，但是图7的分类器的泛化能力强。这是因为分类器没有把样本数据的噪声和异常也进行学习。另一方面说，使用更少的特征，维度灾难就能避免，就不会出现对训练样本过拟合的现象。<br><br>图8用不同的方式解释上面的内容。假设我们只使用一个特征来训练分类器，1D特征值的范围限定在0到1之间，且每只猫和狗对应的特征值是唯一的。如果我们希望训练样本的特征值占特征值范围的20%，那么训练样本的数量就要达到总体样本数的20%。现在，如果增加第二个特征，也就是从直线变为平面2D特征空间，这种情况下，如果要覆盖特征值范围的20%，那么训练样本数量就要达到总体样本数的45%（0.45<em>0.45=0.2）。而在3D空间中，如果要覆盖特征值范围的20%，就需要训练样本数量达到总体样本数的58%（0.58</em>0.58*0.58=0.2）。<br><center><br><img src="http://img.blog.csdn.net/20170512122232781?" alt="这里写图片描述"><br>图8. 覆盖特征值范围20%所需的训练样本数量随着维度增加呈指数型增长<br></center><p>换句话说，如果可用的训练样本数量是固定的，那么如果增加特征维度的话，过拟合就会发生。另一方面，如果增加特征维度，为了覆盖同样的特征值范围、防止过拟合，那么所需的训练样本数量就会成指数型增长。</p><p>在上面的例子中，我们展示了维度灾难会引起训练数据的稀疏化。使用的特征越多，数据就会变得越稀疏，从而导致分类器的分类效果就会越差。维度灾难还会造成搜索空间的数据稀疏程度分布不均。事实上，围绕原点的数据（在超立方体的中心）比在搜索空间的角落处的数据要稀疏得多。这可以用下面这个例子来解释：</p><p>想象一个单位正方形代表了2D的特征空间，特征空间的平均值位于这个单位正方形的中心处，距中心处单位距离的所有点构成了正方形的内接圆。没有落在单位圆的训练样本距离搜索空间的角落处更距离中心处更近，而这些样本由于特征值差异很大（样本分布在正方形角落处），所有难以分类。因此，如果大部分样本落在单位内接圆里，就会更容易分类。如图9所示：</p><center><br><img src="http://img.blog.csdn.net/20170515104749082?" alt="这里写图片描述"><br>图9. 落在单位圆之外的训练样本位于特征空间角落处，比位于特征空间中心处的样本更难进行分类。<br></center><p>一个有趣的问题是当我们增加特征空间的维度时，随着正方形（超立方体）的体积变化，圆形（超球体）的体积是如何变化的？无论维度如何变化，超立方体的体积都是1，而半径为0.5的超球体的体积随着维度d的变化为：<br>$$V(d)=\frac{\pi^{d/2}}{\Gamma(\frac d2+1)}0.5^d$$</p><p>图10展示了随着维度d的增加，超球面的体积是如何变化的：</p><center><br><img src="http://img.blog.csdn.net/20170515110944160?" alt="这里写图片描述"><br>图10. 维度d很大时，超球面的体积趋于零<br></center><p>这表明了随着维度变得越来越大，超球体的体积趋于零，而超立方体的体积是不变的。这种令人惊讶的反直觉发现部分解释了在分类中维度灾难的问题：在高维空间中，大部分的训练数据分布在定义为特征空间的超立方体的角落处。就像之前提到的，特征空间角落处的样本比超球体内的样本更加难以进行正确分类。图11分别从2D、3D和可视化的8D超立方体（$2^8=256$个角落）的例子论证了这个结论。</p><center><br><img src="http://img.blog.csdn.net/20170515112107896?" alt="这里写图片描述"><br>图11. 随着维度增加，大部分数量数据分布在角落处<br></center><p>对于8维的超球体，大约98%的数据集中在它256个角落处。其结果是，当特征空间的维度变得无限大时，从样本点到质心的最大、最小欧氏距离的差值与其最小欧式距离的比值趋于零：<br>$$\lim_{d\rightarrow \infty}\frac{dist_{max}-dist_{min}}{dist_{min}}\rightarrow0$$</p><p>因此，距离测量在高维空间中逐渐变得无效。因为分类器是基于这些距离测量的（例如Euclidean距离、Mahalanobis距离、Manhattan距离），所以低维空间特征更少，分类更加容易。同样地，在高维空间的高斯分布会变平坦且尾巴更长。</p><h3 id="如何避免维度灾难"><a href="#如何避免维度灾难" class="headerlink" title="如何避免维度灾难"></a>如何避免维度灾难</h3><p>图1展示了随着维度变得很大，分类器的性能是下降的。那么问题是“很大”意味着什么？过拟合如何避免？很遗憾，在分类问题中，没有固定的规则来指定应该使用多少特征。事实上，这依赖于训练样本的数量、决策边界的复杂性和使用的是哪个分类器。</p><p>如果理论上训练样本时无限多的，那么维度灾难不会发生，我们可以使用无限多的特征来获得一个完美的分类器。训练数据越少，使用的特征就要越少。如果N个训练样本覆盖了1D特征空间的范围，那么在2D中，覆盖同样密度就需要N<em>N个数据，同样在3D中，就需要N</em>N*N个数据。也就是说，随着维度增加，训练样本的数量要求随指数增加。</p><p>此外，那些非线性决策边界的分类器（例如神经网络、KNN分类器、决策树等）分类效果好但是泛化能力差且容易发生过拟合。因此，当使用这些分类器的时候，维度不能太高。如果使用泛化能力好的分类器（例如贝叶斯分类器、线性分类器），可以使用更多的特征，因为分类器模型并不复杂。图6展示了高维中的简单分类器对应于地位空间的复杂分类器。</p><p>因此，过拟合只在高维空间中预测相对少的参数和低维空间中预测多参数这两种情况下发生。举个例子，高斯密度函数有两类参数：均值和协方差矩阵。在3D空间中，协方差矩阵是3x3的对称阵，总共有6个值（3个主对角线值和3个非对角线值），还有3个均值，加在一起，一共要求9个参数；而在1D，高斯密度函数只要求2个参数（1个均值，1个方差）；在2D中，高斯密度函数要求5个参数（2个均值，3个协方差参数）。我们可以发现，随着维度增加，参数数量呈平方式增长。</p><p>在之前的文章里我们发现，如果参数数量增加，那么参数的方差就会增大（前提是估计偏差和训练样本数量保持不变）。这就意味着，如果围度增加，估计的参数方差增大，导致参数估计的质量下降。分类器的方差增大意味着出现过拟合。</p><p>另一个有趣的问题是：应该选择哪些特征。如果有N个特征，我们应该如何选取M个特征？一种方法是在图1曲线中找到性能最佳的位置。但是，由于很难对所有的特征组合进行训练和测试，所以有一些其他办法来找到最佳选择。这些方法称之为特征选择算法，经常用启发式方法（例如贪心算法、best-first方法等）来定位最佳的特征组合和数量。</p><p>还有一种方法是用M个特征替换N个特征，M个特征由原始特征组合而成。这种通过对原始特征进行优化的线性或非线性组合来减少问题维度的算法称为特征提取。一个著名的维度降低技术是主成分分析法（PCA），它去除不相关维度，对N个原始特征进行线性组合。PCA算法试着找到低维的线性子空间，保持原始数据的最大方差。然而，数据方差最大不一定代表数据最显著的分类信息。</p><p>最后，一项非常有用的被用来测试和避免过拟合的技术是交叉验证。交叉验证将原始训练数据分成多个训练样本子集。在分类器进行训练过程中，一个样本子集被用来测试分类器的准确性，其他样本用来进行参数估计。如果交叉验证的结果与训练样本子集得到的结果不一致，那么就表示发生了过拟合。如果训练样本有限，那么可以使用k折法或者留一发进行交叉验证。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>这篇文章我们讨论了特征选择、特征提取、交叉验证的重要性，以及避免由维度灾难导致的过拟合。通过一个过拟合的简单例子，我们复习了维度灾难的重要影响。</p><p><strong><em>原文出处：</em></strong><br><a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/" target="_blank" rel="noopener">http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170514233502869?&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记16（完结） -- Three Learning Principles</title>
    <link href="https://redstonewill.github.io/2018/03/17/16/"/>
    <id>https://redstonewill.github.io/2018/03/17/16/</id>
    <published>2018-03-17T14:01:42.000Z</published>
    <updated>2018-03-17T14:03:29.262Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170605194127527?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们讲了一个机器学习很重要的工具——Validation。我们将整个训练集分成两部分：$D_{train}$和$D_{val}$，一部分作为机器学习模型建立的训练数据，另一部分作为验证模型好坏的数据，从而选择到更好的模型，实现更好的泛化能力。这节课，我们主要介绍机器学习中非常实用的三个“锦囊妙计”。</p><h3 id="Occam’s-Razor"><a href="#Occam’s-Razor" class="headerlink" title="Occam’s Razor"></a>Occam’s Razor</h3><p>奥卡姆剃刀定律（Occam’s Razor），是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。奥卡姆（Ockham）在英格兰的萨里郡，那是他出生的地方。他在《箴言书注》2卷15题说“切勿浪费较多东西去做用较少的东西同样可以做好的事情。” 这个原理称为“如无必要，勿增实体”（Entities must not be multiplied unnecessarily），就像剃刀一样，将不必要的部分去除掉。</p><p>Occam’s Razor反映到机器学习领域中，指的是在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。</p><p><img src="http://img.blog.csdn.net/20170605194127527?" alt="这里写图片描述"></p><p>上图就是一个模型选择的例子，左边的模型很简单，可能有分错的情况；而右边的模型非常复杂，所有的训练样本都分类正确。但是，我们会选择左边的模型，它更简单，符合人类直觉的解释方式。这样的结果带来两个问题：一个是什么模型称得上是简单的？另一个是为什么简单模型比复杂模型要好？</p><p>简单的模型一方面指的是简单的hypothesis h，简单的hypothesis就是指模型使用的特征比较少，例如多项式阶数比较少。简单模型另一方面指的是模型H包含的hypothesis数目有限，不会太多，这也是简单模型包含的内容。</p><p><img src="http://img.blog.csdn.net/20170605195055446?" alt="这里写图片描述"></p><p>其实，simple hypothesis h和simple model H是紧密联系的。如果hypothesis的特征个数是l，那么H中包含的hypothesis个数就是$2^l$，也就是说，hypothesis特征数目越少，H中hypothesis数目也就越少。</p><p>所以，为了让模型简单化，我们可以一开始就选择简单的model，或者用regularization，让hypothesis中参数个数减少，都能降低模型复杂度。</p><p>那为什么简单的模型更好呢？下面从哲学的角度简单解释一下。机器学习的目的是“找规律”，即分析数据的特征，总结出规律性的东西出来。假设现在有一堆没有规律的杂乱的数据需要分类，要找到一个模型，让它的$E_{in}=0$，是很难的，大部分时候都无法正确分类，但是如果是很复杂的模型，也有可能将其分开。反过来说，如果有另一组数据，如果可以比较容易找到一个模型能完美地把数据分开，那表明数据本身应该是有某种规律性。也就是说杂乱的数据应该不可以分开，能够分开的数据应该不是杂乱的。如果使用某种简单的模型就可以将数据分开，那表明数据本身应该符合某种规律性。相反地，如果用很复杂的模型将数据分开，并不能保证数据本身有规律性存在，也有可能是杂乱的数据，因为无论是有规律数据还是杂乱数据，复杂模型都能分开。这就不是机器学习模型解决的内容了。所以，模型选择中，我们应该尽量先选择简单模型，例如最简单的线性模型。</p><h3 id="Sampling-Bias"><a href="#Sampling-Bias" class="headerlink" title="Sampling Bias"></a>Sampling Bias</h3><p>首先引入一个有趣的例子：1948年美国总统大选的两位热门候选人是Truman和Dewey。一家报纸通过电话采访，统计人们把选票投给了Truman还是Dewey。经过大量的电话统计显示，投给Dewey的票数要比投个Truman的票数多，所以这家报纸就在选举结果还没公布之前，信心满满地发表了“Dewey Defeats Truman”的报纸头版，认为Dewey肯定赢了。但是大选结果公布后，让这家报纸大跌眼镜，最终Truman赢的了大选的胜利。</p><p>为什么会出现跟电话统计完全相反的结果呢？是因为电话统计数据出错还是投票运气不好？都不是。其实是因为当时电话比较贵，有电话的家庭比较少，而正好是有电话的美国人支持Dewey的比较多，而没有电话的支持Truman比较多。也就是说样本选择偏向于有钱人那边，可能不具有广泛的代表性，才造成Dewey支持率更多的假象。</p><p>这个例子表明，抽样的样本会影响到结果，用一句话表示“If the data is sampled in a biased way, learning will produce a similarly biased outcome.”意思是，如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。</p><p>从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。</p><h3 id="Data-Snooping"><a href="#Data-Snooping" class="headerlink" title="Data Snooping"></a>Data Snooping</h3><p>之前的课程，我们介绍过在模型选择时应该尽量避免偷窥数据，因为这样会使我们人为地倾向于某种模型，而不是根据数据进行随机选择。所以，$\Phi$应该自由选取，最好不要偷窥到原始数据，这会影响我们的判断。</p><p>事实上，数据偷窥发生的情况有很多，不仅仅指我们看到了原始数据。什么意思呢？其实，当你在使用这些数据的任何过程，都是间接地偷看到了数据本身，然后你会进行一些模型的选择或者决策，这就增加了许多的model complexity，也就是引入了污染。</p><p>下面举个例子来说明。假如我们有8年的货比交易数据，我们希望从这些数据中找出规律，来预测货比的走势。如果选择前6年数据作为训练数据，后2年数据作为测试数据的话，来训练模型。现在我们有前20天的数据，根据之前训练的模型，来预测第21天的货比交易走势。</p><p><img src="http://img.blog.csdn.net/20170605213904953?" alt="这里写图片描述"></p><p>现在有两种训练模型的方法，如图所示，一种是使用前6年数据进行模型训练，后2年数据作为测试，图中蓝色曲线表示后2年的预测收益；另一种是直接使用8年数据进行模型训练，图中红色曲线表示后2年的预测收益情况。图中，很明显，使用8年数据进行训练的模型对后2年的预测的收益更大，似乎效果更好。但是这是一种自欺欺人的做法，因为训练的时候已经拿到了后2年的数据，用这样的模型再来预测后2年的走势是不科学的。这种做法也属于间接偷窥数据的行为。直接偷窥和间接偷窥数据的行为都是不科学的做法，并不能表示训练的模型有多好。</p><p><img src="http://img.blog.csdn.net/20170605214612062?" alt="这里写图片描述"></p><p>还有一个偷窥数据的例子，比如对于某个基准数据集D，某人对它建立了一个模型H1，并发表了论文。第二个人看到这篇论文后，又会对D，建立一个新的好的模型H2。这样，不断地有人看过前人的论文后，建立新的模型。其实，后面人选择模型时，已经被前人影响了，这也是偷窥数据的一种情况。也许你能对D训练很好的模型，但是可能你仅仅只根据前人的模型，成功避开了一些错误，甚至可能发生了overfitting或者bad generalization。所以，机器学习领域有这样一句有意思的话“If you torture the data long enough, it will confess.”所以，我们不能太“折磨”我们的数据了，否则它只能“妥协”了~哈哈。</p><p>在机器学习过程中，避免“偷窥数据”非常重要，但实际上，完全避免也很困难。实际操作中，有一些方法可以帮助我们尽量避免偷窥数据。第一个方法是“看不见”数据。就是说当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。第二个方法是保持怀疑。就是说时刻保持对别人的论文或者研究成果保持警惕与怀疑，要通过自己的研究与测试来进行模型选择，这样才能得到比较正确的结论。</p><p><img src="http://img.blog.csdn.net/20170605220807260?" alt="这里写图片描述"></p><h3 id="Power-of-Three"><a href="#Power-of-Three" class="headerlink" title="Power of Three"></a>Power of Three</h3><p>本小节，我们对16节课做个简单的总结，用“三的威力”进行概括。因为课程中我们介绍的很多东西都与三有关。</p><p>首先，我们介绍了跟机器学习相关的三个领域：</p><ul><li><p>Data Mining</p></li><li><p>Artificial Intelligence</p></li><li><p>Statistics</p></li></ul><p><img src="http://img.blog.csdn.net/20170606080342125?" alt="这里写图片描述"></p><p>我们还介绍了三个理论保证：</p><ul><li><p>Hoeffding</p></li><li><p>Multi-Bin Hoeffding</p></li><li><p>VC</p></li></ul><p><img src="http://img.blog.csdn.net/20170606080556063?" alt="这里写图片描述"></p><p>然后，我们又介绍了三种线性模型：</p><ul><li><p>PLA/pocket</p></li><li><p>linear regression</p></li><li><p>logistic regression</p></li></ul><p><img src="http://img.blog.csdn.net/20170606080846308?" alt="这里写图片描述"></p><p>同时，我们介绍了三种重要的工具：</p><ul><li><p>Feature Transform</p></li><li><p>Regularization</p></li><li><p>Validation</p></li></ul><p><img src="http://img.blog.csdn.net/20170606081141583?" alt="这里写图片描述"></p><p>还有我们本节课介绍的三个锦囊妙计：</p><ul><li><p>Occam’s Razer</p></li><li><p>Sampling Bias</p></li><li><p>Data Snooping</p></li></ul><p><img src="http://img.blog.csdn.net/20170606081339252?" alt="这里写图片描述"></p><p>最后，我们未来机器学习的方向也分为三种：</p><ul><li><p>More Transform</p></li><li><p>More Regularization</p></li><li><p>Less Label</p></li></ul><p><img src="http://img.blog.csdn.net/20170606081719882?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了机器学习三个重要的锦囊妙计：Occam’s Razor, Sampling Bias, Data Snooping。并对《机器学习基石》课程中介绍的所有知识和方法进行“三的威力”这种形式的概括与总结，“三的威力”也就构成了坚固的机器学习基石。</p><p>整个机器学习基石的课程笔记总结完毕！后续将会推出机器学习技法的学习笔记，谢谢！</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170605194127527?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记15 -- Validation</title>
    <link href="https://redstonewill.github.io/2018/03/17/15/"/>
    <id>https://redstonewill.github.io/2018/03/17/15/</id>
    <published>2018-03-17T13:58:47.000Z</published>
    <updated>2018-03-17T14:19:31.276Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170601204900309?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要讲了为了避免overfitting，可以使用regularization方法来解决。在之前的$E_{in}$上加上一个regularizer，生成$E_{aug}$，将其最小化，这样可以有效减少模型的复杂度，避免过拟合现象的发生。那么，机器学习领域还有许多选择，如何保证训练的模型具有良好的泛化能力？本节课将介绍一些概念和方法来解决这个选择性的问题。</p><h3 id="Model-Selection-Problem"><a href="#Model-Selection-Problem" class="headerlink" title="Model Selection Problem"></a>Model Selection Problem</h3><p>机器学习模型建立的过程中有许多选择，例如对于简单的二元分类问题，首先是算法A的选择，有PLA，pocket，linear regression，logistic regression等等；其次是迭代次数T的选择，有100，1000,10000等等；之后是学习速率$\eta$的选择，有1，0.01,0.0001等等；接着是模型特征转换$\Phi$的选择，有linear，quadratic，poly-10，Legendre-poly-10等等；然后是正则化regularizer的选择，有L2，L1等等；最后是正则化系数$\lambda$的选择，有0，0.01，1等等。不同的选择搭配，有不同的机器学习效果。我们的目标就是找到最合适的选择搭配，得到一个好的矩g，构建最佳的机器学习模型。</p><p><img src="http://img.blog.csdn.net/20170601204900309?" alt="这里写图片描述"></p><p>假设有M个模型，对应有$H_1,H_2,\cdots,H_M$，即有M个hypothesis set，演算法为$A_1,A_2,\cdots,A_M$，共M个。我们的目标是从这M个hypothesis set中选择一个模型$H_{m^*}$，通过演算法$A_{m^*}$对样本集D的训练，得到一个最好的矩$g_{m^*}$，使其$E_{out}(g_{m^*})$最小。所以，问题的关键就是机器学习中如何选择到最好的矩$g_{m^*}$。</p><p>考虑有这样一种方法，对M个模型分别计算使$E_{in}$最小的矩g，再横向比较，取其中能使$E_{in}$最小的模型的矩$g_{m^*}$：</p><p><img src="http://img.blog.csdn.net/20170601213602033?" alt="这里写图片描述"></p><p>但是$E_{in}$足够小并不能表示模型好，反而可能表示训练的矩$g_{m^*}$发生了过拟合，泛化能力很差。而且这种“模型选择+学习训练”的过程，它的VC Dimension是$d_{VC}(H_1\cup H_2)$，模型复杂度增加。总的来说，泛化能力差，用$E_{in}$来选择模型是不好的。</p><p>另外一种方法，如果有这样一个独立于训练样本的测试集，将M个模型在测试集上进行测试，看一下$E_{test}$的大小，则选取$E_{test}$最小的模型作为最佳模型：</p><p><img src="http://img.blog.csdn.net/20170601214722299?" alt="这里写图片描述"></p><p>这种测试集验证的方法，根据finite-bin Hoffding不等式，可以得到：</p><p>$$E_{out}(g_{m^*})\leq E_{test}(g_{m^*})+O(\sqrt \frac{log M}{N_{test}})$$</p><p>由上式可以看出，模型个数M越少，测试集数目越大，那么$O(\sqrt \frac{log M}{N_{test}})$越小，即$E_{test}(g_{m^*})$越接近于$E_{out}(g_{m^*})$。</p><p>下面比较一下之前讲的两种方法，第一种方法使用$E_{in}$作为判断基准，使用的数据集就是训练集D本身；第二种方法使用$E_{test}$作为判断基准，使用的是独立于训练集D之外的测试集。前者不仅使用D来训练不同的$g_m$，而且又使用D来选择最好的$g_{m^*}$，那么$g_{m^*}$对未知数据并不一定泛化能力好。举个例子，这相当于老师用学生做过的练习题再来对学生进行考试，那么即使学生得到高分，也不能说明他的学习能力强。所以最小化$E_{in}$的方法并不科学。而后者使用的是独立于D的测试集，相当于新的考试题能更好地反映学生的真实水平，所以最小化$E_{test}$更加理想。</p><p><img src="http://img.blog.csdn.net/20170601231504059?" alt="这里写图片描述"></p><p>但是，我们拿到的一都是训练集D，测试集是拿不到的。所以，寻找一种折中的办法，我们可以使用已有的训练集D来创造一个验证集validation set，即从D中划出一部分$D_{val}$作为验证集。D另外的部分作为训练模型使用，$D_{val}$独立开来，用来测试各个模型的好坏，最小化$E_{val}$，从而选择最佳的$g_{m^*}$。</p><p><img src="http://img.blog.csdn.net/20170601233528299?" alt="这里写图片描述"></p><h3 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h3><p>从训练集D中抽出一部分K个数据作为验证集$D_{val}$，$D_{val}$对应的error记为$E_{val}$。这样做的一个前提是保证$D_{val}$独立同分布（iid）于P(x,y)，也就是说$D_{val}$的选择是从D中平均随机抽样得到的，这样能够把$E_{val}$与$E_{out}$联系起来。D中去除$D_{val}$后的数据就是供模型选择的训练数据$D_{train}$，其大小为N-k。从$D_{train}$中选择最好的矩，记为$g_m^-$。</p><p><img src="http://img.blog.csdn.net/20170602080623574?" alt="这里写图片描述"></p><p>假如D共有1000个样本，那么可以选择其中900个$D_{train}$，剩下的100个作为$D_{val}$。使用$D_{train}$训练模型，得到最佳的$g_m^-$，使用$g_m^-$对$D_{val}$进行验证，得到如下Hoffding不等式：</p><p>$$E_{out}(g_m^-)\leq E_{val}(g_m^-)+O(\sqrt \frac{log M}{K})$$</p><p>假设有M种模型hypothesis set，$D_{val}$的数量为K，那么从每种模型m中得到一个在$D_{val}$上表现最好的矩，再横向比较，从M个矩中选择一个最好的$m^*$作为我们最终得到的模型。</p><p><img src="http://img.blog.csdn.net/20170602082146642?" alt="这里写图片描述"></p><p>现在由于数量为N的总样本D的一部分K作为验证集，那么只有N-k个样本可供训练。从$D_{train}$中得到最好的$g_{m^*}^-$，而总样本D对应的最好的矩为$g_{m^*}$。根据之前的leraning curve很容易知道，训练样本越多，得到的模型越准确，其hypothesis越接近target function，即D的$E_{out}$比$D_{train}$的$E_{out}$要小：</p><p><img src="http://img.blog.csdn.net/20170602083650808?" alt="这里写图片描述"></p><p>所以，我们通常的做法是通过$D_{val}$来选择最好的矩$g_{m^*}^-$对应的模型$m^*$，再对整体样本集D使用该模型进行训练，最终得到最好的矩$g_{m^*}$。</p><p>总结一下，使用验证集进行模型选择的整个过程为：先将D分成两个部分，一个是训练样本$D_{train}$，一个是验证集$D_{val}$。若有M个模型，那么分别对每个模型在$D_{train}$上进行训练，得到矩$g_{m}^-$，再用$D_{val}$对每个$g_{m}^-$进行验证，选择表现最好的矩$g_{m^*}^-$，则该矩对应的模型被选择。最后使用该模型对整个D进行训练，得到最终的$g_{m^*}$。下图展示了整个模型选择的过程：</p><p><img src="http://img.blog.csdn.net/20170602085825099?" alt="这里写图片描述"></p><p>不等式关系满足：</p><p>$$E_{out}(g_{m^*})\leq E_{out}(g_{m^*}^-)\leq E_{val}(g_{m^*}^-)+O(\sqrt \frac{log M}{K})$$</p><p>下面我们举个例子来解释这种模型选择的方法的优越性，假设有两个模型：一个是5阶多项式$H_{\Phi_5}$，一个是10阶多项式$H_{\Phi_{10}}$。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较，分析结果如下：</p><p><img src="http://img.blog.csdn.net/20170602090711596?" alt="这里写图片描述"></p><p>图中，横坐标表示验证集数量K，纵坐标表示$E_{out}$大小。黑色水平线表示没有验证集，完全使用$E_{in}$进行判断基准，那么$H_{\Phi_{10}}$更好一些，但是这种方法的$E_{out}$比较大，而且与K无关。黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，其$E_{out}$很小，同样也与K无关，实际中很难得到这条虚线。红色曲线表示使用验证集，但是最终选取的矩是$g_{m^*}^-$，其趋势是随着K的增加，它对应的$E_{out}$先减小再增大，当K大于一定值的时候，甚至会超过黑色水平线。蓝色曲线表示也使用验证集，最终选取的矩是$g_{m^*}$，其趋势是随着K的增加，它对应的$E_{out}$先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。</p><p>这里提一点，当K大于一定的值时，红色曲线会超过黑色直线。这是因为随着K的增大，$D_{val}$增大，但可供模型训练的$D_{train}$在减小，那得到的$g_{m^*}^-$不具有很好的泛化能力，即对应的$E_{out}$会增大，甚至当K增大到一定值时，比$E_{in}$模型更差。</p><p>那么，如何设置验证集K值的大小呢？根据之前的分析：</p><p><img src="http://img.blog.csdn.net/20170602104358868?" alt="这里写图片描述"></p><p>当K值很大时，$E_{val}\approx E_{out}$，但是$g_m^-$与$g_m$相差很大；当K值很小是，$g_m^-\approx g_m$，但是$E_{val}$与$E_{out}$可能相差很大。所以有个折中的办法，通常设置$k=\frac N5$。值得一提的是，划分验证集，通常并不会增加整体时间复杂度，反而会减少，因为$D_{train}$减少了。</p><h3 id="Leave-One-Out-Cross-Validation"><a href="#Leave-One-Out-Cross-Validation" class="headerlink" title="Leave-One-Out Cross Validation"></a>Leave-One-Out Cross Validation</h3><p>假如考虑一个极端的例子，k=1，也就是说验证集大小为1，即每次只用一组数据对$g_m$进行验证。这样做的优点是$g_m^-\approx g_m$，但是$E_{val}$与$E_{out}$可能相差很大。为了避免$E_{val}$与$E_{out}$相差很大，每次从D中取一组作为验证集，直到所有样本都作过验证集，共计算N次，最后对验证误差求平均，得到$E_{loocv}(H,A)$，这种方法称之为留一法交叉验证，表达式为：</p><p>$$E_{loocv}(H,A)=\frac1N\sum_{n=1}^Ne_n=\frac1N\sum_{n=1}^Nerr(g_n^-(x_n),y_n)$$</p><p>这样求平均的目的是为了让$E_{loocv}(H,A)$尽可能地接近$E_{out}(g)$。</p><p>下面用一个例子图解留一法的过程：</p><p><img src="http://img.blog.csdn.net/20170602143003909?" alt="这里写图片描述"></p><p>如上图所示，要对二维平面上的三个点做拟合，上面三个图表示的是线性模型，下面三个图表示的是常数模型。对于两种模型，分别使用留一交叉验证法来计算$E_{loocv}$，计算过程都是每次将一个点作为验证集，其他两个点作为训练集，最终将得到的验证误差求平均值，就得到了$E_{loocv}(linear)$和$E_{loocv}(constant)$，比较两个值的大小，取值小对应的模型即为最佳模型。</p><p><img src="http://img.blog.csdn.net/20170602143613402?" alt="这里写图片描述"></p><p>接下来，我们从理论上分析Leave-One-Out方法的可行性，即$E_{loocv}(H,A)$是否能保证$E_{out}$的矩足够好？假设有不同的数据集D，它的期望分布记为$\varepsilon_D$，则其$E_{loocv}(H,A)$可以通过推导，等于$E_{out}(N-1)$的平均值。由于N-1近似为N，$E_{out}(N-1)$的平均值也近似等于$E_{out}(N)$的平均值。具体推导过程如下：</p><p><img src="http://img.blog.csdn.net/20170602150552663?" alt="这里写图片描述"></p><p>最终我们得到的结论是$E_{loocv}(H,A)$的期望值和$E_{out}(g^-)$的期望值是相近的，这代表得到了比较理想的$E_{out}(g)$，Leave-One-Out方法是可行的。</p><p>举一个例子，使用两个特征：Average Intensity和Symmetry加上这两个特征的非线性变换（例如高阶项）来进行手写数字识别。平面特征分布如下图所示：</p><p><img src="http://img.blog.csdn.net/20170602154420527?" alt="这里写图片描述"></p><p>Error与特征数量的关系如下图所示：</p><p><img src="http://img.blog.csdn.net/20170602154609826?" alt="这里写图片描述"></p><p>从图中我们看出，随着特征数量的增加，$E_{in}$不断减小，$E_{out}$先减小再增大，虽然$E_{in}$是不断减小的，但是它与$E_{out}$的差距越来越大，发生了过拟合，泛化能力太差。而$E_{cv}$与$E_{out}$的分布基本一致，能较好地反映$E_{out}$的变化。所以，我们只要使用Leave-One-Out方法得到使$E_{cv}$最小的模型，就能保证其$E_{out}$足够小。下图是分别使用$E_{in}$和$E_{out}$进行训练得到的分类曲线：</p><p><img src="http://img.blog.csdn.net/20170602155243060?" alt="这里写图片描述"></p><p>很明显可以看出，使用$E_{in}$发生了过拟合，而$E_{loocv}$分类效果更好，泛化能力强。</p><h3 id="V-Fold-Cross-Validation"><a href="#V-Fold-Cross-Validation" class="headerlink" title="V-Fold Cross Validation"></a>V-Fold Cross Validation</h3><p>接下来我们看看Leave-One-Out可能的问题是什么。首先，第一个问题是计算量，假设N=1000，那么就需要计算1000次的$E_{loocv}$，再计算其平均值。当N很大的时候，计算量是巨大的，很耗费时间。第二个问题是稳定性，例如对于二分类问题，取值只有0和1两种，预测本身存在不稳定的因素，那么对所有的$E_{loocv}$计算平均值可能会带来很大的数值跳动，稳定性不好。所以，这两个因素决定了Leave-One-Out方法在实际中并不常用。</p><p>针对Leave-One-Out的缺点，我们对其作出了改进。Leave-One-Out是将N个数据分成N分，那么改进措施是将N个数据分成V份（例如V=10），计算过程与Leave-One-Out相似。这样可以减少总的计算量，又能进行交叉验证，得到最好的矩，这种方法称为V-折交叉验证。其实Leave-One-Out就是V-折交叉验证的一个极端例子。</p><p>$$E_{cv}(H,A)=\frac1V\sum_{v=1}^VE_{val}^{(V)}(g_V^-)$$</p><p>所以呢，一般的Validation使用V-折交叉验证来选择最佳的模型。值得一提的是Validation的数据来源也是样本集中的，所以并不能保证交叉验证的效果好，它的模型一定好。只有样本数据越多，越广泛，那么Validation的结果越可信，其选择的模型泛化能力越强。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Validation验证。先从如何选择一个好的模型开始切入，例如使用$E_{in}$、$E_{test}$都是不太好的，最终使用$E_{val}$来进行模型选择。然后详细介绍了Validation的过程。最后，介绍了Leave-One-Out和V-Fold Cross两种验证方法，比较它们各自的优点和缺点，实际情况下，V-Fold Cross更加常用。</p><p><strong><em>注明：</em></strong><br>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170601204900309?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记14 -- Regularization</title>
    <link href="https://redstonewill.github.io/2018/03/17/14/"/>
    <id>https://redstonewill.github.io/2018/03/17/14/</id>
    <published>2018-03-17T13:55:35.000Z</published>
    <updated>2018-03-17T13:58:20.891Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170526135401178?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们介绍了过拟合发生的原因：excessive power, stochastic/deterministic noise 和limited data。并介绍了解决overfitting的简单方法。本节课，我们将介绍解决overfitting的另一种非常重要的方法：Regularization规则化。</p><h3 id="Regularized-Hypothesis-Set"><a href="#Regularized-Hypothesis-Set" class="headerlink" title="Regularized Hypothesis Set"></a>Regularized Hypothesis Set</h3><p>先来看一个典型的overfitting的例子：</p><p><img src="http://img.blog.csdn.net/20170526134202674?" alt="这里写图片描述"></p><p>如图所示，在数据量不够大的情况下，如果我们使用一个高阶多项式（图中红色曲线所示），例如10阶，对目标函数（蓝色曲线）进行拟合。拟合曲线波动很大，虽然$E_{in}$很小，但是$E_{out}$很大，也就造成了过拟合现象。</p><p>那么如何对过拟合现象进行修正，使hypothesis更接近于target function呢？一种方法就是regularized fit。</p><p><img src="http://img.blog.csdn.net/20170526134855871?" alt="这里写图片描述"></p><p>这种方法得到的红色fit曲线，要比overfit的红色曲线平滑很多，更接近与目标函数，它的阶数要更低一些。那么问题就变成了我们要把高阶（10阶）的hypothesis sets转换为低阶（2阶）的hypothesis sets。通过下图我们发现，不同阶数的hypothesis存在如下包含关系：</p><p><img src="http://img.blog.csdn.net/20170526135401178?" alt="这里写图片描述"></p><p>我们发现10阶多项式hypothesis sets里包含了2阶多项式hypothesis sets的所有项，那么在$H_{10}$中加入一些限定条件，使它近似为$H_2$即可。这种函数近似曾被称之为不适定问题（ill-posed problem）。</p><p>如何从10阶转换为2阶呢？首先，$H_{10}$可表示为：<br>$$H_{10}=w_0+w_1x+w_2x^2+w_3x^3+\cdots+w_{10}x^{10}$$</p><p>而$H_2$可表示为：<br>$$H_2=w_0+w_1x+w_2x^2$$</p><p>所以，如果限定条件是$w_3=w_4=\cdots=w_{10}=0$，那么就有$H_2=H_{10}$。也就是说，对于高阶的hypothesis，为了防止过拟合，我们可以将其高阶部分的权重w限制为0，这样，就相当于从高阶的形式转换为低阶，fit波形更加平滑，不容易发生过拟合。</p><p><img src="http://img.blog.csdn.net/20170526154350108?" alt="这里写图片描述"></p><p>那有一个问题，令$H_{10}$高阶权重w为0，为什么不直接使用$H_2$呢？这样做的目的是拓展我们的视野，为即将讨论的问题做准备。刚刚我们讨论的限制是$H_{10}$高阶部分的权重w限制为0，这是比较苛刻的一种限制。下面，我们把这个限制条件变得更宽松一点，即令任意8个权重w为0，并不非要限定$w_3=w_4=\cdots=w_{10}=0$，这个Looser Constraint可以写成：<br>$$\sum_{q=0}^{10}(w_q\neq0)\leq3$$</p><p>也就只是限定了w不为0的个数，并不限定必须是高阶的w。这种hypothesis记为$H_2’$，称为sparse hypothesis set，它与$H_2$和$H_{10}$的关系为：<br>$$H_2\subset H_2’\subset H_{10}$$</p><p><img src="http://img.blog.csdn.net/20170526160916246?" alt="这里写图片描述"></p><p>Looser Constraint对应的hypothesis应该更好解一些，但事实是sparse hypothesis set $H_2’$被证明也是NP-hard，求解非常困难。所以，还要转换为另一种易于求解的限定条件。</p><p>那么，我们寻找一种更容易求解的宽松的限定条件Softer Constraint，即：</p><p>$$\sum_{q=0}^{10}w_q^2=||w||^2\leq C$$</p><p>其中，C是常数，也就是说，所有的权重w的平方和的大小不超过C，我们把这种hypothesis sets记为$H(C)$。</p><p>$H_2’$与$H(C)$的关系是，它们之间有重叠，有交集的部分，但是没有完全包含的关系，也不一定相等。对应$H(C)$，C值越大，限定的范围越大，即越宽松：</p><p>$$H(0)\subset H(1.126)\subset \cdots \subset H(1126)\subset \cdots \subset H(\infty)=H_{10}$$</p><p>当C无限大的时候，即限定条件非常宽松，相当于没有加上任何限制，就与$H_{10}$没有什么两样。$H(C)$称为regularized hypothesis set，这种形式的限定条件是可以进行求解的，我们把求解的满足限定条件的权重w记为$w_{REG}$。接下来就要探讨如何求解$w_{REG}$。</p><h3 id="Weight-Decay-Regularization"><a href="#Weight-Decay-Regularization" class="headerlink" title="Weight Decay Regularization"></a>Weight Decay Regularization</h3><p>现在，针对H(c)，即加上限定条件，我们的问题变成：</p><p><img src="http://img.blog.csdn.net/20170526214203283?" alt="这里写图片描述"></p><p>我们的目的是计算$E_{in}(w)$的最小值，限定条件是$||w^2||\leq C$。这个限定条件从几何角度上的意思是，权重w被限定在半径为$\sqrt C$的圆内，而球外的w都不符合要求，即便它是靠近$E_{in}(w)$梯度为零的w。</p><p><img src="http://img.blog.csdn.net/20170527103629017?" alt="这里写图片描述"></p><p>下面用一张图来解释在限定条件下，最小化$E_{in}(w)$的过程：</p><p><img src="http://img.blog.csdn.net/20170527103932990?" alt="这里写图片描述"></p><p>如上图所示，假设在空间中的一点w，根据梯度下降算法，w会朝着$-\nabla E_{in}$的方向移动（图中蓝色箭头指示的方向），在没有限定条件的情况下，w最终会取得最小值$w_{lin}$，即“谷底”的位置。现在，加上限定条件，即w被限定在半径为$\sqrt C$的圆内，w距离原点的距离不能超过圆的半径，球如图中红色圆圈所示$w^Tw=C$。那么，这种情况下，w不能到达$w_{lin}$的位置，最大只能位于圆上，沿着圆的切线方向移动（图中绿色箭头指示的方向）。与绿色向量垂直的向量（图中红色箭头指示的方向）是圆切线的法向量，即w的方向，w不能靠近红色箭头方向移动。那么随着迭代优化过程，只要$-\nabla E_{in}$与w点切线方向不垂直，那么根据向量知识，$-\nabla E_{in}$一定在w点切线方向上有不为零的分量，即w点会继续移动。只有当$-\nabla E_{in}$与绿色切线垂直，即与红色法向量平行的时候，$-\nabla E_{in}$在切线方向上没有不为零的分量了，也就表示这时w达到了最优解的位置。</p><p>有了这个平行的概念，我们就得到了获得最优解需要满足的性质：</p><p>$$\nabla E_{in}(w_{REG})+\frac{2\lambda}{N}w_{REG}=0$$</p><p>上面公式中的$\lambda$称为Lagrange multiplier，是用来解有条件的最佳化问题常用的数学工具，$\frac2N$是方便后面公式推导。那么我们的目标就变成了求解满足上面公式的$w_{REG}$。</p><p>之前我们推导过，线性回归的$E_{in}$的表达式为：</p><p>$$E_{in}=\frac1N\sum_{n=1}^N(x_n^Tw-y_n)^2$$</p><p>计算$E_{in}$梯度，并代入到平行条件中，得到：</p><p>$$\frac2N(Z^TZw_{REG}-Z^Ty)+\frac{2\lambda}Nw_{REG}=0$$</p><p>这是一个线性方程式，直接得到$w_{REG}$为：</p><p>$$w_{REG}=(Z^TZ+\lambda I)^{-1}Z^Ty$$</p><p>上式中包含了求逆矩阵的过程，因为$Z^TZ$是半正定矩阵，如果$\lambda$大于零，那么$Z^TZ+\lambda I$一定是正定矩阵，即一定可逆。另外提一下，统计学上把这叫做ridge regression，可以看成是linear regression的进阶版。</p><p>如果对于更一般的情况，例如逻辑回归问题中，$\nabla E_{in}$不是线性的，那么将其代入平行条件中得到的就不是一个线性方程式，$w_{REG}$不易求解。下面我们从另一个角度来看一下平行等式：</p><p>$$\nabla E_{in}(w_{REG})+\frac{2\lambda}{N}w_{REG}=0$$</p><p>已知$\nabla E_{in}$是$E_{in}$对$w_{REG}$的导数，而$\frac{2\lambda}{N}w_{REG}$也可以看成是$\frac{\lambda}Nw_{REG}^2$的导数。那么平行等式左边可以看成一个函数的导数，导数为零，即求该函数的最小值。也就是说，问题转换为最小化该函数：</p><p>$$E_{aug}(w)=E_{in}(w)+\frac{\lambda}Nw^Tw$$</p><p>该函数中第二项就是限定条件regularizer，也称为weight-decay regularization。我们把这个函数称为Augmented Error，即$E_{aug}(w)$。</p><p>如果$\lambda$不为零，对应于加上了限定条件，若$\lambda$等于零，则对应于没有任何限定条件，问题转换成之前的最小化$E_{in}(w)$。</p><p>下面给出一个曲线拟合的例子，$\lambda$取不同的值时，得到的曲线也不相同：</p><p><img src="http://img.blog.csdn.net/20170531165847641?" alt="这里写图片描述"></p><p>从图中可以看出，当$\lambda=0$时，发生了过拟合；当$\lambda=0.0001$时，拟合的效果很好；当$\lambda=0.01$和$\lambda=1$时，发生了欠拟合。我们可以把$\lambda$看成是一种penality，即对hypothesis复杂度的惩罚，$\lambda$越大，w就越小，对应于C值越小，即这种惩罚越大，拟合曲线就会越平滑，高阶项就会削弱，容易发生欠拟合。$\lambda$一般取比较小的值就能达到良好的拟合效果，过大过小都有问题，但究竟取什么值，要根据具体训练数据和模型进行分析与调试。</p><p><img src="http://img.blog.csdn.net/20170531171049190?" alt="这里写图片描述"></p><p>事实上，这种regularization不仅可以用在多项式的hypothesis中，还可以应用在logistic regression等其他hypothesis中，都可以达到防止过拟合的效果。</p><p>我们目前讨论的多项式是形如$x,x^2,x^3,\cdots,x^n$的形式，若x的范围限定在[-1,1]之间，那么可能导致$x^n$相对于低阶的值要小得多，则其对于的w非常大，相当于要给高阶项设置很大的惩罚。为了避免出现这种数据大小差别很大的情况，可以使用Legendre Polynomials代替$x,x^2,x^3,\cdots,x^n$这种形式，Legendre Polynomials各项之间是正交的，用它进行多项式拟合的效果更好。关于Legendre Polynomials的概念这里不详细介绍，有兴趣的童鞋可以看一下<a href="https://en.wikipedia.org/wiki/Legendre_polynomials" target="_blank" rel="noopener">维基百科</a>。</p><h3 id="Regularization-and-VC-Theory"><a href="#Regularization-and-VC-Theory" class="headerlink" title="Regularization and VC Theory"></a>Regularization and VC Theory</h3><p>下面我们研究一下Regularization与VC理论之间的关系。Augmented Error表达式如下：</p><p>$$E_{aug}(w)=E_{in}(w)+\frac{\lambda}Nw^Tw$$</p><p>VC Bound表示为：</p><p>$$E_{out}(w)\leq E_{in}(w)+\Omega(H)$$</p><p>其中$w^Tw$表示的是单个hypothesis的复杂度，记为$\Omega(w)$；而$\Omega(H)$表示整个hypothesis set的复杂度。根据Augmented Error和VC Bound的表达式，$\Omega(w)$包含于$\Omega(H)$之内，所以，$E_{aug}(w)$比$E_{in}$更接近于$E_{out}$，即更好地代表$E_{out}$，$E_{aug}(w)$与$E_{out}$之间的误差更小。</p><p><img src="http://img.blog.csdn.net/20170531200442866?" alt="这里写图片描述"></p><p>根据VC Dimension理论，整个hypothesis set的$d_{VC}=\breve d+1$，这是因为所有的w都考虑了，没有任何限制条件。而引入限定条件的$d_{VC}(H(C))=d_{EFF}(H,A)$，即有效的VC dimension。也就是说，$d_{VC}(H)$比较大，因为它代表了整个hypothesis set，但是$d_{EFF}(H,A)$比较小，因为由于regularized的影响，限定了w只取一小部分。其中A表示regularized算法。当$\lambda&gt;0$时，有：</p><p>$$d_{EFF}(H,A)\leq d_{VC}$$</p><p>这些与实际情况是相符的，比如对多项式拟合模型，当$\lambda=0$时，所有的w都给予考虑，相应的$d_{VC}$很大，容易发生过拟合。当$\lambda&gt;0$且越来越大时，很多w将被舍弃，$d_{EFF}(H,A)$减小，拟合曲线越来越平滑，容易发生欠拟合。</p><h3 id="General-Regularizers"><a href="#General-Regularizers" class="headerlink" title="General Regularizers"></a>General Regularizers</h3><p>那么通用的Regularizers，即$\Omega(w)$，应该选择什么样的形式呢？一般地，我们会朝着目标函数的方向进行选取。有三种方式：</p><ul><li><p><strong>target-dependent</strong></p></li><li><p><strong>plausible</strong></p></li><li><p><strong>friendly</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170531203944555?" alt="这里写图片描述"></p><p>其实这三种方法跟之前error measure类似，其也有三种方法：</p><ul><li><p><strong>user-dependent</strong></p></li><li><p><strong>plausible</strong></p></li><li><p><strong>friendly</strong></p></li></ul><p>regularizer与error measure是机器学习模型设计中的重要步骤。</p><p><img src="http://img.blog.csdn.net/20170531204300526?" alt="这里写图片描述"></p><p>接下来，介绍两种Regularizer：L2和L1。L2 Regularizer一般比较通用，其形式如下：</p><p>$$\Omega(w)=\sum_{q=0}^Qw_q^2=||w||_2^2$$</p><p>这种形式的regularizer计算的是w的平方和，是凸函数，比较平滑，易于微分，容易进行最优化计算。</p><p>L1 Regularizer的表达式如下：</p><p>$$\Omega(w)=\sum_{q=0}^Q|w_q|=||w||_1$$</p><p>L1计算的不是w的平方和，而是绝对值和，即长度和，也是凸函数。已知$w^Tw=C$围成的是圆形，而$||w||<em>1=C$围成的是正方形，那么在正方形的四个顶点处，是不可微分的（不像圆形，处处可微分）。根据之前介绍的平行等式推导过程，对应这种正方形，它的解大都位于四个顶点处（不太理解，欢迎补充赐教），因为正方形边界处的w绝对值都不为零，若$-\nabla E</em>{in}$不与其平行，那么w就会向顶点处移动，顶点处的许多w分量为零，所以，L1 Regularizer的解是稀疏的，称为sparsity。优点是计算速度快。</p><p><img src="http://img.blog.csdn.net/20170531211835309?" alt="这里写图片描述"></p><p>下面来看一下$\lambda$如何取值，首先，若stochastic noise不同，那么一般情况下，$\lambda$取值有如下特点：</p><p><img src="http://img.blog.csdn.net/20170531212104701?" alt="这里写图片描述"></p><p>从图中可以看出，stochastic noise越大，$\lambda$越大。</p><p>另一种情况，不同的deterministic noise，$\lambda$取值有如下特点：</p><p><img src="http://img.blog.csdn.net/20170531212415879?" alt="这里写图片描述"></p><p>从图中可以看出，deterministic noise越大，$\lambda$越大。</p><p>以上两种noise的情况下，都是noise越大，相应的$\lambda$也就越大。这也很好理解，如果在开车的情况下，路况也不好，即noise越多，那么就越会踩刹车，这里踩刹车指的就是regularization。但是大多数情况下，noise是不可知的，这种情况下如何选择$\lambda$？这部分内容，我们下节课将会讨论。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Regularization。首先，原来的hypothesis set加上一些限制条件，就成了Regularized Hypothesis Set。加上限制条件之后，我们就可以把问题转化为$E_{aug}$最小化问题，即把w的平方加进去。这种过程，实际上回降低VC Dimension。最后，介绍regularization是通用的机器学习工具，设计方法通常包括target-dependent，plausible，friendly等等。下节课将介绍如何选取合适的$\lambda$来建立最佳拟合模型。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170526135401178?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记13 -- Hazard of Overfitting</title>
    <link href="https://redstonewill.github.io/2018/03/17/13/"/>
    <id>https://redstonewill.github.io/2018/03/17/13/</id>
    <published>2018-03-17T12:34:23.000Z</published>
    <updated>2018-03-17T14:11:56.501Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170524101427907?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了非线性分类模型，通过非线性变换，将非线性模型映射到另一个空间，转换为线性模型，再来进行分类，分析了非线性变换可能会使计算复杂度增加。本节课介绍这种模型复杂度增加带来机器学习中一个很重要的问题：过拟合（overfitting）。</p><h3 id="What-is-Overfitting"><a href="#What-is-Overfitting" class="headerlink" title="What is Overfitting?"></a>What is Overfitting?</h3><p>首先，我们通过一个例子来介绍什么bad generalization。假设平面上有5个点，目标函数f(x)是2阶多项式，如果hypothesis是二阶多项式加上一些小的noise的话，那么这5个点很靠近这个hypothesis，$E_{in}$很小。如果hypothesis是4阶多项式，那么这5点会完全落在hypothesis上，$E_{in}=0$。虽然4阶hypothesis的$E_{in}$比2阶hypothesis的要好很多，但是它的$E_{out}$很大。因为根据VC Bound理论，阶数越大，即VC Dimension越大，就会让模型复杂度更高，$E_{out}$更大。我们把这种$E_{in}$很小，$E_{out}$很大的情况称之为bad generation，即泛化能力差。</p><p><img src="http://img.blog.csdn.net/20170524101427907?" alt="这里写图片描述"></p><p>我们回过头来看一下VC曲线：</p><p><img src="http://img.blog.csdn.net/20170524102634154?" alt="这里写图片描述"></p><p>hypothesis的阶数越高，表示VC Dimension越大。随着VC Dimension增大，$E_{in}$是一直减小的，而$E_{out}$先减小后增大。在$d^*$位置，$E_{out}$取得最小值。在$d_{VC}^*$右侧，随着VC Dimension越来越大，$E_{in}$越来越小，接近于0，$E_{out}$越来越大。即当VC Dimension很大的时候，这种对训练样本拟合过分好的情况称之为过拟合（overfitting）。另一方面，在$d_{VC}^*$左侧，随着VC Dimension越来越小，$E_{in}$和$E_{out}$都越来越大，这种情况称之为欠拟合（underfitting），即模型对训练样本的拟合度太差，VC Dimension太小了。</p><p><img src="http://img.blog.csdn.net/20170524131656547?" alt="这里写图片描述"></p><p>bad generation和overfitting的关系可以理解为：overfitting是VC Dimension过大的一个过程，bad generation是overfitting的结果。</p><p><img src="http://img.blog.csdn.net/20170524132024546?" alt="这里写图片描述"></p><p>一个好的fit，$E_{in}$和$E_{out}$都比较小，尽管$E_{in}$没有足够接近零；而对overfitting来说，$E_{in}\approx0$，但是$E_{out}$很大。那么，overfitting的原因有哪些呢？</p><p>我们举个开车的例子，把发生车祸比作成overfitting，那么造成车祸的原因包括：</p><ul><li><p><strong>车速太快（VC Dimension太大）；</strong></p></li><li><p><strong>道路崎岖（noise）；</strong></p></li><li><p><strong>对路况的了解程度（训练样本数量N不够）；</strong></p></li></ul><p>也就是说，VC Dimension、noise、N这三个因素是影响过拟合现象的关键。</p><p><img src="http://img.blog.csdn.net/20170524141212919?" alt="这里写图片描述"></p><h3 id="The-Role-of-Noise-and-Data-Size"><a href="#The-Role-of-Noise-and-Data-Size" class="headerlink" title="The Role of Noise and Data Size"></a>The Role of Noise and Data Size</h3><p>为了尽可能详细地解释overfitting，我们进行这样一个实验，试验中的数据集不是很大。首先，在二维平面上，一个模型的分布由目标函数f(x)（x的10阶多项式）加上一些noise构成，下图中，离散的圆圈是数据集，目标函数是蓝色的曲线。数据没有完全落在曲线上，是因为加入了noise。</p><p><img src="http://img.blog.csdn.net/20170524141922827?" alt="这里写图片描述"></p><p>然后，同样在二维平面上，另一个模型的分布由目标函数f(x)（x的50阶多项式）构成，没有加入noise。下图中，离散的圆圈是数据集，目标函数是蓝色的曲线。可以看出由于没有noise，数据集完全落在曲线上。</p><p><img src="http://img.blog.csdn.net/20170524142417621?" alt="这里写图片描述"></p><p>现在，有两个学习模型，一个是2阶多项式，另一个是10阶多项式，分别对上面两个问题进行建模。首先，对于第一个目标函数是10阶多项式包含noise的问题，这两个学习模型的效果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170524144520619?" alt="这里写图片描述"></p><p>由上图可知，2阶多项式的学习模型$E_{in}=0.050$，$E_{out}=0.127$；10阶多项式的学习模型$E_{in}=0.034$，$E_{out}=9.00$。虽然10阶模型的$E_{in}$比2阶的小，但是其$E_{out}$要比2阶的大得多，而2阶的$E_{in}$和$E_{out}$相差不大，很明显用10阶的模型发生了过拟合。</p><p>然后，对于第二个目标函数是50阶多项式没有noise的问题，这两个学习模型的效果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170524153305297?" alt="这里写图片描述"></p><p>由上图可知，2阶多项式的学习模型$E_{in}=0.029$，$E_{out}=0.120$；10阶多项式的学习模型$E_{in}=0.00001$，$E_{out}=7680$。虽然10阶模型的$E_{in}$比2阶的小，但是其$E_{out}$要比2阶的大得多的多，而2阶的$E_{in}$和$E_{out}$相差不大，很明显用10阶的模型仍然发生了明显的过拟合。</p><p>上面两个问题中，10阶模型都发生了过拟合，反而2阶的模型却表现得相对不错。这好像违背了我们的第一感觉，比如对于目标函数是10阶多项式，加上noise的模型，按道理来说应该是10阶的模型更能接近于目标函数，因为它们阶数相同。但是，事实却是2阶模型泛化能力更强。这种现象产生的原因，从哲学上来说，就是“以退为进”。有时候，简单的学习模型反而能表现的更好。</p><p>下面从learning curve来分析一下具体的原因，learning curve描述的是$E_{in}$和$E_{out}$随着数据量N的变化趋势。下图中左边是2阶学习模型的learning curve，右边是10阶学习模型的learning curve。</p><p><img src="http://img.blog.csdn.net/20170524155529000?" alt="这里写图片描述"></p><p>我们的第9次课的笔记<a href="http://blog.csdn.net/red_stone1/article/details/71599034" target="_blank" rel="noopener"> NTU林轩田机器学习基石课程学习笔记9 – Linear Regression</a>已经介绍过了learning curve。在learning curve中，横轴是样本数量N，纵轴是Error。$E_{in}$和$E_{out}$可表示为：</p><p>$$E_{in}=noise level\ast (1-\frac{d+1}N) $$</p><p>$$E_{out}=noise level\ast (1+\frac{d+1}N) $$</p><p>其中d为模型阶次，左图中d=2，右图中d=10。</p><p>本节的实验问题中，数据量N不大，即对应于上图中的灰色区域。左图的灰色区域中，因为d=2，$E_{in}$和$E_{out}$相对来说比较接近；右图中的灰色区域中，d=10，根据$E_{in}$和$E_{out}$的表达式，$E_{in}$很小，而$E_{out}$很大。这就解释了之前2阶多项式模型的$E_{in}$更接近$E_{out}$，泛化能力更好。</p><p>值得一提的是，如果数据量N很大的时候，上面两图中$E_{in}$和$E_{out}$都比较接近，但是对于高阶模型，z域中的特征很多的时候，需要的样本数量N很大，且容易发生维度灾难。关于维度灾难的详细生动解释，请参考我另一篇博文：</p><p><a href="http://blog.csdn.net/red_stone1/article/details/71692444" target="_blank" rel="noopener">机器学习中的维度灾难</a></p><p>另一个例子中，目标函数是50阶多项式，且没有加入noise。这种情况下，我们发现仍然是2阶的模型拟合的效果更好一些，明明没有noise，为什么是这样的结果呢？</p><p>实际上，我们忽略了一个问题：这种情况真的没有noise吗？其实，当模型很复杂的时候，即50阶多项式的目标函数，无论是2阶模型还是10阶模型，都不能学习的很好，这种复杂度本身就会引入一种‘noise’。所以，这种高阶无noise的问题，也可以类似于10阶多项式的目标函数加上noise的情况，只是二者的noise有些许不同，下面一部分将会详细解释。</p><h3 id="Deterministic-Noise"><a href="#Deterministic-Noise" class="headerlink" title="Deterministic Noise"></a>Deterministic Noise</h3><p>下面我们介绍一个更细节的实验来说明 什么时候小心overfit会发生。假设我们产生的数据分布由两部分组成：第一部分是目标函数f(x)，$Q_f$阶多项式；第二部分是噪声$\epsilon$，服从Gaussian分布。接下来我们分析的是noise强度不同对overfitting有什么样的影响。总共的数据量是N。</p><p><img src="http://img.blog.csdn.net/20170524164606683?" alt="这里写图片描述"></p><p>那么下面我们分析不同的$(N,\sigma^2)$和$(N,Q_f)$对overfit的影响。overfit可以量化为$E_{out}-E_{in}$。结果如下：</p><p><img src="http://img.blog.csdn.net/20170524165036419?" alt="这里写图片描述"></p><p>上图中，红色越深，代表overfit程度越高，蓝色越深，代表overfit程度越低。先看左边的图，左图中阶数$Q_f$固定为20，横坐标代表样本数量N，纵坐标代表噪声水平$\sigma^2$。红色区域集中在N很小或者$\sigma^2$很大的时候，也就是说N越大，$\sigma^2$越小，越不容易发生overfit。右边图中$\sigma^2=0.1$，横坐标代表样本数量N，纵坐标代表目标函数阶数$Q_f$。红色区域集中在N很小或者$Q_f$很大的时候，也就是说N越大，$Q_f$越小，越不容易发生overfit。上面两图基本相似。</p><p>从上面的分析，我们发现$\sigma^2$对overfit是有很大的影响的，我们把这种noise称之为stochastic noise。同样地，$Q_f$即模型复杂度也对overfit有很大影响，而且二者影响是相似的，所以我们把这种称之为deterministic noise。之所以把它称为noise，是因为模型高复杂度带来的影响。</p><p>总结一下，有四个因素会导致发生overfitting：</p><ul><li><p><strong>data size N $\downarrow$</strong></p></li><li><p><strong>stochastic noise $\sigma^2\uparrow$</strong></p></li><li><p><strong>deterministic noise $Q_f\uparrow$</strong></p></li><li><p><strong>excessive power $\uparrow$</strong></p></li></ul><p>我们刚才解释了如果目标函数f(x)的复杂度很高的时候，那么跟有noise也没有什么两样。因为目标函数很复杂，那么再好的hypothesis都会跟它有一些差距，我们把这种差距称之为deterministic noise。deterministic noise与stochastic noise不同，但是效果一样。其实deterministic noise类似于一个伪随机数发生器，它不会产生真正的随机数，而只产生伪随机数。它的值与hypothesis有关，且固定点x的deterministic noise值是固定的。</p><h3 id="Dealing-with-Overfitting"><a href="#Dealing-with-Overfitting" class="headerlink" title="Dealing with Overfitting"></a>Dealing with Overfitting</h3><p>现在我们知道了什么是overfitting，和overfitting产生的原因，那么如何避免overfitting呢？避免overfitting的方法主要包括：</p><ul><li><p><strong>start from simple model</strong></p></li><li><p><strong>data cleaning/pruning</strong></p></li><li><p><strong>data hinting</strong></p></li><li><p><strong>regularization</strong></p></li><li><p><strong>validataion</strong></p></li></ul><p>这几种方法类比于之前举的开车的例子，对应如下：</p><p><img src="http://img.blog.csdn.net/20170524215244275?" alt="这里写图片描述"></p><p>regularization和validation我们之后的课程再介绍，本节课主要介绍简单的data cleaning/pruning和data hinting两种方法。</p><p>data cleaning/pruning就是对训练数据集里label明显错误的样本进行修正（data cleaning），或者对错误的样本看成是noise，进行剔除（data pruning）。data cleaning/pruning关键在于如何准确寻找label错误的点或者是noise的点，而且如果这些点相比训练样本N很小的话，这种处理效果不太明显。</p><p>data hinting是针对N不够大的情况，如果没有办法获得更多的训练集，那么data hinting就可以对已知的样本进行简单的处理、变换，从而获得更多的样本。举个例子，数字分类问题，可以对已知的数字图片进行轻微的平移或者旋转，从而让N丰富起来，达到扩大训练集的目的。这种额外获得的例子称之为virtual examples。但是要注意一点的就是，新获取的virtual examples可能不再是iid某个distribution。所以新构建的virtual examples要尽量合理，且是独立同分布的。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了overfitting的概念，即当$E_{in}$很小，$E_{out}$很大的时候，会出现overfitting。详细介绍了overfitting发生的四个常见原因data size N、stochastic noise、deterministic noise和excessive power。解决overfitting的方法有很多，本节课主要介绍了data cleaning/pruning和data hinting两种简单的方法，之后的课程将会详细介绍regularization和validataion两种更重要的方法。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170524101427907?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记12 -- Nonlinear Transformation</title>
    <link href="https://redstonewill.github.io/2018/03/17/12/"/>
    <id>https://redstonewill.github.io/2018/03/17/12/</id>
    <published>2018-03-17T12:27:40.000Z</published>
    <updated>2018-03-17T12:33:25.012Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170523005800773?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们介绍了分类问题的三种线性模型，可以用来解决binary classification和multiclass classification问题。本节课主要介绍非线性的模型来解决分类问题。</p><h3 id="Quadratic-Hypothesis"><a href="#Quadratic-Hypothesis" class="headerlink" title="Quadratic Hypothesis"></a>Quadratic Hypothesis</h3><p>之前介绍的线性模型，在2D平面上是一条直线，在3D空间中是一个平面。数学上，我们用线性得分函数s来表示：$s=w^Tx$。其中，x为特征值向量，w为权重，s是线性的。</p><p><img src="http://img.blog.csdn.net/20170522205736151?" alt="这里写图片描述"></p><p>线性模型的优点就是，它的VC Dimension比较小，保证了$E_{in}\approx E_{out}$。但是缺点也很明显，对某些非线性问题，可能会造成$E_{in}$很大，虽然$E_{in}\approx E_{out}$，但是也造成$E_{out}$很大，分类效果不佳。</p><p><img src="http://img.blog.csdn.net/20170522210031309?" alt="这里写图片描述"></p><p>为了解决线性模型的缺点，我们可以使用非线性模型来进行分类。例如数据集D不是线性可分的，而是圆形可分的，圆形内部是正类，外面是负类。假设它的hypotheses可以写成：<br>$$h_{SEP}(x)=sign(-x_1^2-x_2^2+0.6)$$<br>基于这种非线性思想，我们之前讨论的PLA、Regression问题都可以有非线性的形式进行求解。</p><p><img src="http://img.blog.csdn.net/20170522210959142?" alt="这里写图片描述"></p><p>下面介绍如何设计这些非线性模型的演算法。还是上面介绍的平面圆形分类例子，它的h(x)的权重w0=0.6，w1=-1，w2=-1，但是h(x)的特征不是线性模型的$(1,x_1,x_2)$，而是$(1,x_1^2,x_2^2)$。我们令$z_0=1$，$z_1=x_1^2$，$z_2=x_2^2$，那么，h(x)变成：</p><p>$$h(x)=sign(\breve{w}_0\cdot z_0+\breve{w}_1\cdot z_1+\breve{w}_2\cdot z_2)=sign(0.6\cdot z_0-1\cdot z_1-1\cdot z_2)=sign(\breve{w}^Tz)$$</p><p>这种$x_n\rightarrow z_n$的转换可以看成是x空间的点映射到z空间中去，而在z域中，可以用一条直线进行分类，也就是从x空间的圆形可分映射到z空间的线性可分。z域中的直线对应于x域中的圆形。因此，我们把$x_n\rightarrow z_n$这个过程称之为特征转换（Feature Transform）。通过这种特征转换，可以将非线性模型转换为另一个域中的线性模型。</p><p><img src="http://img.blog.csdn.net/20170523005800773?" alt="这里写图片描述"></p><p>已知x域中圆形可分在z域中是线性可分的，那么反过来，如果在z域中线性可分，是否在x域中一定是圆形可分的呢？答案是否定的。由于权重向量w取值不同，x域中的hypothesis可能是圆形、椭圆、双曲线等等多种情况。</p><p><img src="http://img.blog.csdn.net/20170523091547803?" alt="这里写图片描述"></p><p>目前讨论的x域中的圆形都是圆心过原点的，对于圆心不过原点的一般情况，$x_n\rightarrow z_n$映射公式包含的所有项为：</p><p>$$\Phi_2(x)=(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)$$</p><p>也就是说，对于二次hypothesis，它包含二次项、一次项和常数项1，z域中每一条线对应x域中的某二次曲线的分类方式，也许是圆，也许是椭圆，也许是双曲线等等。那么z域中的hypothesis可以写成：</p><p><img src="http://img.blog.csdn.net/20170523093213481?" alt="这里写图片描述"></p><h3 id="Nonlinear-Transform"><a href="#Nonlinear-Transform" class="headerlink" title="Nonlinear Transform"></a>Nonlinear Transform</h3><p>上一部分我们定义了什么了二次hypothesis，那么这部分将介绍如何设计一个好的二次hypothesis来达到良好的分类效果。那么目标就是在z域中设计一个最佳的分类线。</p><p><img src="http://img.blog.csdn.net/20170523094016913?" alt="这里写图片描述"></p><p>其实，做法很简单，利用映射变换的思想，通过映射关系，把x域中的最高阶二次的多项式转换为z域中的一次向量，也就是从quardratic hypothesis转换成了perceptrons问题。用z值代替x多项式，其中向量z的个数与x域中x多项式的个数一致（包含常数项）。这样就可以在z域中利用线性分类模型进行分类训练。训练好的线性模型之后，再将z替换为x的多项式就可以了。具体过程如下：</p><p><img src="http://img.blog.csdn.net/20170523095731798?" alt="这里写图片描述"></p><p>整个过程就是通过映射关系，换个空间去做线性分类，重点包括两个：</p><ul><li><p>特征转换</p></li><li><p>训练线性模型</p></li></ul><p>其实，我们以前处理机器学习问题的时候，已经做过类似的特征变换了。比如数字识别问题，我们从原始的像素值特征转换为一些实际的concrete特征，比如密度、对称性等等，这也用到了feature transform的思想。</p><p><img src="http://img.blog.csdn.net/20170523100508738?" alt="这里写图片描述"></p><h3 id="Price-of-Nonlinear-Transform"><a href="#Price-of-Nonlinear-Transform" class="headerlink" title="Price of Nonlinear Transform"></a>Price of Nonlinear Transform</h3><p>若x特征维度是d维的，也就是包含d个特征，那么二次多项式个数，即z域特征维度是：<br>$$\breve d=1+C_d^1+C_d^2+d=\frac{d(d+3)}2+1$$<br>如果x特征维度是2维的，即$(x_1,x_2)$，那么它的二次多项式为$(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)$，有6个。</p><p>现在，如果阶数更高，假设阶数为Q，那么对于x特征维度是d维的，它的z域特征维度为：<br>$$\breve d=C_{Q+d}^Q=C_{Q+d}^d=O(Q^d)$$<br>由上式可以看出，计算z域特征维度个数的时间复杂度是Q的d次方，随着Q和d的增大，计算量会变得很大。同时，空间复杂度也大。也就是说，这种特征变换的一个代价是计算的时间、空间复杂度都比较大。</p><p><img src="http://img.blog.csdn.net/20170523105942602?" alt="这里写图片描述"></p><p>另一方面，z域中特征个数随着Q和d增加变得很大，同时权重w也会增大，即自由度增加，VC Dimension增大。令z域中的特征维度是$1+\breve d$，则在在域中，任何$\breve d+2$的输入都不能被shattered；同样，在x域中，任何$\breve d+2$的输入也不能被shattered。$\breve d+1$是VC Dimension的上界，如果$\breve d+1$很大的时候，相应的VC Dimension就会很大。根据之前章节课程的讨论，VC Dimension过大，模型的泛化能力会比较差。</p><p><img src="http://img.blog.csdn.net/20170523111019007?" alt="这里写图片描述"></p><p>下面通过一个例子来解释为什么VC Dimension过大，会造成不好的分类效果：</p><p><img src="http://img.blog.csdn.net/20170523111216948?" alt="这里写图片描述"></p><p>上图中，左边是用直线进行线性分类，有部分点分类错误；右边是用四次曲线进行非线性分类，所有点都分类正确，那么哪一个分类效果好呢？单从平面上这些训练数据来看，四次曲线的分类效果更好，但是四次曲线模型很容易带来过拟合的问题，虽然它的$E_{in}$比较小，从泛化能力上来说，还是左边的分类器更好一些。也就是说VC Dimension过大会带来过拟合问题，$\breve d+1$不能太大了。</p><p>那么如何选择合适的Q，来保证不会出现过拟合问题，使模型的泛化能力强呢？一般情况下，为了尽量减少特征自由度，我们会根据训练样本的分布情况，人为地减少、省略一些项。但是，这种人为地删减特征会带来一些“自我分析”代价，虽然对训练样本分类效果好，但是对训练样本外的样本，不一定效果好。所以，一般情况下，还是要保存所有的多项式特征，避免对训练样本的人为选择。</p><p><img src="http://img.blog.csdn.net/20170523113636362?" alt="这里写图片描述"></p><h3 id="Structured-Hypothesis-Sets"><a href="#Structured-Hypothesis-Sets" class="headerlink" title="Structured Hypothesis Sets"></a>Structured Hypothesis Sets</h3><p>下面，我们讨论一下从x域到z域的多项式变换。首先，如果特征维度只有1维的话，那么变换多项式只有常数项：</p><p>$$\Phi_0(x)=(1)$$</p><p>如果特征维度是两维的，变换多项式包含了一维的$\Phi_0(x)$：</p><p>$$\Phi_1(x)=(\Phi_0(x),x_1,x_2,\ldots,x_d)$$</p><p>如果特征维度是三维的，变换多项式包含了二维的$\Phi_1(x)$：</p><p>$$\Phi_2(x)=(\Phi_1(x),x_1^2,x_1x_2,\ldots,x_d^2)$$</p><p>以此类推，如果特征维度是Q次，那么它的变换多项式为：</p><p>$$\Phi_Q(x)=(\Phi_{Q-1}(x),x_1^Q,x_1^{Q-1}x_2,\cdots,x_d^Q)$$</p><p>那么对于不同阶次构成的hypothesis有如下关系：</p><p>$$H_{\Phi_0} \subset H_{\Phi_1} \subset H_{\Phi_2} \subset \cdots \subset H_{\Phi_Q}$$</p><p>我们把这种结构叫做Structured Hypothesis Sets：</p><p><img src="http://img.blog.csdn.net/20170523132015872?" alt="这里写图片描述"></p><p>那么对于这种Structured Hypothesis Sets，它们的VC Dimension满足下列关系：</p><p>$$d_{VC}(H_0)\leq d_{VC}(H_1)\leq d_{VC}(H_2)\leq \cdots \leq d_{VC}(H_Q)$$</p><p>它的$E_{in}$满足下列关系：</p><p>$$E_{in}(g_0)\geq E_{in}(g_1)\geq E_{in}(g_2)\geq \cdots \geq E_{in}(g_Q)$$</p><p><img src="http://img.blog.csdn.net/20170523133004123?" alt="这里写图片描述"></p><p>从上图中也可以看到，随着变换多项式的阶数增大，虽然$E_{in}$逐渐减小，但是model complexity会逐渐增大，造成$E_{out}$很大，所以阶数不能太高。</p><p>那么，如果选择的阶数很大，确实能使$E_{in}$接近于0，但是泛化能力通常很差，我们把这种情况叫做tempting sin。所以，一般最合适的做法是先从低阶开始，如先选择一阶hypothesis，看看$E_{in}$是否很小，如果$E_{in}$足够小的话就选择一阶，如果$E_{in}$大的话，再逐渐增加阶数，直到满足要求为止。也就是说，尽量选择低阶的hypothes，这样才能得到较强的泛化能力。</p><p><img src="http://img.blog.csdn.net/20170523133948471?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>这节课主要介绍了非线性分类模型，通过非线性变换，将非线性模型映射到另一个空间，转换为线性模型，再来进行线性分类。本节课完整介绍了非线性变换的整体流程，以及非线性变换可能会带来的一些问题：时间复杂度和空间复杂度的增加。最后介绍了在要付出代价的情况下，使用非线性变换的最安全的做法，尽可能使用简单的模型，而不是模型越复杂越好。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170523005800773?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记11 -- Linear Models for Classification</title>
    <link href="https://redstonewill.github.io/2018/03/17/11/"/>
    <id>https://redstonewill.github.io/2018/03/17/11/</id>
    <published>2018-03-17T06:44:47.000Z</published>
    <updated>2018-03-17T06:46:21.892Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170517220835526?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们介绍了Logistic Regression问题，建立cross-entropy error，并提出使用梯度下降算法gradient descnt来获得最好的logistic hypothesis。本节课继续介绍使用线性模型来解决分类问题。</p><h3 id="Linear-Models-for-Binary-Classification"><a href="#Linear-Models-for-Binary-Classification" class="headerlink" title="Linear Models for Binary Classification"></a>Linear Models for Binary Classification</h3><p>之前介绍几种线性模型都有一个共同点，就是都有样本特征x的加权运算，我们引入一个线性得分函数s：</p><p>$$s=w^Tx$$</p><p>三种线性模型，第一种是linear classification。线性分类模型的hypothesis为$h(x)=sign(s)$,取值范围为{-1,+1}两个值，它的err是0/1的，所以对应的$E_{in}(w)$是离散的，并不好解，这是个NP-hard问题。第二种是linear regression。线性回归模型的hypothesis为$h(x)=s$，取值范围为整个实数空间，它的err是squared的，所以对应的$E_{in}(w)$是开口向上的二次曲线，其解是closed-form的，直接用线性最小二乘法求解即可。第三种是logistic regression。逻辑回归模型的hypothesis为$h(x)=\theta(s)$，取值范围为(-1,1)之间，它的err是cross-entropy的，所有对应的$E_{in}(w)$是平滑的凸函数，可以使用梯度下降算法求最小值。</p><p><img src="http://img.blog.csdn.net/20170517220835526?" alt="这里写图片描述"></p><p>从上图中，我们发现，linear regression和logistic regression的error function都有最小解。那么可不可以用这两种方法来求解linear classification问题呢？下面，我们来对这三种模型的error function进行分析，看看它们之间有什么联系。</p><p>对于linear classification，它的error function可以写成：<br>$$err_{0/1}(s,y)=|sign(s)\neq y|=|sign(ys)\neq 1|$$<br>对于linear regression，它的error function可以写成：<br>$$err_{SQR}(s,y)=(s-y)^2=(ys-1)^2$$<br>对于logistic regression，它的error function可以写成：<br>$$err_{CE}(s,y)=ln(1+exp(-ys))$$<br>上述三种模型的error function都引入了ys变量，那么ys的物理意义是什么？ys就是指分类的正确率得分，其值越大越好，得分越高。</p><p><img src="http://img.blog.csdn.net/20170517232731911?" alt="这里写图片描述"></p><p>下面，我们用图形化的方式来解释三种模型的error function到底有什么关系：</p><p><img src="http://img.blog.csdn.net/20170518095055714?" alt="这里写图片描述"></p><p>从上图中可以看出，ys是横坐标轴，$err_{0/1}$是呈阶梯状的，在ys&gt;0时，$err_{0/1}$恒取最小值0。$err_{SQR}$呈抛物线形式，在ys=1时，取得最小值，且在ys=1左右很小区域内，$err_{0/1}$和$err_{SQR}$近似。$err_{CE}$是呈指数下降的单调函数，ys越大，其值越小。同样在ys=1左右很小区域内，$err_{0/1}$和$err_{CE}$近似。但是我们发现$err_{CE}$并不是始终在$err_{0/1}$之上，所以为了计算讨论方便，我们把$err_{CE}$做幅值上的调整，引入$err_{SCE}=log_2(1+exp(-ys))=\frac1{ln2}err_{CE}$，这样能保证$err_{SCE}$始终在$err_{0/1}$上面，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170518134827804?" alt="这里写图片描述"></p><p>由上图可以看出：<br>$$err_{0/1}(s,y)\leq err_{SCE}(s,y)=\frac1{ln2}err_{CE}(s,y)$$<br>$$E_{in}^{0/1}(w)\leq E_{in}^{SCE}(w)=\frac1{ln2}E_{in}^{CE}(w)$$<br>$$E_{out}^{0/1}(w)\leq E_{out}^{SCE}(w)=\frac1{ln2}E_{out}^{CE}(w)$$<br>那么由VC理论可以知道：<br>从0/1出发：<br>$$E_{out}^{0/1}(w)\leq E_{in}^{0/1}(w)+\Omega^{0/1}\leq \frac1{ln2}E_{in}^{CE}(w)+\Omega^{0/1}$$<br>从CE出发：<br>$$E_{out}^{0/1}(w)\leq \frac1{ln2}E_{out}^{CE}(w)\leq \frac1{ln2}E_{in}^{CE}(w)+\frac1{ln2}\Omega^{CE}$$</p><p><img src="http://img.blog.csdn.net/20170518205526337?" alt="这里写图片描述"></p><p>通过上面的分析，我们看到err 0/1是被限定在一个上界中。这个上界是由logistic regression模型的error function决定的。而linear regression其实也是linear classification的一个upper bound，只是随着sy偏离1的位置越来越远，linear regression的error function偏差越来越大。综上所述，linear regression和logistic regression都可以用来解决linear classification的问题。</p><p>下图列举了PLA、linear regression、logistic regression模型用来解linear classification问题的优点和缺点。通常，我们使用linear regression来获得初始化的$w_0$，再用logistic regression模型进行最优化解。</p><p><img src="http://img.blog.csdn.net/20170518142147055?" alt="这里写图片描述"></p><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>之前介绍的PLA算法和logistic regression算法，都是用到了迭代操作。PLA每次迭代只会更新一个点，它每次迭代的时间复杂度是O(1)；而logistic regression每次迭代要对所有N个点都进行计算，它的每时间复杂度是O(N)。为了提高logistic regression中gradient descent算法的速度，可以使用另一种算法：随机梯度下降算法(Stochastic Gradient Descent)。</p><p>随机梯度下降算法每次迭代只找到一个点，计算该点的梯度，作为我们下一步更新w的依据。这样就保证了每次迭代的计算量大大减小，我们可以把整体的梯度看成这个随机过程的一个期望值。</p><p><img src="http://img.blog.csdn.net/20170518144632739?" alt="这里写图片描述"></p><p>随机梯度下降可以看成是真实的梯度加上均值为零的随机噪声方向。单次迭代看，好像会对每一步找到正确梯度方向有影响，但是整体期望值上看，与真实梯度的方向没有差太多，同样能找到最小值位置。随机梯度下降的优点是减少计算量，提高运算速度，而且便于online学习；缺点是不够稳定，每次迭代并不能保证按照正确的方向前进，而且达到最小值需要迭代的次数比梯度下降算法一般要多。</p><p><img src="http://img.blog.csdn.net/20170518150924733?" alt="这里写图片描述"></p><p>对于logistic regression的SGD，它的表达式为：<br>$$w_{t+1}\leftarrow w_t+\eta\theta(-y_nw_t^Tx_n)(y_nx_n)$$</p><p>我们发现，SGD与PLA的迭代公式有类似的地方，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170518153018200?" alt="这里写图片描述"></p><p>我们把SGD logistic regression称之为’soft’ PLA，因为PLA只对分类错误的点进行修正，而SGD logistic regression每次迭代都会进行或多或少的修正。另外，当$\eta=1$，且$w_t^Tx_n$足够大的时候，PLA近似等于SGD。</p><p><img src="http://img.blog.csdn.net/20170518154930194?" alt="这里写图片描述"></p><p>除此之外，还有两点需要说明：1、SGD的终止迭代条件。没有统一的终止条件，一般让迭代次数足够多；2、学习速率$\eta$。$\eta$的取值是根据实际情况来定的，一般取值0.1就可以了。</p><h3 id="Multiclass-via-Logistic-Regression"><a href="#Multiclass-via-Logistic-Regression" class="headerlink" title="Multiclass via Logistic Regression"></a>Multiclass via Logistic Regression</h3><p>之前我们一直讲的都是二分类问题，本节主要介绍多分类问题，通过linear classification来解决。假设平面上有四个类，分别是正方形、菱形、三角形和星形，如何进行分类模型的训练呢？</p><p>首先我们可以想到这样一个办法，就是先把正方形作为正类，其他三种形状都是负类，即把它当成一个二分类问题，通过linear classification模型进行训练，得出平面上某个图形是不是正方形，且只有{-1,+1}两种情况。然后再分别以菱形、三角形、星形为正类，进行二元分类。这样进行四次二分类之后，就完成了这个多分类问题。</p><p><img src="http://img.blog.csdn.net/20170518195225232?" alt="这里写图片描述"></p><p>但是，这样的二分类会带来一些问题，因为我们只用{-1，+1}两个值来标记，那么平面上某些可能某些区域都被上述四次二分类模型判断为负类，即不属于四类中的任何一类；也可能会出现某些区域同时被两个类甚至多个类同时判断为正类，比如某个区域又判定为正方形又判定为菱形。那么对于这种情况，我们就无法进行多类别的准确判断，所以对于多类别，简单的binary classification不能解决问题。</p><p>针对这种问题，我们可以使用另外一种方法来解决：soft软性分类，即不用{-1，+1}这种binary classification，而是使用logistic regression，计算某点属于某类的概率、可能性，去概率最大的值为那一类就好。</p><p>soft classification的处理过程和之前类似，同样是分别令某类为正，其他三类为负，不同的是得到的是概率值，而不是{-1，+1}。最后得到某点分别属于四类的概率，取最大概率对应的哪一个类别就好。效果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170518200527283?" alt="这里写图片描述"></p><p>这种多分类的处理方式，我们称之为One-Versus-All(OVA) Decomposition。这种方法的优点是简单高效，可以使用logistic regression模型来解决；缺点是如果数据类别很多时，那么每次二分类问题中，正类和负类的数量差别就很大，数据不平衡unbalanced，这样会影响分类效果。但是，OVA还是非常常用的一种多分类算法。</p><p><img src="http://img.blog.csdn.net/20170518201255231?" alt="这里写图片描述"></p><h3 id="Multiclass-via-Binary-Classification"><a href="#Multiclass-via-Binary-Classification" class="headerlink" title="Multiclass via Binary Classification"></a>Multiclass via Binary Classification</h3><p>上一节，我们介绍了多分类算法OVA，但是这种方法存在一个问题，就是当类别k很多的时候，造成正负类数据unbalanced，会影响分类效果，表现不好。现在，我们介绍另一种方法来解决当k很大时，OVA带来的问题。</p><p>这种方法呢，每次只取两类进行binary classification，取值为{-1，+1}。假如k=4，那么总共需要进行$C_4^2=6$次binary classification。那么，六次分类之后，如果平面有个点，有三个分类器判断它是正方形，一个分类器判断是菱形，另外两个判断是三角形，那么取最多的那个，即判断它属于正方形，我们的分类就完成了。这种形式就如同k个足球对进行单循环的比赛，每场比赛都有一个队赢，一个队输，赢了得1分，输了得0分。那么总共进行了$C_k^2$次的比赛，最终取得分最高的那个队就可以了。</p><p><img src="http://img.blog.csdn.net/20170518203420275?" alt="这里写图片描述"></p><p>这种区别于OVA的多分类方法叫做One-Versus-One(OVO)。这种方法的优点是更加高效，因为虽然需要进行的分类次数增加了，但是每次只需要进行两个类别的比较，也就是说单次分类的数量减少了。而且一般不会出现数据unbalanced的情况。缺点是需要分类的次数多，时间复杂度和空间复杂度可能都比较高。</p><p><img src="http://img.blog.csdn.net/20170518203941888?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了分类问题的三种线性模型：linear classification、linear regression和logistic regression。首先介绍了这三种linear models都可以来做binary classification。然后介绍了比梯度下降算法更加高效的SGD算法来进行logistic regression分析。最后讲解了两种多分类方法，一种是OVA，另一种是OVO。这两种方法各有优缺点，当类别数量k不多的时候，建议选择OVA，以减少分类次数。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170517220835526?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记10 -- Logistic Regression</title>
    <link href="https://redstonewill.github.io/2018/03/17/10/"/>
    <id>https://redstonewill.github.io/2018/03/17/10/</id>
    <published>2018-03-17T06:42:15.000Z</published>
    <updated>2018-03-17T06:44:06.377Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170515225915831?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们介绍了Linear Regression线性回归，以及用平方错误来寻找最佳的权重向量w，获得最好的线性预测。本节课将介绍Logistic Regression逻辑回归问题。</p><h3 id="Logistic-Regression-Problem"><a href="#Logistic-Regression-Problem" class="headerlink" title="Logistic Regression Problem"></a>Logistic Regression Problem</h3><p>一个心脏病预测的问题：根据患者的年龄、血压、体重等信息，来预测患者是否会有心脏病。很明显这是一个二分类问题，其输出y只有{-1,1}两种情况。</p><p>二元分类，一般情况下，理想的目标函数f(x)&gt;0.5，则判断为正类1；若f(x)&lt;0.5，则判断为负类-1。</p><p><img src="http://img.blog.csdn.net/20170515225915831?" alt="这里写图片描述"></p><p>但是，如果我们想知道的不是患者有没有心脏病，而是到底患者有多大的几率是心脏病。这表示，我们更关心的是目标函数的值（分布在0,1之间），表示是正类的概率（正类表示是心脏病）。这跟我们原来讨论的二分类问题不太一样，我们把这个问题称为软性二分类问题（’soft’ binary classification）。这个值越接近1，表示正类的可能性越大；越接近0，表示负类的可能性越大。</p><p><img src="http://img.blog.csdn.net/20170515230741609?" alt="这里写图片描述"></p><p>对于软性二分类问题，理想的数据是分布在[0,1]之间的具体值，但是实际中的数据只可能是0或者1，我们可以把实际中的数据看成是理想数据加上了噪声的影响。</p><p><img src="http://img.blog.csdn.net/20170515232055978?" alt="这里写图片描述"></p><p>如果目标函数是$f(x)=P(+1|x)\in[0,1]$的话，我们如何找到一个好的Hypothesis跟这个目标函数很接近呢？</p><p>首先，根据我们之前的做法，对所有的特征值进行加权处理。计算的结果s，我们称之为’risk score’：</p><p><img src="http://img.blog.csdn.net/20170515233141740?" alt="这里写图片描述"></p><p>但是特征加权和$s\in(-\infty,+\infty)$，如何将s值限定在[0,1]之间呢？一个方法是使用sigmoid Function，记为$\theta(s)$。那么我们的目标就是找到一个hypothesis：$h(x)=\theta(w^Tx)$。</p><p><img src="http://img.blog.csdn.net/20170515233941909?" alt="这里写图片描述"></p><p>Sigmoid Function函数记为$\theta(s)=\frac1{1+e^{-s}}$，满足$\theta(-\infty)=0$，$\theta(0)=\frac12$，$\theta(+\infty)=1$。这个函数是平滑的、单调的S型函数。则对于逻辑回归问题，hypothesis就是这样的形式：</p><p>$$h(x)=\frac1{1+e^{-w^Tx}}$$</p><p>那我们的目标就是求出这个预测函数h(x)，使它接近目标函数f(x)。</p><h3 id="Logistic-Regression-Error"><a href="#Logistic-Regression-Error" class="headerlink" title="Logistic Regression Error"></a>Logistic Regression Error</h3><p>现在我们将Logistic Regression与之前讲的Linear Classification、Linear Regression做个比较：</p><p><img src="http://img.blog.csdn.net/20170515235513276?" alt="这里写图片描述"></p><p>这三个线性模型都会用到线性scoring function $s=w^Tx$。linear classification的误差使用的是0/1 err；linear regression的误差使用的是squared err。那么logistic regression的误差该如何定义呢？</p><p>先介绍一下“似然性”的概念。目标函数$f(x)=P(+1|x)$，如果我们找到了hypothesis很接近target function。也就是说，在所有的Hypothesis集合中找到一个hypothesis与target function最接近，能产生同样的数据集D，包含y输出label，则称这个hypothesis是最大似然likelihood。</p><p><img src="http://img.blog.csdn.net/20170516001146598?" alt="这里写图片描述"></p><p>logistic function: $h(x)=\theta(w^Tx)$满足一个性质：$1-h(x)=h(-x)$。那么，似然性h:</p><p>$$likelihood(h)=P(x_1)h(+x_1)\times P(x_2)h(-x_2)\times \cdots P(x_N)h(-x_N)$$</p><p>因为$P(x_n)$对所有的h来说，都是一样的，所以我们可以忽略它。那么我们可以得到logistic h正比于所有的$h(y_nx)$乘积。我们的目标就是让乘积值最大化。</p><p><img src="http://img.blog.csdn.net/20170516002847044?" alt="这里写图片描述"></p><p>如果将w代入的话：</p><p><img src="http://img.blog.csdn.net/20170516002958701?" alt="这里写图片描述"></p><p>为了把连乘问题简化计算，我们可以引入ln操作，让连乘转化为连加：</p><p><img src="http://img.blog.csdn.net/20170516003257812?" alt="这里写图片描述"></p><p>接着，我们将maximize问题转化为minimize问题，添加一个负号就行，并引入平均数操作$\frac1N$：</p><p><img src="http://img.blog.csdn.net/20170516003559844?" alt="这里写图片描述"></p><p>将logistic function的表达式带入，那么minimize问题就会转化为如下形式：</p><p><img src="http://img.blog.csdn.net/20170516003823843?" alt="这里写图片描述"></p><p>至此，我们得到了logistic regression的err function，称之为cross-entropy error交叉熵误差：</p><p><img src="http://img.blog.csdn.net/20170516004024658?" alt="这里写图片描述"></p><h3 id="Gradient-of-Logistic-Regression-Error"><a href="#Gradient-of-Logistic-Regression-Error" class="headerlink" title="Gradient of Logistic Regression Error"></a>Gradient of Logistic Regression Error</h3><p>我们已经推导了$E_{in}$的表达式，那接下来的问题就是如何找到合适的向量w，让$E_{in}$最小。</p><p><img src="http://img.blog.csdn.net/20170516090812306?" alt="这里写图片描述"></p><p>Logistic Regression的$E_{in}$是连续、可微、二次可微的凸曲线（开口向上），根据之前Linear Regression的思路，我们只要计算$E_{in}$的梯度为零时的w，即为最优解。</p><p><img src="http://img.blog.csdn.net/20170516091637644?" alt="这里写图片描述"></p><p>对$E_{in}$计算梯度，学过微积分的都应该很容易计算出来：</p><p><img src="http://img.blog.csdn.net/20170516092312216?" alt="这里写图片描述"></p><p>最终得到的梯度表达式为：</p><p><img src="http://img.blog.csdn.net/20170516092503784?" alt="这里写图片描述"></p><p>为了计算$E_{in}$最小值，我们就要找到让$\nabla E_{in}(w)$等于0的位置。</p><p><img src="http://img.blog.csdn.net/20170516093504716?" alt="这里写图片描述"></p><p>上式可以看成$\theta(-y_nw^Tx_n)$是$-y_nx_n$的线性加权。要求$\theta(-y_nw^Tx_n)$与$-y_nx_n$的线性加权和为0，那么一种情况是线性可分，如果所有的权重$\theta(-y_nw^Tx_n)$为0，那就能保证$\nabla E_{in}(w)$为0。$\theta(-y_nw^Tx_n)$是sigmoid function，根据其特性，只要让$-y_nw^Tx_n≪0 $，即$y_nw^Tx_n≫0 $。$y_nw^Tx_n≫0 $表示对于所有的点，$y_n$与$w^Tx_n$都是同号的，这表示数据集D必须是全部线性可分的才能成立。</p><p>然而，保证所有的权重$\theta(-y_nw^Tx_n)$为0是不太现实的，总有不等于0的时候，那么另一种常见的情况是非线性可分，只能通过使加权和为零，来求解w。这种情况没有closed-form解，与Linear Regression不同，只能用迭代方法求解。</p><p><img src="http://img.blog.csdn.net/20170516100435914?" alt="这里写图片描述"></p><p>之前所说的Linear Regression有closed-form解，可以说是“一步登天”的；但是PLA算法是一步一步修正迭代进行的，每次对错误点进行修正，不断更新w值。PLA的迭代优化过程表示如下：</p><p><img src="http://img.blog.csdn.net/20170516100842890?" alt="这里写图片描述"></p><p>w每次更新包含两个内容：一个是每次更新的方向$y_nx_n$，用$v$表示，另一个是每次更新的步长$\eta$。参数$(v,\eta)$和终止条件决定了我们的迭代优化算法。</p><p><img src="http://img.blog.csdn.net/20170516101506325?" alt="这里写图片描述"></p><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>根据上一小节PLA的思想，迭代优化让每次w都有更新：</p><p><img src="http://img.blog.csdn.net/20170516102731492?" alt="这里写图片描述"></p><p>我们把$E_{in}(w)$曲线看做是一个山谷的话，要求$E_{in}(w)$最小，即可比作下山的过程。整个下山过程由两个因素影响：一个是下山的单位方向$v$；另外一个是下山的步长$\eta$。</p><p><img src="http://img.blog.csdn.net/20170516103259068?" alt="这里写图片描述"></p><p>利用微分思想和线性近似，假设每次下山我们只前进一小步，即$\eta$很小，那么根据泰勒Taylor一阶展开，可以得到：<br>$$E_{in}(w_t+\eta v)\approx E_{in}(w_t)+\eta v^T\nabla E_{in}(w_t)$$</p><p>关于Taylor展开的介绍，可参考我另一篇博客：<br><a href="http://blog.csdn.net/red_stone1/article/details/70260070" target="_blank" rel="noopener">多元函数的泰勒(Taylor)展开式</a></p><p>迭代的目的是让$E_{in}$越来越小，即让$E_{in}(w_t+\eta v)&lt;E_{in}(w_t)$。$\eta$是标量，因为如果两个向量方向相反的话，那么他们的内积最小（为负），也就是说如果方向$v$与梯度$\nabla E_{in}(w_t)$反向的话，那么就能保证每次迭代$E_{in}(w_t+\eta v)&lt;E_{in}(w_t)$都成立。则，我们令下降方向$v$为：<br>$$v=-\frac{\nabla E_{in}(w_t)}{||\nabla E_{in}(w_t)||}$$</p><p>$v$是单位向量，$v$每次都是沿着梯度的反方向走，这种方法称为梯度下降（gradient descent）算法。那么每次迭代公式就可以写成：<br>$$w_{t+1}\leftarrow w_t-\eta\frac{\nabla E_{in}(w_t)}{||\nabla E_{in}(w_t)||}$$</p><p>下面讨论一下$\eta$的大小对迭代优化的影响：$\eta$如果太小的话，那么下降的速度就会很慢；$\eta$如果太大的话，那么之前利用Taylor展开的方法就不准了，造成下降很不稳定，甚至会上升。因此，$\eta$应该选择合适的值，一种方法是在梯度较小的时候，选择小的$\eta$，梯度较大的时候，选择大的$\eta$，即$\eta$正比于$||\nabla E_{in}(w_t)||$。这样保证了能够快速、稳定地得到最小值$E_{in}(w)$。</p><p><img src="http://img.blog.csdn.net/20170516111145698?" alt="这里写图片描述"></p><p>对学习速率$\eta$做个更修正，梯度下降算法的迭代公式可以写成：<br>$$w_{t+1}\leftarrow w_t-\eta’\nabla E_{in}(w_t)$$<br>其中：<br>$$\eta’=\frac{\eta}{||\nabla E_{in}(w_t)||}$$</p><p>总结一下基于梯度下降的Logistic Regression算法步骤如下：</p><ul><li><strong>初始化$w_0$</strong></li><li><strong>计算梯度$\nabla E_{in}(w_t)=\frac1N\sum_{n=1}^N\theta(-y_nw_t^Tx_n)(-y_nx_n)$</strong></li><li><strong>迭代跟新$w_{t+1}\leftarrow w_t-\eta\nabla E_{in}(w_t)$</strong></li><li><strong>满足$\nabla E_{in}(w_{t+1})\approx0$或者达到迭代次数，迭代结束</strong></li></ul><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>我们今天介绍了Logistic Regression。首先，从逻辑回归的问题出发，将$P(+1|x)$作为目标函数，将$\theta(w^Tx)$作为hypothesis。接着，我们定义了logistic regression的err function，称之为cross-entropy error交叉熵误差。然后，我们计算logistic regression error的梯度，最后，通过梯度下降算法，计算$\nabla E_{in}(w_t)\approx0$时对应的$w_t$值。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170515225915831?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记9 -- Linear Regression</title>
    <link href="https://redstonewill.github.io/2018/03/17/9/"/>
    <id>https://redstonewill.github.io/2018/03/17/9/</id>
    <published>2018-03-17T06:39:42.000Z</published>
    <updated>2018-03-17T06:43:17.774Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170511000626691?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要介绍了在有noise的情况下，VC Bound理论仍然是成立的。同时，介绍了不同的error measure方法。本节课介绍机器学习最常见的一种算法：Linear Regression.</p><h3 id="线性回归问题"><a href="#线性回归问题" class="headerlink" title="线性回归问题"></a>线性回归问题</h3><p>在之前的Linear Classification课程中，讲了信用卡发放的例子，利用机器学习来决定是否给用户发放信用卡。本节课仍然引入信用卡的例子，来解决给用户发放信用卡额度的问题，这就是一个线性回归（Linear Regression）问题。</p><p><img src="http://img.blog.csdn.net/20170511000626691?" alt="这里写图片描述"></p><p>令用户特征集为d维的$X$，加上常数项，维度为$d+1$，与权重$w$的线性组合即为Hypothesis,记为$h(x)$。线性回归的预测函数取值在整个实数空间，这跟线性分类不同。</p><p>$$h(x)=w^TX$$</p><p><img src="http://img.blog.csdn.net/20170511091939131?" alt="这里写图片描述"></p><p>根据上图，在一维或者多维空间里，线性回归的目标是找到一条直线（对应一维）、一个平面（对应二维）或者更高维的超平面，使样本集中的点更接近它，也就是残留误差Residuals最小化。</p><p>一般最常用的错误测量方式是基于最小二乘法，其目标是计算误差的最小平方和对应的权重w，即上节课介绍的squared error：</p><p><img src="http://img.blog.csdn.net/20170511092613937?" alt="这里写图片描述"></p><p>这里提一点，最小二乘法可以解决线性问题和非线性问题。线性最小二乘法的解是closed-form，即$X=(A^TA)^{-1}A^Ty$，而非线性最小二乘法没有closed-form，通常用迭代法求解。本节课的解就是closed-form的。关于最小二乘法的一些介绍，请参见我的另一篇博文：</p><p><a href="http://blog.csdn.net/red_stone1/article/details/70306403" target="_blank" rel="noopener">最小二乘法和梯度下降法的一些总结</a></p><h3 id="线性回归算法"><a href="#线性回归算法" class="headerlink" title="线性回归算法"></a>线性回归算法</h3><p>样本数据误差$E_{in}$是权重$w$的函数，因为$X$和$y$都是已知的。我们的目标就是找出合适的$w$，使$E_{in}$能够最小。那么如何计算呢？</p><p>首先，运用矩阵转换的思想，将$E_{in}$计算转换为矩阵的形式。</p><p><img src="http://img.blog.csdn.net/20170511093750121?" alt="这里写图片描述"></p><p>然后，对于此类线性回归问题，$E_{in}(w)$一般是个凸函数。凸函数的话，我们只要找到一阶导数等于零的位置，就找到了最优解。那么，我们将$E_{w}$对每个$w_i,i=0,1,\cdots,d$求偏导，偏导为零的$w_i$，即为最优化的权重值分布。</p><p><img src="http://img.blog.csdn.net/20170511094302883?" alt="这里写图片描述"></p><p>根据梯度的思想，对$E_{w}$进行矩阵话求偏导处理：</p><p><img src="http://img.blog.csdn.net/20170511094548562?" alt="这里写图片描述"></p><p>令偏导为零，最终可以计算出权重向量$w$为：</p><p><img src="http://img.blog.csdn.net/20170511094724296?" alt="这里写图片描述"></p><p>最终，我们推导得到了权重向量$w=(X^TX)^{-1}X^Ty$，这是上文提到的closed-form解。其中，$(X^TX)^{-1}X^T$又称为伪逆矩阵pseudo-inverse，记为$X^+$，维度是(d+1)xN。</p><p>但是，我们注意到，伪逆矩阵中有逆矩阵的计算，逆矩阵$(X^TX)^{-1}$是否一定存在？一般情况下，只要满足样本数量N远大于样本特征维度d+1，就能保证矩阵的逆是存在的，称之为非奇异矩阵。但是如果是奇异矩阵，不可逆怎么办呢？其实，大部分的计算逆矩阵的软件程序，都可以处理这个问题，也会计算出一个逆矩阵。所以，一般伪逆矩阵是可解的。</p><h3 id="泛化问题"><a href="#泛化问题" class="headerlink" title="泛化问题"></a>泛化问题</h3><p>现在，可能有这样一个疑问，就是这种求解权重向量的方法是机器学习吗？或者说这种方法满足我们之前推导VC Bound，即是否泛化能力强$E_{in}\approx E_{out}$？</p><p><img src="http://img.blog.csdn.net/20170511101558353?" alt="这里写图片描述"></p><p>有两种观点：1、这不属于机器学习范畴。因为这种closed-form解的形式跟一般的机器学习算法不一样，而且在计算最小化误差的过程中没有用到迭代。2、这属于机器学习范畴。因为从结果上看，$E_{in}$和$E_{out}$都实现了最小化，而且实际上在计算逆矩阵的过程中，也用到了迭代。</p><p>其实，只从结果来看，这种方法的确实现了机器学习的目的。下面通过介绍一种更简单的方法，证明linear regression问题是可以通过线下最小二乘法方法计算得到好的$E_{in}$和$E_{out}$的。</p><p><img src="http://img.blog.csdn.net/20170511103154804?" alt="这里写图片描述"></p><p>首先，我们根据平均误差的思想，把$E_{in}(w_{LIN})$写成如图的形式，经过变换得到:<br>$$E_{in}(w_{LIN})=\frac1N||(I-XX^+)y||^2=\frac1N||(I-H)y||^2$$</p><p>我们称$XX^+$为帽子矩阵，用H表示。</p><p>下面从几何图形的角度来介绍帽子矩阵H的物理意义。</p><p><img src="http://img.blog.csdn.net/20170511103912793?" alt="这里写图片描述"></p><p>图中，y是N维空间的一个向量，粉色区域表示输入矩阵X乘以不同权值向量w所构成的空间，根据所有w的取值，预测输出都被限定在粉色的空间中。向量$\hat y$就是粉色空间中的一个向量，代表预测的一种。y是实际样本数据输出值。</p><p>机器学习的目的是在粉色空间中找到一个$\hat y$，使它最接近真实的y，那么我们只要将y在粉色空间上作垂直投影即可，投影得到的$\hat y$即为在粉色空间内最接近y的向量。这样即使平均误差$\overline E$最小。</p><p>从图中可以看出，$\hat y$是y的投影，已知$\hat y=Hy$，那么H表示的就是将y投影到$\hat y$的一种操作。图中绿色的箭头$y-\hat y$是向量y与$\hat y$相减，$y-\hat y$垂直于粉色区域。已知$(I-H)y=y-\hat y$那么I-H表示的就是将y投影到$y-\hat y$即垂直于粉色区域的一种操作。这样的话，我们就赋予了H和I-H不同但又有联系的物理意义。</p><p>这里trace(I-H)称为I-H的迹，值为N-(d+1)。这条性质很重要，一个矩阵的 trace等于该矩阵的所有特征值(Eigenvalues)之和。下面给出简单证明：</p><p>$trace(I-H)=trace(I)-trace(H)$<br>$=N-trace(XX^+)=N-trace(X(X^TX)^{-1}X^T$<br>$=N-trace(X^TX(X^TX)^{-1})=N-trace(I_{d+1})$<br>$=N-(d+1)$</p><p>介绍下该I-H这种转换的物理意义：原来有一个有N个自由度的向量y，投影到一个有d+1维的空间x（代表一列的自由度，即单一输入样本的参数，如图中粉色区域），而余数剩余的自由度最大只有N-(d+1)种。</p><p>在存在noise的情况下，上图变为：</p><p><img src="http://img.blog.csdn.net/20170511110854010?" alt="这里写图片描述"></p><p>图中，粉色空间的红色箭头是目标函数f(x)，虚线箭头是noise，可见，真实样本输出y由f(x)和noise相加得到。由上面推导，已知向量y经过I-H转换为$y-\hat y$，而noise与y是线性变换关系，那么根据线性函数知识，我们推导出noise经过I-H也能转换为$y-\hat y$。则对于样本平均误差，有下列推导成立：</p><p>$$E_{in}(w_{LIN})=\frac1N||y-\hat y||^2=\frac1N||(I-H)noise||^2=\frac1N(N-(d+1))||noise||^2$$</p><p>即</p><p>$$\overline E_{in}=noise level\ast (1-\frac{d+1}N) $$</p><p>同样，对$E_{out}$有如下结论：</p><p>$$\overline E_{out}=noise level\ast (1+\frac{d+1}N) $$</p><p>这个证明有点复杂，但是我们可以这样理解：$\overline E_{in}$与$\overline E_{out}$形式上只差了$\frac{(d+1)}N$项，从哲学上来说，$\overline E_{in}$是我们看得到的样本的平均误差，如果有noise，我们把预测往noise那边偏一点，让$\overline E_{in}$好看一点点，所以减去$\frac{(d+1)}N$项。那么同时，新的样本$\overline E_{out}$是我们看不到的，如果noise在反方向，那么$\overline E_{out}$就应该加上$\frac{(d+1)}N$项。</p><p>我们把$\overline E_{in}$与$\overline E_{out}$画出来，得到学习曲线：</p><p><img src="http://img.blog.csdn.net/20170511133854709?" alt="这里写图片描述"></p><p>当N足够大时，$\overline E_{in}$与$\overline E_{out}$逐渐接近，满足$\overline E_{in}\approx \overline E_{out}$，且数值保持在noise level。这就类似VC理论，证明了当N足够大的时候，这种线性最小二乘法是可以进行机器学习的，算法有效！</p><h3 id="Linear-Regression方法解决Linear-Classification问题"><a href="#Linear-Regression方法解决Linear-Classification问题" class="headerlink" title="Linear Regression方法解决Linear Classification问题"></a>Linear Regression方法解决Linear Classification问题</h3><p>之前介绍的Linear Classification问题使用的Error Measure方法用的是0/1 error，那么Linear Regression的squared error是否能够应用到Linear Classification问题？</p><p><img src="http://img.blog.csdn.net/20170511134801850?" alt="这里写图片描述"></p><p>下图展示了两种错误的关系，一般情况下，squared error曲线在0/1 error曲线之上。即$err_{0/1}\leq err_{sqr}$.</p><p><img src="http://img.blog.csdn.net/20170511135243106?" alt="这里写图片描述"></p><p>根据之前的VC理论，$E_{out}$的上界满足：</p><p><img src="http://img.blog.csdn.net/20170511135656953?" alt="这里写图片描述"></p><p>从图中可以看出，用$err_{sqr}$代替$err_{0/1}$，$E_{out}$仍然有上界，只不过是上界变得宽松了。也就是说用线性回归方法仍然可以解决线性分类问题，效果不会太差。二元分类问题得到了一个更宽松的上界，但是也是一种更有效率的求解方式。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本节课，我们主要介绍了Linear Regression。首先，我们从问题出发，想要找到一条直线拟合实际数据值；然后，我们利用最小二乘法，用解析形式推导了权重w的closed-form解；接着，用图形的形式得到$E_{out}-E_{in}\approx \frac{2(N+1)}{N}$，证明了linear regression是可以进行机器学习的，；最后，我们证明linear regressin这种方法可以用在binary classification上，虽然上界变宽松了，但是仍然能得到不错的学习方法。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170511000626691?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记8 -- Noise and Error</title>
    <link href="https://redstonewill.github.io/2018/03/17/8/"/>
    <id>https://redstonewill.github.io/2018/03/17/8/</id>
    <published>2018-03-17T06:37:13.000Z</published>
    <updated>2018-03-17T06:39:03.797Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170509214259658?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们主要介绍了VC Dimension的概念。如果Hypotheses set的VC Dimension是有限的，且有足够多N的资料，同时能够找到一个hypothesis使它的$E_{in}\approx 0$，那么就能说明机器学习是可行的。本节课主要讲了数据集有Noise的情况下，是否能够进行机器学习，并且介绍了假设空间H下演算法A的Error估计。</p><h3 id="Noise-and-Probablistic-target"><a href="#Noise-and-Probablistic-target" class="headerlink" title="Noise and Probablistic target"></a>Noise and Probablistic target</h3><p>上节课推导VC Dimension的数据集是在没有Noise的情况下，本节课讨论如果数据集本身存在Noise，那VC Dimension的推导是否还成立呢？</p><p>首先，Data Sets的Noise一般有三种情况：</p><ul><li><p><strong>由于人为因素，正类被误分为负类，或者负类被误分为正类；</strong></p></li><li><p><strong>同样特征的样本被模型分为不同的类；</strong></p></li><li><p><strong>样本的特征被错误记录和使用。</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170509214259658?" alt="这里写图片描述"></p><p>之前的数据集是确定的，即没有Noise的，我们称之为Deterministic。现在有Noise了，也就是说在某点处不再是确定分布，而是概率分布了，即对每个(x，y)出现的概率是$P(y|x)$。</p><p>因为Noise的存在，比如在x点，有0.7的概率y=1，有0.3的概率y=0，即y是按照$P(y|x)$分布的。数学上可以证明如果数据集按照$P(y|x)$概率分布且是iid的，那么以前证明机器可以学习的方法依然奏效，VC Dimension有限即可推断$E{in}$和$E{out}$是近似的。</p><p><img src="http://img.blog.csdn.net/20170509234714781?" alt="这里写图片描述"></p><p>$P(y|x)$称之为目标分布（Target Distribution）。它实际上告诉我们最好的选择是什么，同时伴随着多少noise。其实，没有noise的数据仍然可以看成“特殊”的$P(y|x)$概率分布，即概率仅是1和0.对于以前确定的数据集：<br>$$P(y|x)=1,for \space y=f(x)$$<br>$$P(y|x)=0,for \space y\neq f(x)$$</p><p><img src="http://img.blog.csdn.net/20170509234729403?" alt="这里写图片描述"></p><p>在引入noise的情况下，新的学习流程图如下所示：</p><p><img src="http://img.blog.csdn.net/20170616080950726?" alt="这里写图片描述"></p><h3 id="ERROR-Measure"><a href="#ERROR-Measure" class="headerlink" title="ERROR Measure"></a>ERROR Measure</h3><p>机器学习需要考虑的问题是找出的矩g与目标函数f有多相近，我们一直使用$E_{out}$进行误差的估计，那一般的错误测量有哪些形式呢？</p><p>我们介绍的矩g对错误的衡量有三个特性：</p><ul><li><p><strong>out-of-sample：样本外的未知数据</strong></p></li><li><p><strong>pointwise：对每个数据点x进行测试</strong></p></li><li><p><strong>classification：看prediction与target是否一致，classification error通常称为0/1 error</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170509235041811?" alt="这里写图片描述"></p><p>PointWise error实际上就是对数据集的每个点计算错误并计算平均，$E{in}$和$E{out}$的pointwise error的表达式为：</p><p><img src="http://img.blog.csdn.net/20170616082857078?" alt="这里写图片描述"></p><p>pointwise error是机器学习中最常用也是最简单的一种错误衡量方式，未来课程中，我们主要考虑这种方式。pointwise error一般可以分成两类：0/1 error和squared error。0/1 error通常用在分类（classification）问题上，而squared error通常用在回归（regression）问题上。</p><p><img src="http://img.blog.csdn.net/20170616083449361?" alt="这里写图片描述"></p><p>Ideal Mini-Target由$P(y|x)$和err共同决定，0/1 error和squared error的Ideal Mini-Target计算方法不一样。例如下面这个例子，分别用0/1 error和squared error来估计最理想的mini-target是多少。0/1 error中的mini-target是取P(y|x)最大的那个类，而squared error中的mini-target是取所有类的加权平方和。</p><p><img src="http://img.blog.csdn.net/20170509235617200?" alt="这里写图片描述"></p><p>有了错误衡量，就会知道当前的矩g是好还是不好，并会让演算法不断修正，得到更好的矩g，从而使得g与目标函数更接近。所以，引入error measure后，学习流程图如下所示：</p><p><img src="http://img.blog.csdn.net/20170616084802840?" alt="这里写图片描述"></p><h3 id="Algorithmic-Error-Measure"><a href="#Algorithmic-Error-Measure" class="headerlink" title="Algorithmic Error Measure"></a>Algorithmic Error Measure</h3><p>Error有两种：false accept和false reject。false accept意思是误把负类当成正类，false reject是误把正类当成负类。 根据不同的机器学习问题，false accept和false reject应该有不同的权重，这根实际情况是符合的，比如是超市优惠，那么false reject应该设的大一些；如果是安保系统，那么false accept应该设的大一些。</p><p><img src="http://img.blog.csdn.net/20170510000042846?" alt="这里写图片描述"></p><p>机器学习演算法A的cost function error估计有多种方法，真实的err一般难以计算，常用的方法可以采用plausible或者friendly，根据具体情况而定。</p><p><img src="http://img.blog.csdn.net/20170510000434925?" alt="这里写图片描述"></p><p>引入algorithm error measure之后，学习流程图如下：</p><p><img src="http://img.blog.csdn.net/20170616093909660?" alt="这里写图片描述"></p><h3 id="Weighted-Classification"><a href="#Weighted-Classification" class="headerlink" title="Weighted Classification"></a>Weighted Classification</h3><p>实际上，机器学习的Cost Function即来自于这些error，也就是算法里面的迭代的目标函数，通过优化使得Error（Ein）不断变小。<br>cost function中，false accept和false reject赋予不同的权重，在演算法中体现。对不同权重的错误惩罚，可以选用virtual copying的方法。</p><p><img src="http://img.blog.csdn.net/20170510001149489?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170510001245521?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要讲了在有Noise的情况下，即数据集按照$P(y|x)$概率分布，那么VC Dimension仍然成立，机器学习算法推导仍然有效。机器学习cost function常用的Error有0/1 error和squared error两类。实际问题中，对false accept和false reject应该选择不同的权重。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170509214259658?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记7 -- The VC Dimension</title>
    <link href="https://redstonewill.github.io/2018/03/17/7/"/>
    <id>https://redstonewill.github.io/2018/03/17/7/</id>
    <published>2018-03-17T06:33:34.000Z</published>
    <updated>2018-03-17T06:36:12.540Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170505103811400?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>前几节课着重介绍了机器能够学习的条件并做了详细的推导和解释。机器能够学习必须满足两个条件：</p><ul><li><strong>假设空间H的Size M是有限的，即当N足够大的时候，那么对于假设空间中任意一个假设g，$E_{out}\approx E_{in}$</strong>。</li><li><strong>利用算法A从假设空间H中，挑选一个g，使$E_{in}(g)\approx0$，则$E_{out}\approx0$</strong>。</li></ul><p>这两个条件，正好对应着test和trian两个过程。train的目的是使损失期望$E_{in}(g)\approx0$；test的目的是使将算法用到新的样本时的损失期望也尽可能小，即$E_{out}\approx0$。</p><p>正因为如此，上次课引入了break point，并推导出只要break point存在，则M有上界，一定存在$E_{out}\approx E_{in}$。</p><p>本次笔记主要介绍VC Dimension的概念。同时也是总结VC Dimension与$E_{in}(g)\approx0$，$E_{out}\approx0$，Model Complexity Penalty（下面会讲到）的关系。</p><h3 id="Definition-of-VC-Dimension"><a href="#Definition-of-VC-Dimension" class="headerlink" title="Definition of VC Dimension"></a>Definition of VC Dimension</h3><p>首先，我们知道如果一个假设空间H有break point k，那么它的成长函数是有界的，它的上界称为Bound function。根据数学归纳法，Bound function也是有界的，且上界为$N^{k-1}$。从下面的表格可以看出，$N(k-1)$比B(N,k)松弛很多。</p><p><img src="http://img.blog.csdn.net/20170505103811400?" alt="这里写图片描述"></p><p>则根据上一节课的推导，VC bound就可以转换为：</p><p><img src="http://img.blog.csdn.net/20170505104038982?" alt="这里写图片描述"></p><p>这样，不等式只与k和N相关了，一般情况下样本N足够大，所以我们只考虑k值。有如下结论：</p><ul><li><p><strong>若假设空间H有break point k，且N足够大，则根据VC bound理论，算法有良好的泛化能力</strong></p></li><li><p><strong>在假设空间中选择一个矩g，使$E_{in}\approx0$，则其在全集数据中的错误率会较低</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170615075948147?" alt="这里写图片描述"></p><p>下面介绍一个新的名词：VC Dimension。VC Dimension就是某假设集H能够shatter的最多inputs的个数，即最大完全正确的分类能力。（注意，只要存在一种分布的inputs能够正确分类也满足）。</p><p>shatter的英文意思是“粉碎”，也就是说对于inputs的所有情况都能列举出来。例如对N个输入，如果能够将$2^N$种情况都列出来，则称该N个输入能够被假设集H shatter。</p><p>根据之前break point的定义：假设集不能被shatter任何分布类型的inputs的最少个数。则VC Dimension等于break point的个数减一。</p><p><img src="http://img.blog.csdn.net/20170505110608511?" alt="这里写图片描述"></p><p>现在，我们回顾一下之前介绍的四种例子，它们对应的VC Dimension是多少：</p><p><img src="http://img.blog.csdn.net/20170615081035163?" alt="这里写图片描述"></p><p>用$d_{vc}$代替k，那么VC bound的问题也就转换为与$d_{vc}$和N相关了。同时，如果一个假设集H的$d_{vc}$确定了，则就能满足机器能够学习的第一个条件$E_{out}\approx E_{in}$，与算法、样本数据分布和目标函数都没有关系。</p><p><img src="http://img.blog.csdn.net/20170505111118560?" alt="这里写图片描述"></p><h3 id="VC-Dimension-of-Perceptrons"><a href="#VC-Dimension-of-Perceptrons" class="headerlink" title="VC Dimension of Perceptrons"></a>VC Dimension of Perceptrons</h3><p>回顾一下我们之前介绍的2D下的PLA算法，已知Perceptrons的k=4，即$d_{vc}=3$。根据VC Bound理论，当N足够大的时候，$E_{out}(g)\approx E_{in}(g)$。如果找到一个g，使$E_{in}(g)\approx 0$，那么就能证明PLA是可以学习的。</p><p><img src="http://img.blog.csdn.net/20170615081955144?" alt="这里写图片描述"></p><p>这是在2D情况下，那如果是多维的Perceptron，它对应的$d_{vc}$又等于多少呢？</p><p>已知在1D Perceptron，$d_{vc}=2$，在2D Perceptrons，$d_{vc}=3$，那么我们有如下假设：$d_{vc}=d+1$，其中d为维数。</p><p>要证明的话，只需分两步证明：</p><ul><li>$d_{vc}\geq d+1$</li><li>$d_{vc}\leq d+1$</li></ul><p><img src="http://img.blog.csdn.net/20170615082410013?" alt="这里写图片描述"></p><p>首先证明第一个不等式：$d_{vc}\geq d+1$。</p><p>在d维里，我们只要找到某一类的d+1个inputs可以被shatter的话，那么必然得到$d_{vc}\geq d+1$。所以，我们有意构造一个d维的矩阵$X$能够被shatter就行。$X$是d维的，有d+1个inputs，每个inputs加上第零个维度的常数项1，得到$X$的矩阵：</p><p><img src="http://img.blog.csdn.net/20170615084743432?" alt="这里写图片描述"></p><p>矩阵中，每一行代表一个inputs，每个inputs是d+1维的，共有d+1个inputs。这里构造的$X$很明显是可逆的。shatter的本质是假设空间H对$X$的所有情况的判断都是对的，即总能找到权重W，满足$X\ast W=y$，$W=X^{-1}\ast y$。由于这里我们构造的矩阵$X$的逆矩阵存在，那么d维的所有inputs都能被shatter，也就证明了第一个不等式。</p><p><img src="http://img.blog.csdn.net/20170615085447521?" alt="这里写图片描述"></p><p>然后证明第二个不等式：$d_{vc}\leq d+1$。</p><p>在d维里，如果对于任何的d+2个inputs，一定不能被shatter，则不等式成立。我们构造一个任意的矩阵$X$，其包含d+2个inputs，该矩阵有d+1列，d+2行。这d+2个向量的某一列一定可以被另外d+1个向量线性表示，例如对于向量$X_{d+2}$，可表示为：<br>$$X_{d+2}=a_1\ast X_1+a_2\ast X_2+\cdots+a_{d+1}\ast X_{d+1}$$</p><p>其中，假设$a_1&gt;0$，$a_2,\cdots,a_{d+1}&lt;0$.</p><p>那么如果$X_1$是正类，$X_2,\cdots,X_{d+1}$均为负类，则存在$W$，得到如下表达式：<br>$X_{d+2}\ast W=$<font color="#0000ff">$a_1\ast X_1\ast W$</font>+<font color="#ff0000">$a_2\ast X_2\ast W$</font>+$\cdots$+<font color="#ff0000">$a_{d+1}\ast X_{d+1}\ast W$</font>$&gt;0$</p><p>因为其中蓝色项大于0，代表正类；红色项小于0，代表负类。所有对于这种情况，$X_d+2$一定是正类，无法得到负类的情况。也就是说，d+2个inputs无法被shatter。证明完毕！</p><p><img src="http://img.blog.csdn.net/20170505135705345?" alt="这里写图片描述"></p><p>综上证明可得$d_{vc}=d+1$。</p><h3 id="Physical-Intuition-VC-Dimension"><a href="#Physical-Intuition-VC-Dimension" class="headerlink" title="Physical Intuition VC Dimension"></a>Physical Intuition VC Dimension</h3><p><img src="http://img.blog.csdn.net/20170505140028066?" alt="这里写图片描述"></p><p>上节公式中$W$又名features，即自由度。自由度是可以任意调节的，如同上图中的旋钮一样，可以调节。VC Dimension代表了假设空间的分类能力，即反映了H的自由度，产生dichotomy的数量，也就等于features的个数，但也不是绝对的。</p><p><img src="http://img.blog.csdn.net/20170505140713568?" alt="这里写图片描述"></p><p>例如，对2D Perceptrons，线性分类，$d_{vc}=3$，则$W={w_0,w_1,w_2}$，也就是说只要3个features就可以进行学习，自由度为3。</p><p>介绍到这，我们发现M与$d_{vc}$是成正比的，从而得到如下结论：</p><p><img src="http://img.blog.csdn.net/20170505141450682?" alt="这里写图片描述"></p><h3 id="Interpreting-VC-Dimension"><a href="#Interpreting-VC-Dimension" class="headerlink" title="Interpreting VC Dimension"></a>Interpreting VC Dimension</h3><p>下面，我们将更深入地探讨VC Dimension的意义。首先，把VC Bound重新写到这里：</p><p><img src="http://img.blog.csdn.net/20170505141928121?" alt="这里写图片描述"></p><p>根据之前的泛化不等式，如果$|E_{in}-E_{out}|&gt;\epsilon$，即出现bad坏的情况的概率最大不超过$\delta$。那么反过来，对于good好的情况发生的概率最小为$1-\delta$，则对上述不等式进行重新推导：</p><p><img src="http://img.blog.csdn.net/20170505142454586?" alt="这里写图片描述"></p><p>$\epsilon$表现了假设空间H的泛化能力，$\epsilon$越小，泛化能力越大。</p><p><img src="http://img.blog.csdn.net/20170505142745183?" alt="这里写图片描述"></p><p>至此，已经推导出泛化误差$E_{out}$的边界，因为我们更关心其上界（$E_{out}$可能的最大值），即：</p><p><img src="http://img.blog.csdn.net/20170505143029968?" alt="这里写图片描述"></p><p>上述不等式的右边第二项称为模型复杂度，其模型复杂度与样本数量N、假设空间H($d_{vc}$)、$\epsilon$有关。$E_{out}$由$E_{in}$共同决定。下面绘出$E_{out}$、model complexity、$E_{in}$随$d_{vc}$变化的关系：</p><p><img src="http://img.blog.csdn.net/20170505143707333?" alt="这里写图片描述"></p><p>通过该图可以得出如下结论：</p><ul><li><p><strong>$d_{vc}$越大，$E_{in}$越小，$\Omega$越大（复杂）</strong>。</p></li><li><p><strong>$d_{vc}$越小，$E_{in}$越大，$\Omega$越小（简单）</strong>。</p></li><li><p><strong>随着$d_{vc}$增大，$E_{out}$会先减小再增大</strong>。</p></li></ul><p>所以，为了得到最小的$E_{out}$，不能一味地增大$d_{vc}$以减小$E_{in}$，因为$E_{in}$太小的时候，模型复杂度会增加，造成$E_{out}$变大。也就是说，选择合适的$d_{vc}$，选择的features个数要合适。</p><p>下面介绍一个概念：样本复杂度（Sample Complexity）。如果选定$d_{vc}$，样本数据D选择多少合适呢？通过下面一个例子可以帮助我们理解：</p><p><img src="http://img.blog.csdn.net/20170505145501721?" alt="这里写图片描述"></p><p>通过计算得到N=29300，刚好满足$\delta=0.1$的条件。N大约是$d_{vc}$的10000倍。这个数值太大了，实际中往往不需要这么多的样本数量，大概只需要$d_{vc}$的10倍就够了。N的理论值之所以这么大是因为VC Bound 过于宽松了，我们得到的是一个比实际大得多的上界。</p><p><img src="http://img.blog.csdn.net/20170505145842865?" alt="这里写图片描述"></p><p>值得一提的是，VC Bound是比较宽松的，而如何收紧它却不是那么容易，这也是机器学习的一大难题。但是，令人欣慰的一点是，VC Bound基本上对所有模型的宽松程度是基本一致的，所以，不同模型之间还是可以横向比较。从而，VC Bound宽松对机器学习的可行性还是没有太大影响。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了VC Dimension的概念就是最大的non-break point。然后，我们得到了Perceptrons在d维度下的VC Dimension是d+1。接着，我们在物理意义上，将$d_{vc}$与自由度联系起来。最终得出结论$d_{vc}$不能过大也不能过小。选取合适的值，才能让$E_{out}$足够小，使假设空间H具有良好的泛化能力。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170505103811400?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记6 -- Theory of Generalization</title>
    <link href="https://redstonewill.github.io/2018/03/17/6/"/>
    <id>https://redstonewill.github.io/2018/03/17/6/</id>
    <published>2018-03-17T06:24:41.000Z</published>
    <updated>2018-03-17T06:30:32.483Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170614075730000?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们主要探讨了当M的数值大小对机器学习的影响。如果M很大，那么就不能保证机器学习有很好的泛化能力，所以问题转换为验证M有限，即最好是按照多项式成长。然后通过引入了成长函数$m_H(N)$和dichotomy以及break point的概念，提出2D perceptrons的成长函数$m_H(N)$是多项式级别的猜想。这就是本节课将要深入探讨和证明的内容。</p><h3 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h3><p>我们先回顾一下上节课的内容，四种成长函数与break point的关系：</p><p><img src="http://img.blog.csdn.net/20170614075730000?" alt="这里写图片描述"></p><p>下面引入一个例子，如果k=2，那么当N取不同值的时候，计算其成长函数$m_H(N)$是多少。很明显，当N=1时，$m_H(N)$=2,；当N=2时，由break point为2可知，任意两点都不能被shattered（shatter的意思是对N个点，能够分解为$2^N$种dichotomies）；$m_H(N)$最大值只能是3；当N=3时，简单绘图分析可得其$m_H(N)=4$，即最多只有4种dichotomies。</p><p><img src="http://img.blog.csdn.net/20170614081452868?" alt="这里写图片描述"></p><p>所以，我们发现当N&gt;k时，break point限制了$m_H(N)$值的大小，也就是说影响成长函数$m_H(N)$的因素主要有两个：</p><ul><li><p>抽样数据集N</p></li><li><p>break point k（这个变量确定了假设的类型）</p></li></ul><p>那么，如果给定N和k，能够证明其$m_H(N)$的最大值的上界是多项式的，则根据霍夫丁不等式，就能用$m_H(N)$代替M，得到机器学习是可行的。所以，证明$m_H(N)$的上界是poly(N)，是我们的目标。</p><p><img src="http://img.blog.csdn.net/20170614082443877?" alt="这里写图片描述"></p><h3 id="Bounding-Function-Basic-Cases"><a href="#Bounding-Function-Basic-Cases" class="headerlink" title="Bounding Function: Basic Cases"></a>Bounding Function: Basic Cases</h3><p>现在，我们引入一个新的函数：bounding function，B(N,k)。Bound Function指的是当break point为k的时候，成长函数$m_H(N)$可能的最大值。也就是说B(N,k)是$m_H(N)$的上界，对应$m_H(N)$最多有多少种dichotomy。那么，我们新的目标就是证明：</p><p>$$B(N,k)\leq poly(N)$$</p><p>这里值得一提的是，B(N,k)的引入不考虑是1D postive intrervals问题还是2D perceptrons问题，而只关心成长函数的上界是多少，从而简化了问题的复杂度。</p><p><img src="http://img.blog.csdn.net/20170614083709962?" alt="这里写图片描述"></p><p>求解B(N,k)的过程十分巧妙：</p><ul><li><p>当k=1时，B(N,1)恒为1。</p></li><li><p>当N &lt; k时，根据break point的定义，很容易得到$B(N,k)=2^N$。</p></li><li><p>当N = k时，此时N是第一次出现不能被shatter的值，所以最多只能有$2^N-1$个dichotomies，则$B(N,k)=2^N-1$。</p></li></ul><p><img src="http://img.blog.csdn.net/20170614085020015?" alt="这里写图片描述"></p><p>到此，bounding function的表格已经填了一半了，对于最常见的N&gt;k的情况比较复杂，推导过程下一小节再详细介绍。</p><h3 id="Bounding-Function-Inductive-Cases"><a href="#Bounding-Function-Inductive-Cases" class="headerlink" title="Bounding Function: Inductive Cases"></a>Bounding Function: Inductive Cases</h3><p>N &gt; k的情况较为复杂，下面给出推导过程：</p><p>以B(4,3)为例，首先想着能否构建B(4,3)与B(3,x)之间的关系。</p><p>首先，把B(4,3)所有情况写下来，共有11组。也就是说再加一种dichotomy，任意三点都能被shattered，11是极限。</p><p><img src="http://img.blog.csdn.net/20170614091230231?" alt="这里写图片描述"></p><p>对这11种dichotomy分组，目前分成两组，分别是orange和purple，orange的特点是，x1,x2和x3是一致的，x4不同并成对，例如1和5，2和8等，purple则是单一的，x1,x2,x3都不同，如6,7,9三组。</p><p><img src="http://img.blog.csdn.net/20170614091821543?" alt="这里写图片描述"></p><p>将Orange去掉x4后去重得到4个不同的vector并成为$\alpha$，相应的purple为$\beta$。那么$B(4,3) = 2\alpha + \beta$，这个是直接转化。紧接着，由定义，B(4,3)是不能允许任意三点shatter的，所以由$\alpha$和$\beta$构成的所有三点组合也不能shatter（alpha经过去重），即$\alpha + \beta\leq B(3,3)$。</p><p><img src="http://img.blog.csdn.net/20170614094602964?" alt="这里写图片描述"></p><p>另一方面，由于$\alpha$中x4是成对存在的，且$\alpha$是不能被任意三点shatter的，则能推导出$\alpha$是不能被任意两点shatter的。这是因为，如果$\alpha$是不能被任意两点shatter，而x4又是成对存在的，那么x1、x2、x3、x4组成的$\alpha$必然能被三个点shatter。这就违背了条件的设定。这个地方的推导非常巧妙，也解释了为什么会这样分组。此处得到的结论是$\alpha \leq B(3,2)$</p><p><img src="http://img.blog.csdn.net/20170614094707216?" alt="这里写图片描述"></p><p>由此得出B(4,3)与B(3,x)的关系为：</p><p><img src="http://img.blog.csdn.net/20170614094834154?" alt="这里写图片描述"></p><p>最后，推导出一般公式为：</p><p><img src="http://img.blog.csdn.net/20170614094924233?" alt="这里写图片描述"></p><p>根据推导公式，下表给出B(N,K)值</p><p><img src="http://img.blog.csdn.net/20170614095031483?" alt="这里写图片描述"></p><p>根据递推公式，推导出B(N,K)满足下列不等式：</p><p><img src="http://img.blog.csdn.net/20170614095335958?" alt="这里写图片描述"></p><p>上述不等式的右边是最高阶为k-1的N多项式，也就是说成长函数$m_H(N)$的上界B(N,K)的上界满足多项式分布poly(N)，这就是我们想要得到的结果。</p><p>得到了$m_H(N)$的上界B(N,K)的上界满足多项式分布poly(N)后，我们回过头来看看之前介绍的几种类型它们的$m_H(N)$与break point的关系：</p><p><img src="http://img.blog.csdn.net/20170614100013092?" alt="这里写图片描述"></p><p>我们得到的结论是，对于2D perceptrons，break point为k=4，$m_H(N)$的上界是$N^{k-1}$。推广一下，也就是说，如果能找到一个模型的break point，且是有限大的，那么就能推断出其成长函数$m_H(N)$有界。</p><h3 id="A-Pictorial-Proof"><a href="#A-Pictorial-Proof" class="headerlink" title="A Pictorial Proof"></a>A Pictorial Proof</h3><p>我们已经知道了成长函数的上界是poly(N)的，下一步，如果能将$m_H(N)$代替M，代入到Hoffding不等式中，就能得到$E_{out}\approx E_{in}$的结论：</p><p><img src="http://img.blog.csdn.net/20170614101927607?" alt="这里写图片描述"></p><p>实际上并不是简单的替换就可以了，正确的表达式为：</p><p><img src="http://img.blog.csdn.net/20170614102141749?" alt="这里写图片描述"></p><p>该推导的证明比较复杂，我们可以简单概括为三个步骤来证明：</p><p><img src="http://img.blog.csdn.net/20170614103551436?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170614103603584?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170614103615889?" alt="这里写图片描述"></p><p>这部分内容，我也只能听个大概内容，对具体的证明过程有兴趣的童鞋可以自行研究一下，研究的结果记得告诉一下我哦。</p><p>最终，我们通过引入成长函数$m_H$，得到了一个新的不等式，称为Vapnik-Chervonenkis(VC) bound：</p><p><img src="http://img.blog.csdn.net/20170614105404608?" alt="这里写图片描述"></p><p>对于2D perceptrons，它的break point是4，那么成长函数$m_H(N)=O(N^3)$。所以，我们可以说2D perceptrons是可以进行机器学习的，只要找到hypothesis能让$E_{in}\approx0$，就能保证$E_{in}\approx E_{out}$。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课我们主要介绍了只要存在break point，那么其成长函数$m_H(N)$就满足poly(N)。推导过程是先引入$m_H(N)$的上界B(N,k)，B(N,k)的上界是N的k-1阶多项式，从而得到$m_H(N)$的上界就是N的k-1阶多项式。然后，我们通过简单的三步证明，将$m_H(N)$代入了Hoffding不等式中，推导出了Vapnik-Chervonenkis(VC) bound，最终证明了只要break point存在，那么机器学习就是可行的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170614075730000?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记5 -- Training versus Testing</title>
    <link href="https://redstonewill.github.io/2018/03/16/5/"/>
    <id>https://redstonewill.github.io/2018/03/16/5/</id>
    <published>2018-03-16T14:45:12.000Z</published>
    <updated>2018-03-17T06:31:32.547Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170613075450559?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要介绍了机器学习的可行性。首先，由NFL定理可知，机器学习貌似是不可行的。但是，随后引入了统计学知识，如果样本数据足够大，且hypothesis个数有限，那么机器学习一般就是可行的。本节课将讨论机器学习的核心问题，严格证明为什么机器可以学习。从上节课最后的问题出发，即当hypothesis的个数是无限多的时候，机器学习的可行性是否仍然成立？</p><h3 id="Recap-and-Preview"><a href="#Recap-and-Preview" class="headerlink" title="Recap and Preview"></a>Recap and Preview</h3><p>我们先来看一下基于统计学的机器学习流程图：</p><p><img src="http://img.blog.csdn.net/20170613075450559?" alt="这里写图片描述"></p><p>该流程图中，训练样本D和最终测试h的样本都是来自同一个数据分布，这是机器能够学习的前提。另外，训练样本D应该足够大，且hypothesis set的个数是有限的，这样根据霍夫丁不等式，才不会出现Bad Data，保证$E_{in}\approx E_{out}$，即有很好的泛化能力。同时，通过训练，得到使$E_{in}$最小的h，作为模型最终的矩g，g接近于目标函数。</p><p>这里，我们总结一下前四节课的主要内容：第一节课，我们介绍了机器学习的定义，目标是找出最好的矩g，使$g\approx f$，保证$E_{out}(g)\approx 0$；第二节课，我们介绍了如何让$E_{in}\approx 0$，可以使用PLA、pocket等演算法来实现；第三节课，我们介绍了机器学习的分类，我们的训练样本是批量数据（batch），处理监督式（supervised）二元分类（binary classification）问题；第四节课，我们介绍了机器学习的可行性，通过统计学知识，把$E_{in}(g)$与$E_{out}(g)$联系起来，证明了在一些条件假设下，$E_{in}(g)\approx E_{out}(g)$成立。</p><p><img src="http://img.blog.csdn.net/20170613081415084?" alt="这里写图片描述"></p><p>这四节课总结下来，我们把机器学习的主要目标分成两个核心的问题：</p><ul><li><p>$E_{in}(g)\approx E_{out}(g)$</p></li><li><p>$E_{in}(g)$足够小</p></li></ul><p>上节课介绍的机器学习可行的一个条件是hypothesis set的个数M是有限的，那M跟上面这两个核心问题有什么联系呢？</p><p>我们先来看一下，当M很小的时候，由上节课介绍的霍夫丁不等式，得到$E_{in}(g)\approx E_{out}(g)$，即能保证第一个核心问题成立。但M很小时，演算法A可以选择的hypothesis有限，不一定能找到使$E_{in}(g)$足够小的hypothesis，即不能保证第二个核心问题成立。当M很大的时候，同样由霍夫丁不等式，$E_{in}(g)$与$E_{out}(g)$的差距可能比较大，第一个核心问题可能不成立。而M很大，使的演算法A的可以选择的hypothesis就很多，很有可能找到一个hypothesis，使$E_{in}(g)$足够小，第二个核心问题可能成立。</p><p><img src="http://img.blog.csdn.net/20170613083237643?" alt="这里写图片描述"></p><p>从上面的分析来看，M的选择直接影响机器学习两个核心问题是否满足，M不能太大也不能太小。那么如果M无限大的时候，是否机器就不可以学习了呢？例如PLA算法中直线是无数条的，但是PLA能够很好地进行机器学习，这又是为什么呢？如果我们能将无限大的M限定在一个有限的$m_H$内，问题似乎就解决了。</p><h3 id="Effective-Number-of-Line"><a href="#Effective-Number-of-Line" class="headerlink" title="Effective Number of Line"></a>Effective Number of Line</h3><p>我们先看一下上节课推导的霍夫丁不等式：</p><p>$$P[|E_{in}(g)-E_{out}(g)|&gt;\epsilon]\leq 2\cdot M\cdot exp(-2\epsilon^2N)$$</p><p>其中，M表示hypothesis的个数。每个hypothesis下的BAD events $B_m$级联的形式满足下列不等式：</p><p>$$P[B_1\ or\ B_2\ or\ \cdots B_M]\leq P[B_1]+P[B_2]+\cdots+P[B_M]$$</p><p>当$M=\infty$时，上面不等式右边值将会很大，似乎说明BAD events很大，$E_{in}(g)$与$E_{out}(g)$也并不接近。但是BAD events $B_m$级联的形式实际上是扩大了上界，union bound过大。这种做法假设各个hypothesis之间没有交集，这是最坏的情况，可是实际上往往不是如此，很多情况下，都是有交集的，也就是说M实际上没那么大，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170613092500157?" alt="这里写图片描述"></p><p>也就是说union bound被估计过高了（over-estimating）。所以，我们的目的是找出不同BAD events之间的重叠部分，也就是将无数个hypothesis分成有限个类别。</p><p>如何将无数个hypothesis分成有限类呢？我们先来看这样一个例子，假如平面上用直线将点分开，也就跟PLA一样。如果平面上只有一个点x1，那么直线的种类有两种：一种将x1划为+1，一种将x1划为-1：</p><p><img src="http://img.blog.csdn.net/20170613095709542?" alt="这里写图片描述"></p><p>如果平面上有两个点x1、x2，那么直线的种类共4种：x1、x2都为+1，x1、x2都为-1，x1为+1且x2为-1，x1为-1且x2为+1：</p><p><img src="http://img.blog.csdn.net/20170613103010274?" alt="这里写图片描述"></p><p>如果平面上有三个点x1、x2、x3，那么直线的种类共8种：</p><p><img src="http://img.blog.csdn.net/20170613103032665?" alt="这里写图片描述"></p><p>但是，在三个点的情况下，也会出现不能用一条直线划分的情况：</p><p><img src="http://img.blog.csdn.net/20170613102051356?" alt="这里写图片描述"></p><p>也就是说，对于平面上三个点，不能保证所有的8个类别都能被一条直线划分。那如果是四个点x1、x2、x3、x4，我们发现，平面上找不到一条直线能将四个点组成的16个类别完全分开，最多只能分开其中的14类，即直线最多只有14种：</p><p><img src="http://img.blog.csdn.net/20170613102539221?" alt="这里写图片描述"></p><p>经过分析，我们得到平面上线的种类是有限的，1个点最多有2种线，2个点最多有4种线，3个点最多有8种线，4个点最多有14（$&lt;2^4$）种线等等。我们发现，有效直线的数量总是满足$\leq 2^N$，其中，N是点的个数。所以，如果我们可以用effective(N)代替M，霍夫丁不等式可以写成：</p><p>$$P[|E_{in}(g)-E_{out}(g)|&gt;\epsilon]\leq 2\cdot effective(N)\cdot exp(-2\epsilon^2N)$$</p><p>已知effective(N)&lt;$2^N$，如果能够保证effective(N)&lt;&lt;$2^N$，即不等式右边接近于零，那么即使M无限大，直线的种类也很有限，机器学习也是可能的。</p><p><img src="http://img.blog.csdn.net/20170613110232379?" alt="这里写图片描述"></p><h3 id="Effective-Number-of-Hypotheses"><a href="#Effective-Number-of-Hypotheses" class="headerlink" title="Effective Number of Hypotheses"></a>Effective Number of Hypotheses</h3><p>接下来先介绍一个新名词：二分类（dichotomy）。dichotomy就是将空间中的点（例如二维平面）用一条直线分成正类（蓝色o）和负类（红色x）。令H是将平面上的点用直线分开的所有hypothesis h的集合，dichotomy H与hypotheses H的关系是：hypotheses H是平面上所有直线的集合，个数可能是无限个，而dichotomy H是平面上能将点完全用直线分开的直线种类，它的上界是$2^N$。接下来，我们要做的就是尝试用dichotomy代替M。</p><p><img src="http://img.blog.csdn.net/20170613112843268?" alt="这里写图片描述"></p><p>再介绍一个新的名词：成长函数（growth function），记为$m_H(H)$。成长函数的定义是：对于由N个点组成的不同集合中，某集合对应的dichotomy最大，那么这个dichotomy值就是$m_H(H)$，它的上界是$2^N$：</p><p><img src="http://img.blog.csdn.net/20170613113650318?" alt="这里写图片描述"></p><p>成长函数其实就是我们之前讲的effective lines的数量最大值。根据成长函数的定义，二维平面上，$m_H(H)$随N的变化关系是：</p><p><img src="http://img.blog.csdn.net/20170613113930381?" alt="这里写图片描述"></p><p>接下来，我们讨论如何计算成长函数。先看一个简单情况，一维的Positive Rays：</p><p><img src="http://img.blog.csdn.net/20170613115111665?" alt="这里写图片描述"></p><p>若有N个点，则整个区域可分为N+1段，很容易得到其成长函数$m_H(N)=N+1$。注意当N很大时，$(N+1)&lt;&lt;2^N$，这是我们希望看到的。</p><p>另一种情况是一维的Positive Intervals：</p><p><img src="http://img.blog.csdn.net/20170613133644825?" alt="这里写图片描述"></p><p>它的成长函数可以由下面推导得出：</p><p><img src="http://img.blog.csdn.net/20170613134246055?" alt="这里写图片描述"></p><p>这种情况下，$m_H(N)=\frac12N^2+\frac12N+1&lt;&lt;2^N$，在N很大的时候，仍然是满足的。</p><p>再来看这个例子，假设在二维空间里，如果hypothesis是凸多边形或类圆构成的封闭曲线，如下图所示，左边是convex的，右边不是convex的。那么，它的成长函数是多少呢？</p><p><img src="http://img.blog.csdn.net/20170613135200251?" alt="这里写图片描述"></p><p>当数据集D按照如下的凸分布时，我们很容易计算得到它的成长函数$m_H=2^N$。这种情况下，N个点所有可能的分类情况都能够被hypotheses set覆盖，我们把这种情形称为shattered。也就是说，如果能够找到一个数据分布集，hypotheses set对N个输入所有的分类情况都做得到，那么它的成长函数就是$2^N$。</p><p><img src="http://img.blog.csdn.net/20170613135918235?" alt="这里写图片描述"></p><h3 id="Break-Point"><a href="#Break-Point" class="headerlink" title="Break Point"></a>Break Point</h3><p>上一小节，我们介绍了四种不同的成长函数，分别是：</p><p><img src="http://img.blog.csdn.net/20170613140725974?" alt="这里写图片描述"></p><p>其中，positive rays和positive intervals的成长函数都是polynomial的，如果用$m_H$代替M的话，这两种情况是比较好的。而convex sets的成长函数是exponential的，即等于M，并不能保证机器学习的可行性。那么，对于2D perceptrons，它的成长函数究竟是polynomial的还是exponential的呢？</p><p>对于2D perceptrons，我们之前分析了3个点，可以做出8种所有的dichotomy，而4个点，就无法做出所有16个点的dichotomy了。所以，我们就把4称为2D perceptrons的break point（5、6、7等都是break point）。令有k个点，如果k大于等于break point时，它的成长函数一定小于2的k次方。</p><p>根据break point的定义，我们知道满足$m_H(k)\neq 2^k$的k的最小值就是break point。对于我们之前介绍的四种成长函数，他们的break point分别是：</p><p><img src="http://img.blog.csdn.net/20170613143127195?" alt="这里写图片描述"></p><p>通过观察，我们猜测成长函数可能与break point存在某种关系：对于convex sets，没有break point，它的成长函数是2的N次方；对于positive rays，break point k=2，它的成长函数是O(N)；对于positive intervals，break point k=3，它的成长函数是$O(N^2)$。则根据这种推论，我们猜测2D perceptrons，它的成长函数$m_H(N)=O(N^{k-1})$ 。如果成立，那么就可以用$m_H$代替M，就满足了机器能够学习的条件。关于上述猜测的证明，我们下节课再详细介绍。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课，我们更深入地探讨了机器学习的可行性。我们把机器学习拆分为两个核心问题：$E_{in}(g)\approx E_{out}(g)$和$E_{in}(g)\approx 0$。对于第一个问题，我们探讨了M个hypothesis到底可以划分为多少种，也就是成长函数$m_H$。并引入了break point的概念，给出了break point的计算方法。下节课，我们将详细论证对于2D perceptrons，它的成长函数与break point是否存在多项式的关系，如果是这样，那么机器学习就是可行的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170613075450559?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记4 -- Feasibility of Learning</title>
    <link href="https://redstonewill.github.io/2018/03/16/4/"/>
    <id>https://redstonewill.github.io/2018/03/16/4/</id>
    <published>2018-03-16T14:30:42.000Z</published>
    <updated>2018-03-17T06:32:57.753Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170612080753910?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要介绍了根据不同的设定，机器学习可以分为不同的类型。其中，监督式学习中的二元分类和回归分析是最常见的也是最重要的机器学习问题。本节课，我们将介绍机器学习的可行性，讨论问题是否可以使用机器学习来解决。</p><h3 id="Learning-is-Impossible"><a href="#Learning-is-Impossible" class="headerlink" title="Learning is Impossible"></a>Learning is Impossible</h3><p>首先，考虑这样一个例子，如下图所示，有3个label为-1的九宫格和3个label为+1的九宫格。根据这6个样本，提取相应label下的特征，预测右边九宫格是属于-1还是+1？结果是，如果依据对称性，我们会把它归为+1；如果依据九宫格左上角是否是黑色，我们会把它归为-1。除此之外，还有根据其它不同特征进行分类，得到不同结果的情况。而且，这些分类结果貌似都是正确合理的，因为对于6个训练样本来说，我们选择的模型都有很好的分类效果。</p><p><img src="http://img.blog.csdn.net/20170612080753910?" alt="这里写图片描述"></p><p>再来看一个比较数学化的二分类例子，输入特征x是二进制的、三维的，对应有8种输入，其中训练样本D有5个。那么，根据训练样本对应的输出y，假设有8个hypothesis，这8个hypothesis在D上，对5个训练样本的分类效果效果都完全正确。但是在另外3个测试数据上，不同的hypothesis表现有好有坏。在已知数据D上，$g\approx f$；但是在D以外的未知数据上，$g\approx f$不一定成立。而机器学习目的，恰恰是希望我们选择的模型能在未知数据上的预测与真实结果是一致的，而不是在已知的数据集D上寻求最佳效果。</p><p><img src="http://img.blog.csdn.net/20170612083115583?" alt="这里写图片描述"></p><p>这个例子告诉我们，我们想要在D以外的数据中更接近目标函数似乎是做不到的，只能保证对D有很好的分类结果。机器学习的这种特性被称为没有免费午餐（No Free Lunch）定理。NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。从这个例子来看，NFL说明了无法保证一个机器学习算法在D以外的数据集上一定能分类或预测正确，除非加上一些假设条件，我们以后会介绍。</p><h3 id="Probability-to-the-Rescue"><a href="#Probability-to-the-Rescue" class="headerlink" title="Probability to the Rescue"></a>Probability to the Rescue</h3><p>从上一节得出的结论是：在训练集D以外的样本上，机器学习的模型是很难，似乎做不到正确预测或分类的。那是否有一些工具或者方法能够对未知的目标函数f做一些推论，让我们的机器学习模型能够变得有用呢？</p><p>如果有一个装有很多（数量很大数不过来）橙色球和绿色球的罐子，我们能不能推断橙色球的比例u？统计学上的做法是，从罐子中随机取出N个球，作为样本，计算这N个球中橙色球的比例v，那么就估计出罐子中橙色球的比例约为v。</p><p><img src="http://img.blog.csdn.net/20170612094631233?" alt="这里写图片描述"></p><p>这种随机抽取的做法能否说明罐子里橙色球的比例一定是v呢？答案是否定的。但是从概率的角度来说，样本中的v很有可能接近我们未知的u。下面从数学推导的角度来看v与u是否相近。</p><p>已知u是罐子里橙色球的比例，v是N个抽取的样本中橙色球的比例。当N足够大的时候，v接近于u。这就是Hoeffding’s inequality：</p><p>$$P[|v-u|&gt;\epsilon]\leq 2exp(-2\epsilon^2N)$$</p><p>Hoeffding不等式说明当N很大的时候，v与u相差不会很大，它们之间的差值被限定在$\epsilon$之内。我们把结论v=u称为probably approximately correct(PAC)。</p><p><img src="http://img.blog.csdn.net/20170612100845352?" alt="这里写图片描述"></p><h3 id="Connection-to-Learning"><a href="#Connection-to-Learning" class="headerlink" title="Connection to Learning"></a>Connection to Learning</h3><p>下面，我们将罐子的内容对应到机器学习的概念上来。机器学习中hypothesis与目标函数相等的可能性，类比于罐子中橙色球的概率问题；罐子里的一颗颗弹珠类比于机器学习样本空间的x；橙色的弹珠类比于h(x)与f不相等；绿色的弹珠类比于h(x)与f相等；从罐子中抽取的N个球类比于机器学习的训练样本D，且这两种抽样的样本与总体样本之间都是独立同分布的。所以呢，如果样本N够大，且是独立同分布的，那么，从样本中$h(x)\neq f(x)$的概率就能推导在抽样样本外的所有样本中$h(x)\neq f(x)$的概率是多少。</p><p><img src="http://img.blog.csdn.net/20170612105733424?" alt="这里写图片描述"></p><p>映射中最关键的点是讲抽样中橙球的概率理解为样本数据集D上h(x)错误的概率，以此推算出在所有数据上h(x)错误的概率，这也是机器学习能够工作的本质，即我们为啥在采样数据上得到了一个假设，就可以推到全局呢？因为两者的错误率是PAC的，只要我们保证前者小，后者也就小了。</p><p> <img src="http://img.blog.csdn.net/20170612110350990?" alt="这里写图片描述"></p><p>这里我们引入两个值$E_{in}(h)$和$E_{out}(h)$。$E_{in}(h)$表示在抽样样本中，h(x)与$y_n$不相等的概率；$E_{out}(h)$表示实际所有样本中，h(x)与f(x)不相等的概率是多少。</p><p><img src="http://img.blog.csdn.net/20170612110744008?" alt="这里写图片描述"></p><p>同样，它的Hoeffding’s inequality可以表示为：</p><p>$$P[|E_{in}(h)-E_{out}(h)|&gt;\epsilon]\leq 2exp(-2\epsilon^2N)$$</p><p>该不等式表明，$E_{in}(h)=E_{out}(h)$也是PAC的。如果$E_{in}(h)\approx E_{out}(h)$，$E_{in}(h)$很小，那么就能推断出$E_{out}(h)$很小，也就是说在该数据分布P下，h与f非常接近，机器学习的模型比较准确。</p><p>一般地，h如果是固定的，N很大的时候，$E_{in}(h)\approx E_{out}(h)$，但是并不意味着$g\approx f$。因为h是固定的，不能保证$E_{in}(h)$足够小，即使$E_{in}(h)\approx E_{out}(h)$，也可能使$E_{out}(h)$偏大。所以，一般会通过演算法A，选择最好的h，使$E_{in}(h)$足够小，从而保证$E_{out}(h)$很小。固定的h，使用新数据进行测试，验证其错误率是多少。</p><p><img src="http://img.blog.csdn.net/20170612112902807?" alt="这里写图片描述"></p><h3 id="Connection-to-Real-Learning"><a href="#Connection-to-Real-Learning" class="headerlink" title="Connection to Real Learning"></a>Connection to Real Learning</h3><p><img src="http://img.blog.csdn.net/20170612135223361?" alt="这里写图片描述"></p><p>假设现在有很多罐子M个（即有M个hypothesis），如果其中某个罐子抽样的球全是绿色，那是不是应该选择这个罐子呢？我们先来看这样一个例子：150个人抛硬币，那么其中至少有一个人连续5次硬币都是正面朝上的概率是</p><p>$$1-(\frac{31}{32})^{150}&gt;99\%$$</p><p>可见这个概率是很大的，但是能否说明5次正面朝上的这个硬币具有代表性呢？答案是否定的！并不能说明该硬币单次正面朝上的概率很大，其实都是0.5。一样的道理，抽到全是绿色求的时候也不能一定说明那个罐子就全是绿色球。当罐子数目很多或者抛硬币的人数很多的时候，可能引发Bad Sample，Bad Sample就是$E_{in}$和$E_{out}$差别很大，即选择过多带来的负面影响，选择过多会恶化不好的情形。</p><p>根据许多次抽样的到的不同的数据集D，Hoeffding’s inequality保证了大多数的D都是比较好的情形（即对于某个h，保证$E_{in}\approx E_{out}$），但是也有可能出现Bad Data，即$E_{in}$和$E_{out}$差别很大的数据集D，这是小概率事件。</p><p><img src="http://img.blog.csdn.net/20170612140418003?" alt="这里写图片描述"></p><p>也就是说，不同的数据集$D_n$，对于不同的hypothesis，有可能成为Bad Data。只要$D_n$在某个hypothesis上是Bad Data，那么$D_n$就是Bad Data。只有当$D_n$在所有的hypothesis上都是好的数据，才说明$D_n$不是Bad Data，可以自由选择演算法A进行建模。那么，根据Hoeffding’s inequality，Bad Data的上界可以表示为连级（union bound）的形式：</p><p><img src="http://img.blog.csdn.net/20170612141520550?" alt="这里写图片描述"></p><p>其中，M是hypothesis的个数，N是样本D的数量，$\epsilon$是参数。该union bound表明，当M有限，且N足够大的时候，Bad Data出现的概率就更低了，即能保证D对于所有的h都有$E_{in}\approx E_{out}$，满足PAC，演算法A的选择不受限制。那么满足这种union bound的情况，我们就可以和之前一样，选取一个合理的演算法（PLA/pocket），选择使$E_{in}$最小的$h_m$作为矩g，一般能够保证$g\approx f$，即有不错的泛化能力。</p><p>所以，如果hypothesis的个数M是有限的，N足够大，那么通过演算法A任意选择一个矩g，都有$E_{in}\approx E_{out}$成立；同时，如果找到一个矩g，使$E_{in}\approx 0$，PAC就能保证$E_{out}\approx 0$。至此，就证明了机器学习是可行的。</p><p><img src="http://img.blog.csdn.net/20170612143915944?" alt="这里写图片描述"></p><p>但是，如上面的学习流程图右下角所示，如果M是无数个，例如之前介绍的PLA直线有无数条，是否这些推论就不成立了呢？是否机器就不能进行学习呢？这些内容和问题，我们下节课再介绍。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了机器学习的可行性。首先引入NFL定理，说明机器学习无法找到一个矩g能够完全和目标函数f一样。接着介绍了可以采用一些统计上的假设，例如Hoeffding不等式，建立$E_{in}$和$E_{out}$的联系，证明对于某个h，当N足够大的时候，$E_{in}$和$E_{out}$是PAC的。最后，对于h个数很多的情况，只要有h个数M是有限的，且N足够大，就能保证$E_{in}\approx E_{out}$，证明机器学习是可行的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170612080753910?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
