<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>红色石头的机器学习之路</title>
  
  <subtitle>公众号ID：redstonewill</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://redstonewill.github.io/"/>
  <updated>2018-03-17T13:52:35.979Z</updated>
  <id>https://redstonewill.github.io/</id>
  
  <author>
    <name>红色石头</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记13 -- Hazard of Overfitting</title>
    <link href="https://redstonewill.github.io/2018/03/17/13/"/>
    <id>https://redstonewill.github.io/2018/03/17/13/</id>
    <published>2018-03-17T12:34:23.000Z</published>
    <updated>2018-03-17T13:52:35.979Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170524101427907?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了非线性分类模型，通过非线性变换，将非线性模型映射到另一个空间，转换为线性模型，再来进行分类，分析了非线性变换可能会使计算复杂度增加。本节课介绍这种模型复杂度增加带来机器学习中一个很重要的问题：过拟合（overfitting）。</p><h3 id="What-is-Overfitting"><a href="#What-is-Overfitting" class="headerlink" title="What is Overfitting?"></a>What is Overfitting?</h3><p>首先，我们通过一个例子来介绍什么bad generalization。假设平面上有5个点，目标函数f(x)是2阶多项式，如果hypothesis是二阶多项式加上一些小的noise的话，那么这5个点很靠近这个hypothesis，$E_{in}$很小。如果hypothesis是4阶多项式，那么这5点会完全落在hypothesis上，$E_{in}=0$。虽然4阶hypothesis的$E_{in}$比2阶hypothesis的要好很多，但是它的$E_{out}$很大。因为根据VC Bound理论，阶数越大，即VC Dimension越大，就会让模型复杂度更高，$E_{out}$更大。我们把这种$E_{in}$很小，$E_{out}$很大的情况称之为bad generation，即泛化能力差。</p><p><img src="http://img.blog.csdn.net/20170524101427907?" alt="这里写图片描述"></p><p>我们回过头来看一下VC曲线：</p><p><img src="http://img.blog.csdn.net/20170524102634154?" alt="这里写图片描述"></p><p>hypothesis的阶数越高，表示VC Dimension越大。随着VC Dimension增大，$E_{in}$是一直减小的，而$E_{out}$先减小后增大。在$d^<em>$位置，$E_{out}$取得最小值。在$d_{VC}^</em>$右侧，随着VC Dimension越来越大，$E_{in}$越来越小，接近于0，$E_{out}$越来越大。即当VC Dimension很大的时候，这种对训练样本拟合过分好的情况称之为过拟合（overfitting）。另一方面，在$d_{VC}^*$左侧，随着VC Dimension越来越小，$E_{in}$和$E_{out}$都越来越大，这种情况称之为欠拟合（underfitting），即模型对训练样本的拟合度太差，VC Dimension太小了。</p><p><img src="http://img.blog.csdn.net/20170524131656547?" alt="这里写图片描述"></p><p>bad generation和overfitting的关系可以理解为：overfitting是VC Dimension过大的一个过程，bad generation是overfitting的结果。</p><p><img src="http://img.blog.csdn.net/20170524132024546?" alt="这里写图片描述"></p><p>一个好的fit，$E_{in}$和$E_{out}$都比较小，尽管$E_{in}$没有足够接近零；而对overfitting来说，$E_{in}\approx0$，但是$E_{out}$很大。那么，overfitting的原因有哪些呢？</p><p>我们举个开车的例子，把发生车祸比作成overfitting，那么造成车祸的原因包括：</p><ul><li><p><strong>车速太快（VC Dimension太大）；</strong></p></li><li><p><strong>道路崎岖（noise）；</strong></p></li><li><p><strong>对路况的了解程度（训练样本数量N不够）；</strong></p></li></ul><p>也就是说，VC Dimension、noise、N这三个因素是影响过拟合现象的关键。</p><p><img src="http://img.blog.csdn.net/20170524141212919?" alt="这里写图片描述"></p><h3 id="The-Role-of-Noise-and-Data-Size"><a href="#The-Role-of-Noise-and-Data-Size" class="headerlink" title="The Role of Noise and Data Size"></a>The Role of Noise and Data Size</h3><p>为了尽可能详细地解释overfitting，我们进行这样一个实验，试验中的数据集不是很大。首先，在二维平面上，一个模型的分布由目标函数f(x)（x的10阶多项式）加上一些noise构成，下图中，离散的圆圈是数据集，目标函数是蓝色的曲线。数据没有完全落在曲线上，是因为加入了noise。</p><p><img src="http://img.blog.csdn.net/20170524141922827?" alt="这里写图片描述"></p><p>然后，同样在二维平面上，另一个模型的分布由目标函数f(x)（x的50阶多项式）构成，没有加入noise。下图中，离散的圆圈是数据集，目标函数是蓝色的曲线。可以看出由于没有noise，数据集完全落在曲线上。</p><p><img src="http://img.blog.csdn.net/20170524142417621?" alt="这里写图片描述"></p><p>现在，有两个学习模型，一个是2阶多项式，另一个是10阶多项式，分别对上面两个问题进行建模。首先，对于第一个目标函数是10阶多项式包含noise的问题，这两个学习模型的效果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170524144520619?" alt="这里写图片描述"></p><p>由上图可知，2阶多项式的学习模型$E_{in}=0.050$，$E_{out}=0.127$；10阶多项式的学习模型$E_{in}=0.034$，$E_{out}=9.00$。虽然10阶模型的$E_{in}$比2阶的小，但是其$E_{out}$要比2阶的大得多，而2阶的$E_{in}$和$E_{out}$相差不大，很明显用10阶的模型发生了过拟合。</p><p>然后，对于第二个目标函数是50阶多项式没有noise的问题，这两个学习模型的效果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170524153305297?" alt="这里写图片描述"></p><p>由上图可知，2阶多项式的学习模型$E_{in}=0.029$，$E_{out}=0.120$；10阶多项式的学习模型$E_{in}=0.00001$，$E_{out}=7680$。虽然10阶模型的$E_{in}$比2阶的小，但是其$E_{out}$要比2阶的大得多的多，而2阶的$E_{in}$和$E_{out}$相差不大，很明显用10阶的模型仍然发生了明显的过拟合。</p><p>上面两个问题中，10阶模型都发生了过拟合，反而2阶的模型却表现得相对不错。这好像违背了我们的第一感觉，比如对于目标函数是10阶多项式，加上noise的模型，按道理来说应该是10阶的模型更能接近于目标函数，因为它们阶数相同。但是，事实却是2阶模型泛化能力更强。这种现象产生的原因，从哲学上来说，就是“以退为进”。有时候，简单的学习模型反而能表现的更好。</p><p>下面从learning curve来分析一下具体的原因，learning curve描述的是$E_{in}$和$E_{out}$随着数据量N的变化趋势。下图中左边是2阶学习模型的learning curve，右边是10阶学习模型的learning curve。</p><p><img src="http://img.blog.csdn.net/20170524155529000?" alt="这里写图片描述"></p><p>我们的第9次课的笔记<a href="http://blog.csdn.net/red_stone1/article/details/71599034" target="_blank" rel="noopener"> NTU林轩田机器学习基石课程学习笔记9 – Linear Regression</a>已经介绍过了learning curve。在learning curve中，横轴是样本数量N，纵轴是Error。$E_{in}$和$E_{out}$可表示为：</p><p>$$E_{in}=noise level\ast (1-\frac{d+1}N) $$</p><p>$$E_{out}=noise level\ast (1+\frac{d+1}N) $$</p><p>其中d为模型阶次，左图中d=2，右图中d=10。</p><p>本节的实验问题中，数据量N不大，即对应于上图中的灰色区域。左图的灰色区域中，因为d=2，$E_{in}$和$E_{out}$相对来说比较接近；右图中的灰色区域中，d=10，根据$E_{in}$和$E_{out}$的表达式，$E_{in}$很小，而$E_{out}$很大。这就解释了之前2阶多项式模型的$E_{in}$更接近$E_{out}$，泛化能力更好。</p><p>值得一提的是，如果数据量N很大的时候，上面两图中$E_{in}$和$E_{out}$都比较接近，但是对于高阶模型，z域中的特征很多的时候，需要的样本数量N很大，且容易发生维度灾难。关于维度灾难的详细生动解释，请参考我另一篇博文：</p><p><a href="http://blog.csdn.net/red_stone1/article/details/71692444" target="_blank" rel="noopener">机器学习中的维度灾难</a></p><p>另一个例子中，目标函数是50阶多项式，且没有加入noise。这种情况下，我们发现仍然是2阶的模型拟合的效果更好一些，明明没有noise，为什么是这样的结果呢？</p><p>实际上，我们忽略了一个问题：这种情况真的没有noise吗？其实，当模型很复杂的时候，即50阶多项式的目标函数，无论是2阶模型还是10阶模型，都不能学习的很好，这种复杂度本身就会引入一种‘noise’。所以，这种高阶无noise的问题，也可以类似于10阶多项式的目标函数加上noise的情况，只是二者的noise有些许不同，下面一部分将会详细解释。</p><h3 id="Deterministic-Noise"><a href="#Deterministic-Noise" class="headerlink" title="Deterministic Noise"></a>Deterministic Noise</h3><p>下面我们介绍一个更细节的实验来说明 什么时候小心overfit会发生。假设我们产生的数据分布由两部分组成：第一部分是目标函数f(x)，$Q_f$阶多项式；第二部分是噪声$\epsilon$，服从Gaussian分布。接下来我们分析的是noise强度不同对overfitting有什么样的影响。总共的数据量是N。</p><p><img src="http://img.blog.csdn.net/20170524164606683?" alt="这里写图片描述"></p><p>那么下面我们分析不同的$(N,\sigma^2)$和$(N,Q_f)$对overfit的影响。overfit可以量化为$E_{out}-E_{in}$。结果如下：</p><p><img src="http://img.blog.csdn.net/20170524165036419?" alt="这里写图片描述"></p><p>上图中，红色越深，代表overfit程度越高，蓝色越深，代表overfit程度越低。先看左边的图，左图中阶数$Q_f$固定为20，横坐标代表样本数量N，纵坐标代表噪声水平$\sigma^2$。红色区域集中在N很小或者$\sigma^2$很大的时候，也就是说N越大，$\sigma^2$越小，越不容易发生overfit。右边图中$\sigma^2=0.1$，横坐标代表样本数量N，纵坐标代表目标函数阶数$Q_f$。红色区域集中在N很小或者$Q_f$很大的时候，也就是说N越大，$Q_f$越小，越不容易发生overfit。上面两图基本相似。</p><p>从上面的分析，我们发现$\sigma^2$对overfit是有很大的影响的，我们把这种noise称之为stochastic noise。同样地，$Q_f$即模型复杂度也对overfit有很大影响，而且二者影响是相似的，所以我们把这种称之为deterministic noise。之所以把它称为noise，是因为模型高复杂度带来的影响。</p><p>总结一下，有四个因素会导致发生overfitting：</p><ul><li><p><strong>data size N $\downarrow$</strong></p></li><li><p><strong>stochastic noise $\sigma^2\uparrow$</strong></p></li><li><p><strong>deterministic noise $Q_f\uparrow$</strong></p></li><li><p><strong>excessive power $\uparrow$</strong></p></li></ul><p>我们刚才解释了如果目标函数f(x)的复杂度很高的时候，那么跟有noise也没有什么两样。因为目标函数很复杂，那么再好的hypothesis都会跟它有一些差距，我们把这种差距称之为deterministic noise。deterministic noise与stochastic noise不同，但是效果一样。其实deterministic noise类似于一个伪随机数发生器，它不会产生真正的随机数，而只产生伪随机数。它的值与hypothesis有关，且固定点x的deterministic noise值是固定的。</p><h3 id="Dealing-with-Overfitting"><a href="#Dealing-with-Overfitting" class="headerlink" title="Dealing with Overfitting"></a>Dealing with Overfitting</h3><p>现在我们知道了什么是overfitting，和overfitting产生的原因，那么如何避免overfitting呢？避免overfitting的方法主要包括：</p><ul><li><p><strong>start from simple model</strong></p></li><li><p><strong>data cleaning/pruning</strong></p></li><li><p><strong>data hinting</strong></p></li><li><p><strong>regularization</strong></p></li><li><p><strong>validataion</strong></p></li></ul><p>这几种方法类比于之前举的开车的例子，对应如下：</p><p><img src="http://img.blog.csdn.net/20170524215244275?" alt="这里写图片描述"></p><p>regularization和validation我们之后的课程再介绍，本节课主要介绍简单的data cleaning/pruning和data hinting两种方法。</p><p>data cleaning/pruning就是对训练数据集里label明显错误的样本进行修正（data cleaning），或者对错误的样本看成是noise，进行剔除（data pruning）。data cleaning/pruning关键在于如何准确寻找label错误的点或者是noise的点，而且如果这些点相比训练样本N很小的话，这种处理效果不太明显。</p><p>data hinting是针对N不够大的情况，如果没有办法获得更多的训练集，那么data hinting就可以对已知的样本进行简单的处理、变换，从而获得更多的样本。举个例子，数字分类问题，可以对已知的数字图片进行轻微的平移或者旋转，从而让N丰富起来，达到扩大训练集的目的。这种额外获得的例子称之为virtual examples。但是要注意一点的就是，新获取的virtual examples可能不再是iid某个distribution。所以新构建的virtual examples要尽量合理，且是独立同分布的。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了overfitting的概念，即当$E_{in}$很小，$E_{out}$很大的时候，会出现overfitting。详细介绍了overfitting发生的四个常见原因data size N、stochastic noise、deterministic noise和excessive power。解决overfitting的方法有很多，本节课主要介绍了data cleaning/pruning和data hinting两种简单的方法，之后的课程将会详细介绍regularization和validataion两种更重要的方法。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170524101427907?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记12 -- Nonlinear Transformation</title>
    <link href="https://redstonewill.github.io/2018/03/17/12/"/>
    <id>https://redstonewill.github.io/2018/03/17/12/</id>
    <published>2018-03-17T12:27:40.000Z</published>
    <updated>2018-03-17T12:33:25.012Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170523005800773?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们介绍了分类问题的三种线性模型，可以用来解决binary classification和multiclass classification问题。本节课主要介绍非线性的模型来解决分类问题。</p><h3 id="Quadratic-Hypothesis"><a href="#Quadratic-Hypothesis" class="headerlink" title="Quadratic Hypothesis"></a>Quadratic Hypothesis</h3><p>之前介绍的线性模型，在2D平面上是一条直线，在3D空间中是一个平面。数学上，我们用线性得分函数s来表示：$s=w^Tx$。其中，x为特征值向量，w为权重，s是线性的。</p><p><img src="http://img.blog.csdn.net/20170522205736151?" alt="这里写图片描述"></p><p>线性模型的优点就是，它的VC Dimension比较小，保证了$E_{in}\approx E_{out}$。但是缺点也很明显，对某些非线性问题，可能会造成$E_{in}$很大，虽然$E_{in}\approx E_{out}$，但是也造成$E_{out}$很大，分类效果不佳。</p><p><img src="http://img.blog.csdn.net/20170522210031309?" alt="这里写图片描述"></p><p>为了解决线性模型的缺点，我们可以使用非线性模型来进行分类。例如数据集D不是线性可分的，而是圆形可分的，圆形内部是正类，外面是负类。假设它的hypotheses可以写成：<br>$$h_{SEP}(x)=sign(-x_1^2-x_2^2+0.6)$$<br>基于这种非线性思想，我们之前讨论的PLA、Regression问题都可以有非线性的形式进行求解。</p><p><img src="http://img.blog.csdn.net/20170522210959142?" alt="这里写图片描述"></p><p>下面介绍如何设计这些非线性模型的演算法。还是上面介绍的平面圆形分类例子，它的h(x)的权重w0=0.6，w1=-1，w2=-1，但是h(x)的特征不是线性模型的$(1,x_1,x_2)$，而是$(1,x_1^2,x_2^2)$。我们令$z_0=1$，$z_1=x_1^2$，$z_2=x_2^2$，那么，h(x)变成：</p><p>$$h(x)=sign(\breve{w}_0\cdot z_0+\breve{w}_1\cdot z_1+\breve{w}_2\cdot z_2)=sign(0.6\cdot z_0-1\cdot z_1-1\cdot z_2)=sign(\breve{w}^Tz)$$</p><p>这种$x_n\rightarrow z_n$的转换可以看成是x空间的点映射到z空间中去，而在z域中，可以用一条直线进行分类，也就是从x空间的圆形可分映射到z空间的线性可分。z域中的直线对应于x域中的圆形。因此，我们把$x_n\rightarrow z_n$这个过程称之为特征转换（Feature Transform）。通过这种特征转换，可以将非线性模型转换为另一个域中的线性模型。</p><p><img src="http://img.blog.csdn.net/20170523005800773?" alt="这里写图片描述"></p><p>已知x域中圆形可分在z域中是线性可分的，那么反过来，如果在z域中线性可分，是否在x域中一定是圆形可分的呢？答案是否定的。由于权重向量w取值不同，x域中的hypothesis可能是圆形、椭圆、双曲线等等多种情况。</p><p><img src="http://img.blog.csdn.net/20170523091547803?" alt="这里写图片描述"></p><p>目前讨论的x域中的圆形都是圆心过原点的，对于圆心不过原点的一般情况，$x_n\rightarrow z_n$映射公式包含的所有项为：</p><p>$$\Phi_2(x)=(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)$$</p><p>也就是说，对于二次hypothesis，它包含二次项、一次项和常数项1，z域中每一条线对应x域中的某二次曲线的分类方式，也许是圆，也许是椭圆，也许是双曲线等等。那么z域中的hypothesis可以写成：</p><p><img src="http://img.blog.csdn.net/20170523093213481?" alt="这里写图片描述"></p><h3 id="Nonlinear-Transform"><a href="#Nonlinear-Transform" class="headerlink" title="Nonlinear Transform"></a>Nonlinear Transform</h3><p>上一部分我们定义了什么了二次hypothesis，那么这部分将介绍如何设计一个好的二次hypothesis来达到良好的分类效果。那么目标就是在z域中设计一个最佳的分类线。</p><p><img src="http://img.blog.csdn.net/20170523094016913?" alt="这里写图片描述"></p><p>其实，做法很简单，利用映射变换的思想，通过映射关系，把x域中的最高阶二次的多项式转换为z域中的一次向量，也就是从quardratic hypothesis转换成了perceptrons问题。用z值代替x多项式，其中向量z的个数与x域中x多项式的个数一致（包含常数项）。这样就可以在z域中利用线性分类模型进行分类训练。训练好的线性模型之后，再将z替换为x的多项式就可以了。具体过程如下：</p><p><img src="http://img.blog.csdn.net/20170523095731798?" alt="这里写图片描述"></p><p>整个过程就是通过映射关系，换个空间去做线性分类，重点包括两个：</p><ul><li><p>特征转换</p></li><li><p>训练线性模型</p></li></ul><p>其实，我们以前处理机器学习问题的时候，已经做过类似的特征变换了。比如数字识别问题，我们从原始的像素值特征转换为一些实际的concrete特征，比如密度、对称性等等，这也用到了feature transform的思想。</p><p><img src="http://img.blog.csdn.net/20170523100508738?" alt="这里写图片描述"></p><h3 id="Price-of-Nonlinear-Transform"><a href="#Price-of-Nonlinear-Transform" class="headerlink" title="Price of Nonlinear Transform"></a>Price of Nonlinear Transform</h3><p>若x特征维度是d维的，也就是包含d个特征，那么二次多项式个数，即z域特征维度是：<br>$$\breve d=1+C_d^1+C_d^2+d=\frac{d(d+3)}2+1$$<br>如果x特征维度是2维的，即$(x_1,x_2)$，那么它的二次多项式为$(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)$，有6个。</p><p>现在，如果阶数更高，假设阶数为Q，那么对于x特征维度是d维的，它的z域特征维度为：<br>$$\breve d=C_{Q+d}^Q=C_{Q+d}^d=O(Q^d)$$<br>由上式可以看出，计算z域特征维度个数的时间复杂度是Q的d次方，随着Q和d的增大，计算量会变得很大。同时，空间复杂度也大。也就是说，这种特征变换的一个代价是计算的时间、空间复杂度都比较大。</p><p><img src="http://img.blog.csdn.net/20170523105942602?" alt="这里写图片描述"></p><p>另一方面，z域中特征个数随着Q和d增加变得很大，同时权重w也会增大，即自由度增加，VC Dimension增大。令z域中的特征维度是$1+\breve d$，则在在域中，任何$\breve d+2$的输入都不能被shattered；同样，在x域中，任何$\breve d+2$的输入也不能被shattered。$\breve d+1$是VC Dimension的上界，如果$\breve d+1$很大的时候，相应的VC Dimension就会很大。根据之前章节课程的讨论，VC Dimension过大，模型的泛化能力会比较差。</p><p><img src="http://img.blog.csdn.net/20170523111019007?" alt="这里写图片描述"></p><p>下面通过一个例子来解释为什么VC Dimension过大，会造成不好的分类效果：</p><p><img src="http://img.blog.csdn.net/20170523111216948?" alt="这里写图片描述"></p><p>上图中，左边是用直线进行线性分类，有部分点分类错误；右边是用四次曲线进行非线性分类，所有点都分类正确，那么哪一个分类效果好呢？单从平面上这些训练数据来看，四次曲线的分类效果更好，但是四次曲线模型很容易带来过拟合的问题，虽然它的$E_{in}$比较小，从泛化能力上来说，还是左边的分类器更好一些。也就是说VC Dimension过大会带来过拟合问题，$\breve d+1$不能太大了。</p><p>那么如何选择合适的Q，来保证不会出现过拟合问题，使模型的泛化能力强呢？一般情况下，为了尽量减少特征自由度，我们会根据训练样本的分布情况，人为地减少、省略一些项。但是，这种人为地删减特征会带来一些“自我分析”代价，虽然对训练样本分类效果好，但是对训练样本外的样本，不一定效果好。所以，一般情况下，还是要保存所有的多项式特征，避免对训练样本的人为选择。</p><p><img src="http://img.blog.csdn.net/20170523113636362?" alt="这里写图片描述"></p><h3 id="Structured-Hypothesis-Sets"><a href="#Structured-Hypothesis-Sets" class="headerlink" title="Structured Hypothesis Sets"></a>Structured Hypothesis Sets</h3><p>下面，我们讨论一下从x域到z域的多项式变换。首先，如果特征维度只有1维的话，那么变换多项式只有常数项：</p><p>$$\Phi_0(x)=(1)$$</p><p>如果特征维度是两维的，变换多项式包含了一维的$\Phi_0(x)$：</p><p>$$\Phi_1(x)=(\Phi_0(x),x_1,x_2,\ldots,x_d)$$</p><p>如果特征维度是三维的，变换多项式包含了二维的$\Phi_1(x)$：</p><p>$$\Phi_2(x)=(\Phi_1(x),x_1^2,x_1x_2,\ldots,x_d^2)$$</p><p>以此类推，如果特征维度是Q次，那么它的变换多项式为：</p><p>$$\Phi_Q(x)=(\Phi_{Q-1}(x),x_1^Q,x_1^{Q-1}x_2,\cdots,x_d^Q)$$</p><p>那么对于不同阶次构成的hypothesis有如下关系：</p><p>$$H_{\Phi_0} \subset H_{\Phi_1} \subset H_{\Phi_2} \subset \cdots \subset H_{\Phi_Q}$$</p><p>我们把这种结构叫做Structured Hypothesis Sets：</p><p><img src="http://img.blog.csdn.net/20170523132015872?" alt="这里写图片描述"></p><p>那么对于这种Structured Hypothesis Sets，它们的VC Dimension满足下列关系：</p><p>$$d_{VC}(H_0)\leq d_{VC}(H_1)\leq d_{VC}(H_2)\leq \cdots \leq d_{VC}(H_Q)$$</p><p>它的$E_{in}$满足下列关系：</p><p>$$E_{in}(g_0)\geq E_{in}(g_1)\geq E_{in}(g_2)\geq \cdots \geq E_{in}(g_Q)$$</p><p><img src="http://img.blog.csdn.net/20170523133004123?" alt="这里写图片描述"></p><p>从上图中也可以看到，随着变换多项式的阶数增大，虽然$E_{in}$逐渐减小，但是model complexity会逐渐增大，造成$E_{out}$很大，所以阶数不能太高。</p><p>那么，如果选择的阶数很大，确实能使$E_{in}$接近于0，但是泛化能力通常很差，我们把这种情况叫做tempting sin。所以，一般最合适的做法是先从低阶开始，如先选择一阶hypothesis，看看$E_{in}$是否很小，如果$E_{in}$足够小的话就选择一阶，如果$E_{in}$大的话，再逐渐增加阶数，直到满足要求为止。也就是说，尽量选择低阶的hypothes，这样才能得到较强的泛化能力。</p><p><img src="http://img.blog.csdn.net/20170523133948471?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>这节课主要介绍了非线性分类模型，通过非线性变换，将非线性模型映射到另一个空间，转换为线性模型，再来进行线性分类。本节课完整介绍了非线性变换的整体流程，以及非线性变换可能会带来的一些问题：时间复杂度和空间复杂度的增加。最后介绍了在要付出代价的情况下，使用非线性变换的最安全的做法，尽可能使用简单的模型，而不是模型越复杂越好。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170523005800773?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记11 -- Linear Models for Classification</title>
    <link href="https://redstonewill.github.io/2018/03/17/11/"/>
    <id>https://redstonewill.github.io/2018/03/17/11/</id>
    <published>2018-03-17T06:44:47.000Z</published>
    <updated>2018-03-17T06:46:21.892Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170517220835526?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们介绍了Logistic Regression问题，建立cross-entropy error，并提出使用梯度下降算法gradient descnt来获得最好的logistic hypothesis。本节课继续介绍使用线性模型来解决分类问题。</p><h3 id="Linear-Models-for-Binary-Classification"><a href="#Linear-Models-for-Binary-Classification" class="headerlink" title="Linear Models for Binary Classification"></a>Linear Models for Binary Classification</h3><p>之前介绍几种线性模型都有一个共同点，就是都有样本特征x的加权运算，我们引入一个线性得分函数s：</p><p>$$s=w^Tx$$</p><p>三种线性模型，第一种是linear classification。线性分类模型的hypothesis为$h(x)=sign(s)$,取值范围为{-1,+1}两个值，它的err是0/1的，所以对应的$E_{in}(w)$是离散的，并不好解，这是个NP-hard问题。第二种是linear regression。线性回归模型的hypothesis为$h(x)=s$，取值范围为整个实数空间，它的err是squared的，所以对应的$E_{in}(w)$是开口向上的二次曲线，其解是closed-form的，直接用线性最小二乘法求解即可。第三种是logistic regression。逻辑回归模型的hypothesis为$h(x)=\theta(s)$，取值范围为(-1,1)之间，它的err是cross-entropy的，所有对应的$E_{in}(w)$是平滑的凸函数，可以使用梯度下降算法求最小值。</p><p><img src="http://img.blog.csdn.net/20170517220835526?" alt="这里写图片描述"></p><p>从上图中，我们发现，linear regression和logistic regression的error function都有最小解。那么可不可以用这两种方法来求解linear classification问题呢？下面，我们来对这三种模型的error function进行分析，看看它们之间有什么联系。</p><p>对于linear classification，它的error function可以写成：<br>$$err_{0/1}(s,y)=|sign(s)\neq y|=|sign(ys)\neq 1|$$<br>对于linear regression，它的error function可以写成：<br>$$err_{SQR}(s,y)=(s-y)^2=(ys-1)^2$$<br>对于logistic regression，它的error function可以写成：<br>$$err_{CE}(s,y)=ln(1+exp(-ys))$$<br>上述三种模型的error function都引入了ys变量，那么ys的物理意义是什么？ys就是指分类的正确率得分，其值越大越好，得分越高。</p><p><img src="http://img.blog.csdn.net/20170517232731911?" alt="这里写图片描述"></p><p>下面，我们用图形化的方式来解释三种模型的error function到底有什么关系：</p><p><img src="http://img.blog.csdn.net/20170518095055714?" alt="这里写图片描述"></p><p>从上图中可以看出，ys是横坐标轴，$err_{0/1}$是呈阶梯状的，在ys&gt;0时，$err_{0/1}$恒取最小值0。$err_{SQR}$呈抛物线形式，在ys=1时，取得最小值，且在ys=1左右很小区域内，$err_{0/1}$和$err_{SQR}$近似。$err_{CE}$是呈指数下降的单调函数，ys越大，其值越小。同样在ys=1左右很小区域内，$err_{0/1}$和$err_{CE}$近似。但是我们发现$err_{CE}$并不是始终在$err_{0/1}$之上，所以为了计算讨论方便，我们把$err_{CE}$做幅值上的调整，引入$err_{SCE}=log_2(1+exp(-ys))=\frac1{ln2}err_{CE}$，这样能保证$err_{SCE}$始终在$err_{0/1}$上面，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170518134827804?" alt="这里写图片描述"></p><p>由上图可以看出：<br>$$err_{0/1}(s,y)\leq err_{SCE}(s,y)=\frac1{ln2}err_{CE}(s,y)$$<br>$$E_{in}^{0/1}(w)\leq E_{in}^{SCE}(w)=\frac1{ln2}E_{in}^{CE}(w)$$<br>$$E_{out}^{0/1}(w)\leq E_{out}^{SCE}(w)=\frac1{ln2}E_{out}^{CE}(w)$$<br>那么由VC理论可以知道：<br>从0/1出发：<br>$$E_{out}^{0/1}(w)\leq E_{in}^{0/1}(w)+\Omega^{0/1}\leq \frac1{ln2}E_{in}^{CE}(w)+\Omega^{0/1}$$<br>从CE出发：<br>$$E_{out}^{0/1}(w)\leq \frac1{ln2}E_{out}^{CE}(w)\leq \frac1{ln2}E_{in}^{CE}(w)+\frac1{ln2}\Omega^{CE}$$</p><p><img src="http://img.blog.csdn.net/20170518205526337?" alt="这里写图片描述"></p><p>通过上面的分析，我们看到err 0/1是被限定在一个上界中。这个上界是由logistic regression模型的error function决定的。而linear regression其实也是linear classification的一个upper bound，只是随着sy偏离1的位置越来越远，linear regression的error function偏差越来越大。综上所述，linear regression和logistic regression都可以用来解决linear classification的问题。</p><p>下图列举了PLA、linear regression、logistic regression模型用来解linear classification问题的优点和缺点。通常，我们使用linear regression来获得初始化的$w_0$，再用logistic regression模型进行最优化解。</p><p><img src="http://img.blog.csdn.net/20170518142147055?" alt="这里写图片描述"></p><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>之前介绍的PLA算法和logistic regression算法，都是用到了迭代操作。PLA每次迭代只会更新一个点，它每次迭代的时间复杂度是O(1)；而logistic regression每次迭代要对所有N个点都进行计算，它的每时间复杂度是O(N)。为了提高logistic regression中gradient descent算法的速度，可以使用另一种算法：随机梯度下降算法(Stochastic Gradient Descent)。</p><p>随机梯度下降算法每次迭代只找到一个点，计算该点的梯度，作为我们下一步更新w的依据。这样就保证了每次迭代的计算量大大减小，我们可以把整体的梯度看成这个随机过程的一个期望值。</p><p><img src="http://img.blog.csdn.net/20170518144632739?" alt="这里写图片描述"></p><p>随机梯度下降可以看成是真实的梯度加上均值为零的随机噪声方向。单次迭代看，好像会对每一步找到正确梯度方向有影响，但是整体期望值上看，与真实梯度的方向没有差太多，同样能找到最小值位置。随机梯度下降的优点是减少计算量，提高运算速度，而且便于online学习；缺点是不够稳定，每次迭代并不能保证按照正确的方向前进，而且达到最小值需要迭代的次数比梯度下降算法一般要多。</p><p><img src="http://img.blog.csdn.net/20170518150924733?" alt="这里写图片描述"></p><p>对于logistic regression的SGD，它的表达式为：<br>$$w_{t+1}\leftarrow w_t+\eta\theta(-y_nw_t^Tx_n)(y_nx_n)$$</p><p>我们发现，SGD与PLA的迭代公式有类似的地方，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170518153018200?" alt="这里写图片描述"></p><p>我们把SGD logistic regression称之为’soft’ PLA，因为PLA只对分类错误的点进行修正，而SGD logistic regression每次迭代都会进行或多或少的修正。另外，当$\eta=1$，且$w_t^Tx_n$足够大的时候，PLA近似等于SGD。</p><p><img src="http://img.blog.csdn.net/20170518154930194?" alt="这里写图片描述"></p><p>除此之外，还有两点需要说明：1、SGD的终止迭代条件。没有统一的终止条件，一般让迭代次数足够多；2、学习速率$\eta$。$\eta$的取值是根据实际情况来定的，一般取值0.1就可以了。</p><h3 id="Multiclass-via-Logistic-Regression"><a href="#Multiclass-via-Logistic-Regression" class="headerlink" title="Multiclass via Logistic Regression"></a>Multiclass via Logistic Regression</h3><p>之前我们一直讲的都是二分类问题，本节主要介绍多分类问题，通过linear classification来解决。假设平面上有四个类，分别是正方形、菱形、三角形和星形，如何进行分类模型的训练呢？</p><p>首先我们可以想到这样一个办法，就是先把正方形作为正类，其他三种形状都是负类，即把它当成一个二分类问题，通过linear classification模型进行训练，得出平面上某个图形是不是正方形，且只有{-1,+1}两种情况。然后再分别以菱形、三角形、星形为正类，进行二元分类。这样进行四次二分类之后，就完成了这个多分类问题。</p><p><img src="http://img.blog.csdn.net/20170518195225232?" alt="这里写图片描述"></p><p>但是，这样的二分类会带来一些问题，因为我们只用{-1，+1}两个值来标记，那么平面上某些可能某些区域都被上述四次二分类模型判断为负类，即不属于四类中的任何一类；也可能会出现某些区域同时被两个类甚至多个类同时判断为正类，比如某个区域又判定为正方形又判定为菱形。那么对于这种情况，我们就无法进行多类别的准确判断，所以对于多类别，简单的binary classification不能解决问题。</p><p>针对这种问题，我们可以使用另外一种方法来解决：soft软性分类，即不用{-1，+1}这种binary classification，而是使用logistic regression，计算某点属于某类的概率、可能性，去概率最大的值为那一类就好。</p><p>soft classification的处理过程和之前类似，同样是分别令某类为正，其他三类为负，不同的是得到的是概率值，而不是{-1，+1}。最后得到某点分别属于四类的概率，取最大概率对应的哪一个类别就好。效果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170518200527283?" alt="这里写图片描述"></p><p>这种多分类的处理方式，我们称之为One-Versus-All(OVA) Decomposition。这种方法的优点是简单高效，可以使用logistic regression模型来解决；缺点是如果数据类别很多时，那么每次二分类问题中，正类和负类的数量差别就很大，数据不平衡unbalanced，这样会影响分类效果。但是，OVA还是非常常用的一种多分类算法。</p><p><img src="http://img.blog.csdn.net/20170518201255231?" alt="这里写图片描述"></p><h3 id="Multiclass-via-Binary-Classification"><a href="#Multiclass-via-Binary-Classification" class="headerlink" title="Multiclass via Binary Classification"></a>Multiclass via Binary Classification</h3><p>上一节，我们介绍了多分类算法OVA，但是这种方法存在一个问题，就是当类别k很多的时候，造成正负类数据unbalanced，会影响分类效果，表现不好。现在，我们介绍另一种方法来解决当k很大时，OVA带来的问题。</p><p>这种方法呢，每次只取两类进行binary classification，取值为{-1，+1}。假如k=4，那么总共需要进行$C_4^2=6$次binary classification。那么，六次分类之后，如果平面有个点，有三个分类器判断它是正方形，一个分类器判断是菱形，另外两个判断是三角形，那么取最多的那个，即判断它属于正方形，我们的分类就完成了。这种形式就如同k个足球对进行单循环的比赛，每场比赛都有一个队赢，一个队输，赢了得1分，输了得0分。那么总共进行了$C_k^2$次的比赛，最终取得分最高的那个队就可以了。</p><p><img src="http://img.blog.csdn.net/20170518203420275?" alt="这里写图片描述"></p><p>这种区别于OVA的多分类方法叫做One-Versus-One(OVO)。这种方法的优点是更加高效，因为虽然需要进行的分类次数增加了，但是每次只需要进行两个类别的比较，也就是说单次分类的数量减少了。而且一般不会出现数据unbalanced的情况。缺点是需要分类的次数多，时间复杂度和空间复杂度可能都比较高。</p><p><img src="http://img.blog.csdn.net/20170518203941888?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了分类问题的三种线性模型：linear classification、linear regression和logistic regression。首先介绍了这三种linear models都可以来做binary classification。然后介绍了比梯度下降算法更加高效的SGD算法来进行logistic regression分析。最后讲解了两种多分类方法，一种是OVA，另一种是OVO。这两种方法各有优缺点，当类别数量k不多的时候，建议选择OVA，以减少分类次数。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170517220835526?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记10 -- Logistic Regression</title>
    <link href="https://redstonewill.github.io/2018/03/17/10/"/>
    <id>https://redstonewill.github.io/2018/03/17/10/</id>
    <published>2018-03-17T06:42:15.000Z</published>
    <updated>2018-03-17T06:44:06.377Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170515225915831?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们介绍了Linear Regression线性回归，以及用平方错误来寻找最佳的权重向量w，获得最好的线性预测。本节课将介绍Logistic Regression逻辑回归问题。</p><h3 id="Logistic-Regression-Problem"><a href="#Logistic-Regression-Problem" class="headerlink" title="Logistic Regression Problem"></a>Logistic Regression Problem</h3><p>一个心脏病预测的问题：根据患者的年龄、血压、体重等信息，来预测患者是否会有心脏病。很明显这是一个二分类问题，其输出y只有{-1,1}两种情况。</p><p>二元分类，一般情况下，理想的目标函数f(x)&gt;0.5，则判断为正类1；若f(x)&lt;0.5，则判断为负类-1。</p><p><img src="http://img.blog.csdn.net/20170515225915831?" alt="这里写图片描述"></p><p>但是，如果我们想知道的不是患者有没有心脏病，而是到底患者有多大的几率是心脏病。这表示，我们更关心的是目标函数的值（分布在0,1之间），表示是正类的概率（正类表示是心脏病）。这跟我们原来讨论的二分类问题不太一样，我们把这个问题称为软性二分类问题（’soft’ binary classification）。这个值越接近1，表示正类的可能性越大；越接近0，表示负类的可能性越大。</p><p><img src="http://img.blog.csdn.net/20170515230741609?" alt="这里写图片描述"></p><p>对于软性二分类问题，理想的数据是分布在[0,1]之间的具体值，但是实际中的数据只可能是0或者1，我们可以把实际中的数据看成是理想数据加上了噪声的影响。</p><p><img src="http://img.blog.csdn.net/20170515232055978?" alt="这里写图片描述"></p><p>如果目标函数是$f(x)=P(+1|x)\in[0,1]$的话，我们如何找到一个好的Hypothesis跟这个目标函数很接近呢？</p><p>首先，根据我们之前的做法，对所有的特征值进行加权处理。计算的结果s，我们称之为’risk score’：</p><p><img src="http://img.blog.csdn.net/20170515233141740?" alt="这里写图片描述"></p><p>但是特征加权和$s\in(-\infty,+\infty)$，如何将s值限定在[0,1]之间呢？一个方法是使用sigmoid Function，记为$\theta(s)$。那么我们的目标就是找到一个hypothesis：$h(x)=\theta(w^Tx)$。</p><p><img src="http://img.blog.csdn.net/20170515233941909?" alt="这里写图片描述"></p><p>Sigmoid Function函数记为$\theta(s)=\frac1{1+e^{-s}}$，满足$\theta(-\infty)=0$，$\theta(0)=\frac12$，$\theta(+\infty)=1$。这个函数是平滑的、单调的S型函数。则对于逻辑回归问题，hypothesis就是这样的形式：</p><p>$$h(x)=\frac1{1+e^{-w^Tx}}$$</p><p>那我们的目标就是求出这个预测函数h(x)，使它接近目标函数f(x)。</p><h3 id="Logistic-Regression-Error"><a href="#Logistic-Regression-Error" class="headerlink" title="Logistic Regression Error"></a>Logistic Regression Error</h3><p>现在我们将Logistic Regression与之前讲的Linear Classification、Linear Regression做个比较：</p><p><img src="http://img.blog.csdn.net/20170515235513276?" alt="这里写图片描述"></p><p>这三个线性模型都会用到线性scoring function $s=w^Tx$。linear classification的误差使用的是0/1 err；linear regression的误差使用的是squared err。那么logistic regression的误差该如何定义呢？</p><p>先介绍一下“似然性”的概念。目标函数$f(x)=P(+1|x)$，如果我们找到了hypothesis很接近target function。也就是说，在所有的Hypothesis集合中找到一个hypothesis与target function最接近，能产生同样的数据集D，包含y输出label，则称这个hypothesis是最大似然likelihood。</p><p><img src="http://img.blog.csdn.net/20170516001146598?" alt="这里写图片描述"></p><p>logistic function: $h(x)=\theta(w^Tx)$满足一个性质：$1-h(x)=h(-x)$。那么，似然性h:</p><p>$$likelihood(h)=P(x_1)h(+x_1)\times P(x_2)h(-x_2)\times \cdots P(x_N)h(-x_N)$$</p><p>因为$P(x_n)$对所有的h来说，都是一样的，所以我们可以忽略它。那么我们可以得到logistic h正比于所有的$h(y_nx)$乘积。我们的目标就是让乘积值最大化。</p><p><img src="http://img.blog.csdn.net/20170516002847044?" alt="这里写图片描述"></p><p>如果将w代入的话：</p><p><img src="http://img.blog.csdn.net/20170516002958701?" alt="这里写图片描述"></p><p>为了把连乘问题简化计算，我们可以引入ln操作，让连乘转化为连加：</p><p><img src="http://img.blog.csdn.net/20170516003257812?" alt="这里写图片描述"></p><p>接着，我们将maximize问题转化为minimize问题，添加一个负号就行，并引入平均数操作$\frac1N$：</p><p><img src="http://img.blog.csdn.net/20170516003559844?" alt="这里写图片描述"></p><p>将logistic function的表达式带入，那么minimize问题就会转化为如下形式：</p><p><img src="http://img.blog.csdn.net/20170516003823843?" alt="这里写图片描述"></p><p>至此，我们得到了logistic regression的err function，称之为cross-entropy error交叉熵误差：</p><p><img src="http://img.blog.csdn.net/20170516004024658?" alt="这里写图片描述"></p><h3 id="Gradient-of-Logistic-Regression-Error"><a href="#Gradient-of-Logistic-Regression-Error" class="headerlink" title="Gradient of Logistic Regression Error"></a>Gradient of Logistic Regression Error</h3><p>我们已经推导了$E_{in}$的表达式，那接下来的问题就是如何找到合适的向量w，让$E_{in}$最小。</p><p><img src="http://img.blog.csdn.net/20170516090812306?" alt="这里写图片描述"></p><p>Logistic Regression的$E_{in}$是连续、可微、二次可微的凸曲线（开口向上），根据之前Linear Regression的思路，我们只要计算$E_{in}$的梯度为零时的w，即为最优解。</p><p><img src="http://img.blog.csdn.net/20170516091637644?" alt="这里写图片描述"></p><p>对$E_{in}$计算梯度，学过微积分的都应该很容易计算出来：</p><p><img src="http://img.blog.csdn.net/20170516092312216?" alt="这里写图片描述"></p><p>最终得到的梯度表达式为：</p><p><img src="http://img.blog.csdn.net/20170516092503784?" alt="这里写图片描述"></p><p>为了计算$E_{in}$最小值，我们就要找到让$\nabla E_{in}(w)$等于0的位置。</p><p><img src="http://img.blog.csdn.net/20170516093504716?" alt="这里写图片描述"></p><p>上式可以看成$\theta(-y_nw^Tx_n)$是$-y_nx_n$的线性加权。要求$\theta(-y_nw^Tx_n)$与$-y_nx_n$的线性加权和为0，那么一种情况是线性可分，如果所有的权重$\theta(-y_nw^Tx_n)$为0，那就能保证$\nabla E_{in}(w)$为0。$\theta(-y_nw^Tx_n)$是sigmoid function，根据其特性，只要让$-y_nw^Tx_n≪0 $，即$y_nw^Tx_n≫0 $。$y_nw^Tx_n≫0 $表示对于所有的点，$y_n$与$w^Tx_n$都是同号的，这表示数据集D必须是全部线性可分的才能成立。</p><p>然而，保证所有的权重$\theta(-y_nw^Tx_n)$为0是不太现实的，总有不等于0的时候，那么另一种常见的情况是非线性可分，只能通过使加权和为零，来求解w。这种情况没有closed-form解，与Linear Regression不同，只能用迭代方法求解。</p><p><img src="http://img.blog.csdn.net/20170516100435914?" alt="这里写图片描述"></p><p>之前所说的Linear Regression有closed-form解，可以说是“一步登天”的；但是PLA算法是一步一步修正迭代进行的，每次对错误点进行修正，不断更新w值。PLA的迭代优化过程表示如下：</p><p><img src="http://img.blog.csdn.net/20170516100842890?" alt="这里写图片描述"></p><p>w每次更新包含两个内容：一个是每次更新的方向$y_nx_n$，用$v$表示，另一个是每次更新的步长$\eta$。参数$(v,\eta)$和终止条件决定了我们的迭代优化算法。</p><p><img src="http://img.blog.csdn.net/20170516101506325?" alt="这里写图片描述"></p><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>根据上一小节PLA的思想，迭代优化让每次w都有更新：</p><p><img src="http://img.blog.csdn.net/20170516102731492?" alt="这里写图片描述"></p><p>我们把$E_{in}(w)$曲线看做是一个山谷的话，要求$E_{in}(w)$最小，即可比作下山的过程。整个下山过程由两个因素影响：一个是下山的单位方向$v$；另外一个是下山的步长$\eta$。</p><p><img src="http://img.blog.csdn.net/20170516103259068?" alt="这里写图片描述"></p><p>利用微分思想和线性近似，假设每次下山我们只前进一小步，即$\eta$很小，那么根据泰勒Taylor一阶展开，可以得到：<br>$$E_{in}(w_t+\eta v)\approx E_{in}(w_t)+\eta v^T\nabla E_{in}(w_t)$$</p><p>关于Taylor展开的介绍，可参考我另一篇博客：<br><a href="http://blog.csdn.net/red_stone1/article/details/70260070" target="_blank" rel="noopener">多元函数的泰勒(Taylor)展开式</a></p><p>迭代的目的是让$E_{in}$越来越小，即让$E_{in}(w_t+\eta v)&lt;E_{in}(w_t)$。$\eta$是标量，因为如果两个向量方向相反的话，那么他们的内积最小（为负），也就是说如果方向$v$与梯度$\nabla E_{in}(w_t)$反向的话，那么就能保证每次迭代$E_{in}(w_t+\eta v)&lt;E_{in}(w_t)$都成立。则，我们令下降方向$v$为：<br>$$v=-\frac{\nabla E_{in}(w_t)}{||\nabla E_{in}(w_t)||}$$</p><p>$v$是单位向量，$v$每次都是沿着梯度的反方向走，这种方法称为梯度下降（gradient descent）算法。那么每次迭代公式就可以写成：<br>$$w_{t+1}\leftarrow w_t-\eta\frac{\nabla E_{in}(w_t)}{||\nabla E_{in}(w_t)||}$$</p><p>下面讨论一下$\eta$的大小对迭代优化的影响：$\eta$如果太小的话，那么下降的速度就会很慢；$\eta$如果太大的话，那么之前利用Taylor展开的方法就不准了，造成下降很不稳定，甚至会上升。因此，$\eta$应该选择合适的值，一种方法是在梯度较小的时候，选择小的$\eta$，梯度较大的时候，选择大的$\eta$，即$\eta$正比于$||\nabla E_{in}(w_t)||$。这样保证了能够快速、稳定地得到最小值$E_{in}(w)$。</p><p><img src="http://img.blog.csdn.net/20170516111145698?" alt="这里写图片描述"></p><p>对学习速率$\eta$做个更修正，梯度下降算法的迭代公式可以写成：<br>$$w_{t+1}\leftarrow w_t-\eta’\nabla E_{in}(w_t)$$<br>其中：<br>$$\eta’=\frac{\eta}{||\nabla E_{in}(w_t)||}$$</p><p>总结一下基于梯度下降的Logistic Regression算法步骤如下：</p><ul><li><strong>初始化$w_0$</strong></li><li><strong>计算梯度$\nabla E_{in}(w_t)=\frac1N\sum_{n=1}^N\theta(-y_nw_t^Tx_n)(-y_nx_n)$</strong></li><li><strong>迭代跟新$w_{t+1}\leftarrow w_t-\eta\nabla E_{in}(w_t)$</strong></li><li><strong>满足$\nabla E_{in}(w_{t+1})\approx0$或者达到迭代次数，迭代结束</strong></li></ul><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>我们今天介绍了Logistic Regression。首先，从逻辑回归的问题出发，将$P(+1|x)$作为目标函数，将$\theta(w^Tx)$作为hypothesis。接着，我们定义了logistic regression的err function，称之为cross-entropy error交叉熵误差。然后，我们计算logistic regression error的梯度，最后，通过梯度下降算法，计算$\nabla E_{in}(w_t)\approx0$时对应的$w_t$值。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170515225915831?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记9 -- Linear Regression</title>
    <link href="https://redstonewill.github.io/2018/03/17/9/"/>
    <id>https://redstonewill.github.io/2018/03/17/9/</id>
    <published>2018-03-17T06:39:42.000Z</published>
    <updated>2018-03-17T06:43:17.774Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170511000626691?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要介绍了在有noise的情况下，VC Bound理论仍然是成立的。同时，介绍了不同的error measure方法。本节课介绍机器学习最常见的一种算法：Linear Regression.</p><h3 id="线性回归问题"><a href="#线性回归问题" class="headerlink" title="线性回归问题"></a>线性回归问题</h3><p>在之前的Linear Classification课程中，讲了信用卡发放的例子，利用机器学习来决定是否给用户发放信用卡。本节课仍然引入信用卡的例子，来解决给用户发放信用卡额度的问题，这就是一个线性回归（Linear Regression）问题。</p><p><img src="http://img.blog.csdn.net/20170511000626691?" alt="这里写图片描述"></p><p>令用户特征集为d维的$X$，加上常数项，维度为$d+1$，与权重$w$的线性组合即为Hypothesis,记为$h(x)$。线性回归的预测函数取值在整个实数空间，这跟线性分类不同。</p><p>$$h(x)=w^TX$$</p><p><img src="http://img.blog.csdn.net/20170511091939131?" alt="这里写图片描述"></p><p>根据上图，在一维或者多维空间里，线性回归的目标是找到一条直线（对应一维）、一个平面（对应二维）或者更高维的超平面，使样本集中的点更接近它，也就是残留误差Residuals最小化。</p><p>一般最常用的错误测量方式是基于最小二乘法，其目标是计算误差的最小平方和对应的权重w，即上节课介绍的squared error：</p><p><img src="http://img.blog.csdn.net/20170511092613937?" alt="这里写图片描述"></p><p>这里提一点，最小二乘法可以解决线性问题和非线性问题。线性最小二乘法的解是closed-form，即$X=(A^TA)^{-1}A^Ty$，而非线性最小二乘法没有closed-form，通常用迭代法求解。本节课的解就是closed-form的。关于最小二乘法的一些介绍，请参见我的另一篇博文：</p><p><a href="http://blog.csdn.net/red_stone1/article/details/70306403" target="_blank" rel="noopener">最小二乘法和梯度下降法的一些总结</a></p><h3 id="线性回归算法"><a href="#线性回归算法" class="headerlink" title="线性回归算法"></a>线性回归算法</h3><p>样本数据误差$E_{in}$是权重$w$的函数，因为$X$和$y$都是已知的。我们的目标就是找出合适的$w$，使$E_{in}$能够最小。那么如何计算呢？</p><p>首先，运用矩阵转换的思想，将$E_{in}$计算转换为矩阵的形式。</p><p><img src="http://img.blog.csdn.net/20170511093750121?" alt="这里写图片描述"></p><p>然后，对于此类线性回归问题，$E_{in}(w)$一般是个凸函数。凸函数的话，我们只要找到一阶导数等于零的位置，就找到了最优解。那么，我们将$E_{w}$对每个$w_i,i=0,1,\cdots,d$求偏导，偏导为零的$w_i$，即为最优化的权重值分布。</p><p><img src="http://img.blog.csdn.net/20170511094302883?" alt="这里写图片描述"></p><p>根据梯度的思想，对$E_{w}$进行矩阵话求偏导处理：</p><p><img src="http://img.blog.csdn.net/20170511094548562?" alt="这里写图片描述"></p><p>令偏导为零，最终可以计算出权重向量$w$为：</p><p><img src="http://img.blog.csdn.net/20170511094724296?" alt="这里写图片描述"></p><p>最终，我们推导得到了权重向量$w=(X^TX)^{-1}X^Ty$，这是上文提到的closed-form解。其中，$(X^TX)^{-1}X^T$又称为伪逆矩阵pseudo-inverse，记为$X^+$，维度是(d+1)xN。</p><p>但是，我们注意到，伪逆矩阵中有逆矩阵的计算，逆矩阵$(X^TX)^{-1}$是否一定存在？一般情况下，只要满足样本数量N远大于样本特征维度d+1，就能保证矩阵的逆是存在的，称之为非奇异矩阵。但是如果是奇异矩阵，不可逆怎么办呢？其实，大部分的计算逆矩阵的软件程序，都可以处理这个问题，也会计算出一个逆矩阵。所以，一般伪逆矩阵是可解的。</p><h3 id="泛化问题"><a href="#泛化问题" class="headerlink" title="泛化问题"></a>泛化问题</h3><p>现在，可能有这样一个疑问，就是这种求解权重向量的方法是机器学习吗？或者说这种方法满足我们之前推导VC Bound，即是否泛化能力强$E_{in}\approx E_{out}$？</p><p><img src="http://img.blog.csdn.net/20170511101558353?" alt="这里写图片描述"></p><p>有两种观点：1、这不属于机器学习范畴。因为这种closed-form解的形式跟一般的机器学习算法不一样，而且在计算最小化误差的过程中没有用到迭代。2、这属于机器学习范畴。因为从结果上看，$E_{in}$和$E_{out}$都实现了最小化，而且实际上在计算逆矩阵的过程中，也用到了迭代。</p><p>其实，只从结果来看，这种方法的确实现了机器学习的目的。下面通过介绍一种更简单的方法，证明linear regression问题是可以通过线下最小二乘法方法计算得到好的$E_{in}$和$E_{out}$的。</p><p><img src="http://img.blog.csdn.net/20170511103154804?" alt="这里写图片描述"></p><p>首先，我们根据平均误差的思想，把$E_{in}(w_{LIN})$写成如图的形式，经过变换得到:<br>$$E_{in}(w_{LIN})=\frac1N||(I-XX^+)y||^2=\frac1N||(I-H)y||^2$$</p><p>我们称$XX^+$为帽子矩阵，用H表示。</p><p>下面从几何图形的角度来介绍帽子矩阵H的物理意义。</p><p><img src="http://img.blog.csdn.net/20170511103912793?" alt="这里写图片描述"></p><p>图中，y是N维空间的一个向量，粉色区域表示输入矩阵X乘以不同权值向量w所构成的空间，根据所有w的取值，预测输出都被限定在粉色的空间中。向量$\hat y$就是粉色空间中的一个向量，代表预测的一种。y是实际样本数据输出值。</p><p>机器学习的目的是在粉色空间中找到一个$\hat y$，使它最接近真实的y，那么我们只要将y在粉色空间上作垂直投影即可，投影得到的$\hat y$即为在粉色空间内最接近y的向量。这样即使平均误差$\overline E$最小。</p><p>从图中可以看出，$\hat y$是y的投影，已知$\hat y=Hy$，那么H表示的就是将y投影到$\hat y$的一种操作。图中绿色的箭头$y-\hat y$是向量y与$\hat y$相减，$y-\hat y$垂直于粉色区域。已知$(I-H)y=y-\hat y$那么I-H表示的就是将y投影到$y-\hat y$即垂直于粉色区域的一种操作。这样的话，我们就赋予了H和I-H不同但又有联系的物理意义。</p><p>这里trace(I-H)称为I-H的迹，值为N-(d+1)。这条性质很重要，一个矩阵的 trace等于该矩阵的所有特征值(Eigenvalues)之和。下面给出简单证明：</p><p>$trace(I-H)=trace(I)-trace(H)$<br>$=N-trace(XX^+)=N-trace(X(X^TX)^{-1}X^T$<br>$=N-trace(X^TX(X^TX)^{-1})=N-trace(I_{d+1})$<br>$=N-(d+1)$</p><p>介绍下该I-H这种转换的物理意义：原来有一个有N个自由度的向量y，投影到一个有d+1维的空间x（代表一列的自由度，即单一输入样本的参数，如图中粉色区域），而余数剩余的自由度最大只有N-(d+1)种。</p><p>在存在noise的情况下，上图变为：</p><p><img src="http://img.blog.csdn.net/20170511110854010?" alt="这里写图片描述"></p><p>图中，粉色空间的红色箭头是目标函数f(x)，虚线箭头是noise，可见，真实样本输出y由f(x)和noise相加得到。由上面推导，已知向量y经过I-H转换为$y-\hat y$，而noise与y是线性变换关系，那么根据线性函数知识，我们推导出noise经过I-H也能转换为$y-\hat y$。则对于样本平均误差，有下列推导成立：</p><p>$$E_{in}(w_{LIN})=\frac1N||y-\hat y||^2=\frac1N||(I-H)noise||^2=\frac1N(N-(d+1))||noise||^2$$</p><p>即</p><p>$$\overline E_{in}=noise level\ast (1-\frac{d+1}N) $$</p><p>同样，对$E_{out}$有如下结论：</p><p>$$\overline E_{out}=noise level\ast (1+\frac{d+1}N) $$</p><p>这个证明有点复杂，但是我们可以这样理解：$\overline E_{in}$与$\overline E_{out}$形式上只差了$\frac{(d+1)}N$项，从哲学上来说，$\overline E_{in}$是我们看得到的样本的平均误差，如果有noise，我们把预测往noise那边偏一点，让$\overline E_{in}$好看一点点，所以减去$\frac{(d+1)}N$项。那么同时，新的样本$\overline E_{out}$是我们看不到的，如果noise在反方向，那么$\overline E_{out}$就应该加上$\frac{(d+1)}N$项。</p><p>我们把$\overline E_{in}$与$\overline E_{out}$画出来，得到学习曲线：</p><p><img src="http://img.blog.csdn.net/20170511133854709?" alt="这里写图片描述"></p><p>当N足够大时，$\overline E_{in}$与$\overline E_{out}$逐渐接近，满足$\overline E_{in}\approx \overline E_{out}$，且数值保持在noise level。这就类似VC理论，证明了当N足够大的时候，这种线性最小二乘法是可以进行机器学习的，算法有效！</p><h3 id="Linear-Regression方法解决Linear-Classification问题"><a href="#Linear-Regression方法解决Linear-Classification问题" class="headerlink" title="Linear Regression方法解决Linear Classification问题"></a>Linear Regression方法解决Linear Classification问题</h3><p>之前介绍的Linear Classification问题使用的Error Measure方法用的是0/1 error，那么Linear Regression的squared error是否能够应用到Linear Classification问题？</p><p><img src="http://img.blog.csdn.net/20170511134801850?" alt="这里写图片描述"></p><p>下图展示了两种错误的关系，一般情况下，squared error曲线在0/1 error曲线之上。即$err_{0/1}\leq err_{sqr}$.</p><p><img src="http://img.blog.csdn.net/20170511135243106?" alt="这里写图片描述"></p><p>根据之前的VC理论，$E_{out}$的上界满足：</p><p><img src="http://img.blog.csdn.net/20170511135656953?" alt="这里写图片描述"></p><p>从图中可以看出，用$err_{sqr}$代替$err_{0/1}$，$E_{out}$仍然有上界，只不过是上界变得宽松了。也就是说用线性回归方法仍然可以解决线性分类问题，效果不会太差。二元分类问题得到了一个更宽松的上界，但是也是一种更有效率的求解方式。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本节课，我们主要介绍了Linear Regression。首先，我们从问题出发，想要找到一条直线拟合实际数据值；然后，我们利用最小二乘法，用解析形式推导了权重w的closed-form解；接着，用图形的形式得到$E_{out}-E_{in}\approx \frac{2(N+1)}{N}$，证明了linear regression是可以进行机器学习的，；最后，我们证明linear regressin这种方法可以用在binary classification上，虽然上界变宽松了，但是仍然能得到不错的学习方法。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170511000626691?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记8 -- Noise and Error</title>
    <link href="https://redstonewill.github.io/2018/03/17/8/"/>
    <id>https://redstonewill.github.io/2018/03/17/8/</id>
    <published>2018-03-17T06:37:13.000Z</published>
    <updated>2018-03-17T06:39:03.797Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170509214259658?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们主要介绍了VC Dimension的概念。如果Hypotheses set的VC Dimension是有限的，且有足够多N的资料，同时能够找到一个hypothesis使它的$E_{in}\approx 0$，那么就能说明机器学习是可行的。本节课主要讲了数据集有Noise的情况下，是否能够进行机器学习，并且介绍了假设空间H下演算法A的Error估计。</p><h3 id="Noise-and-Probablistic-target"><a href="#Noise-and-Probablistic-target" class="headerlink" title="Noise and Probablistic target"></a>Noise and Probablistic target</h3><p>上节课推导VC Dimension的数据集是在没有Noise的情况下，本节课讨论如果数据集本身存在Noise，那VC Dimension的推导是否还成立呢？</p><p>首先，Data Sets的Noise一般有三种情况：</p><ul><li><p><strong>由于人为因素，正类被误分为负类，或者负类被误分为正类；</strong></p></li><li><p><strong>同样特征的样本被模型分为不同的类；</strong></p></li><li><p><strong>样本的特征被错误记录和使用。</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170509214259658?" alt="这里写图片描述"></p><p>之前的数据集是确定的，即没有Noise的，我们称之为Deterministic。现在有Noise了，也就是说在某点处不再是确定分布，而是概率分布了，即对每个(x，y)出现的概率是$P(y|x)$。</p><p>因为Noise的存在，比如在x点，有0.7的概率y=1，有0.3的概率y=0，即y是按照$P(y|x)$分布的。数学上可以证明如果数据集按照$P(y|x)$概率分布且是iid的，那么以前证明机器可以学习的方法依然奏效，VC Dimension有限即可推断$E{in}$和$E{out}$是近似的。</p><p><img src="http://img.blog.csdn.net/20170509234714781?" alt="这里写图片描述"></p><p>$P(y|x)$称之为目标分布（Target Distribution）。它实际上告诉我们最好的选择是什么，同时伴随着多少noise。其实，没有noise的数据仍然可以看成“特殊”的$P(y|x)$概率分布，即概率仅是1和0.对于以前确定的数据集：<br>$$P(y|x)=1,for \space y=f(x)$$<br>$$P(y|x)=0,for \space y\neq f(x)$$</p><p><img src="http://img.blog.csdn.net/20170509234729403?" alt="这里写图片描述"></p><p>在引入noise的情况下，新的学习流程图如下所示：</p><p><img src="http://img.blog.csdn.net/20170616080950726?" alt="这里写图片描述"></p><h3 id="ERROR-Measure"><a href="#ERROR-Measure" class="headerlink" title="ERROR Measure"></a>ERROR Measure</h3><p>机器学习需要考虑的问题是找出的矩g与目标函数f有多相近，我们一直使用$E_{out}$进行误差的估计，那一般的错误测量有哪些形式呢？</p><p>我们介绍的矩g对错误的衡量有三个特性：</p><ul><li><p><strong>out-of-sample：样本外的未知数据</strong></p></li><li><p><strong>pointwise：对每个数据点x进行测试</strong></p></li><li><p><strong>classification：看prediction与target是否一致，classification error通常称为0/1 error</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170509235041811?" alt="这里写图片描述"></p><p>PointWise error实际上就是对数据集的每个点计算错误并计算平均，$E{in}$和$E{out}$的pointwise error的表达式为：</p><p><img src="http://img.blog.csdn.net/20170616082857078?" alt="这里写图片描述"></p><p>pointwise error是机器学习中最常用也是最简单的一种错误衡量方式，未来课程中，我们主要考虑这种方式。pointwise error一般可以分成两类：0/1 error和squared error。0/1 error通常用在分类（classification）问题上，而squared error通常用在回归（regression）问题上。</p><p><img src="http://img.blog.csdn.net/20170616083449361?" alt="这里写图片描述"></p><p>Ideal Mini-Target由$P(y|x)$和err共同决定，0/1 error和squared error的Ideal Mini-Target计算方法不一样。例如下面这个例子，分别用0/1 error和squared error来估计最理想的mini-target是多少。0/1 error中的mini-target是取P(y|x)最大的那个类，而squared error中的mini-target是取所有类的加权平方和。</p><p><img src="http://img.blog.csdn.net/20170509235617200?" alt="这里写图片描述"></p><p>有了错误衡量，就会知道当前的矩g是好还是不好，并会让演算法不断修正，得到更好的矩g，从而使得g与目标函数更接近。所以，引入error measure后，学习流程图如下所示：</p><p><img src="http://img.blog.csdn.net/20170616084802840?" alt="这里写图片描述"></p><h3 id="Algorithmic-Error-Measure"><a href="#Algorithmic-Error-Measure" class="headerlink" title="Algorithmic Error Measure"></a>Algorithmic Error Measure</h3><p>Error有两种：false accept和false reject。false accept意思是误把负类当成正类，false reject是误把正类当成负类。 根据不同的机器学习问题，false accept和false reject应该有不同的权重，这根实际情况是符合的，比如是超市优惠，那么false reject应该设的大一些；如果是安保系统，那么false accept应该设的大一些。</p><p><img src="http://img.blog.csdn.net/20170510000042846?" alt="这里写图片描述"></p><p>机器学习演算法A的cost function error估计有多种方法，真实的err一般难以计算，常用的方法可以采用plausible或者friendly，根据具体情况而定。</p><p><img src="http://img.blog.csdn.net/20170510000434925?" alt="这里写图片描述"></p><p>引入algorithm error measure之后，学习流程图如下：</p><p><img src="http://img.blog.csdn.net/20170616093909660?" alt="这里写图片描述"></p><h3 id="Weighted-Classification"><a href="#Weighted-Classification" class="headerlink" title="Weighted Classification"></a>Weighted Classification</h3><p>实际上，机器学习的Cost Function即来自于这些error，也就是算法里面的迭代的目标函数，通过优化使得Error（Ein）不断变小。<br>cost function中，false accept和false reject赋予不同的权重，在演算法中体现。对不同权重的错误惩罚，可以选用virtual copying的方法。</p><p><img src="http://img.blog.csdn.net/20170510001149489?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170510001245521?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要讲了在有Noise的情况下，即数据集按照$P(y|x)$概率分布，那么VC Dimension仍然成立，机器学习算法推导仍然有效。机器学习cost function常用的Error有0/1 error和squared error两类。实际问题中，对false accept和false reject应该选择不同的权重。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170509214259658?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记7 -- The VC Dimension</title>
    <link href="https://redstonewill.github.io/2018/03/17/7/"/>
    <id>https://redstonewill.github.io/2018/03/17/7/</id>
    <published>2018-03-17T06:33:34.000Z</published>
    <updated>2018-03-17T06:36:12.540Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170505103811400?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>前几节课着重介绍了机器能够学习的条件并做了详细的推导和解释。机器能够学习必须满足两个条件：</p><ul><li><strong>假设空间H的Size M是有限的，即当N足够大的时候，那么对于假设空间中任意一个假设g，$E_{out}\approx E_{in}$</strong>。</li><li><strong>利用算法A从假设空间H中，挑选一个g，使$E_{in}(g)\approx0$，则$E_{out}\approx0$</strong>。</li></ul><p>这两个条件，正好对应着test和trian两个过程。train的目的是使损失期望$E_{in}(g)\approx0$；test的目的是使将算法用到新的样本时的损失期望也尽可能小，即$E_{out}\approx0$。</p><p>正因为如此，上次课引入了break point，并推导出只要break point存在，则M有上界，一定存在$E_{out}\approx E_{in}$。</p><p>本次笔记主要介绍VC Dimension的概念。同时也是总结VC Dimension与$E_{in}(g)\approx0$，$E_{out}\approx0$，Model Complexity Penalty（下面会讲到）的关系。</p><h3 id="Definition-of-VC-Dimension"><a href="#Definition-of-VC-Dimension" class="headerlink" title="Definition of VC Dimension"></a>Definition of VC Dimension</h3><p>首先，我们知道如果一个假设空间H有break point k，那么它的成长函数是有界的，它的上界称为Bound function。根据数学归纳法，Bound function也是有界的，且上界为$N^{k-1}$。从下面的表格可以看出，$N(k-1)$比B(N,k)松弛很多。</p><p><img src="http://img.blog.csdn.net/20170505103811400?" alt="这里写图片描述"></p><p>则根据上一节课的推导，VC bound就可以转换为：</p><p><img src="http://img.blog.csdn.net/20170505104038982?" alt="这里写图片描述"></p><p>这样，不等式只与k和N相关了，一般情况下样本N足够大，所以我们只考虑k值。有如下结论：</p><ul><li><p><strong>若假设空间H有break point k，且N足够大，则根据VC bound理论，算法有良好的泛化能力</strong></p></li><li><p><strong>在假设空间中选择一个矩g，使$E_{in}\approx0$，则其在全集数据中的错误率会较低</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170615075948147?" alt="这里写图片描述"></p><p>下面介绍一个新的名词：VC Dimension。VC Dimension就是某假设集H能够shatter的最多inputs的个数，即最大完全正确的分类能力。（注意，只要存在一种分布的inputs能够正确分类也满足）。</p><p>shatter的英文意思是“粉碎”，也就是说对于inputs的所有情况都能列举出来。例如对N个输入，如果能够将$2^N$种情况都列出来，则称该N个输入能够被假设集H shatter。</p><p>根据之前break point的定义：假设集不能被shatter任何分布类型的inputs的最少个数。则VC Dimension等于break point的个数减一。</p><p><img src="http://img.blog.csdn.net/20170505110608511?" alt="这里写图片描述"></p><p>现在，我们回顾一下之前介绍的四种例子，它们对应的VC Dimension是多少：</p><p><img src="http://img.blog.csdn.net/20170615081035163?" alt="这里写图片描述"></p><p>用$d_{vc}$代替k，那么VC bound的问题也就转换为与$d_{vc}$和N相关了。同时，如果一个假设集H的$d_{vc}$确定了，则就能满足机器能够学习的第一个条件$E_{out}\approx E_{in}$，与算法、样本数据分布和目标函数都没有关系。</p><p><img src="http://img.blog.csdn.net/20170505111118560?" alt="这里写图片描述"></p><h3 id="VC-Dimension-of-Perceptrons"><a href="#VC-Dimension-of-Perceptrons" class="headerlink" title="VC Dimension of Perceptrons"></a>VC Dimension of Perceptrons</h3><p>回顾一下我们之前介绍的2D下的PLA算法，已知Perceptrons的k=4，即$d_{vc}=3$。根据VC Bound理论，当N足够大的时候，$E_{out}(g)\approx E_{in}(g)$。如果找到一个g，使$E_{in}(g)\approx 0$，那么就能证明PLA是可以学习的。</p><p><img src="http://img.blog.csdn.net/20170615081955144?" alt="这里写图片描述"></p><p>这是在2D情况下，那如果是多维的Perceptron，它对应的$d_{vc}$又等于多少呢？</p><p>已知在1D Perceptron，$d_{vc}=2$，在2D Perceptrons，$d_{vc}=3$，那么我们有如下假设：$d_{vc}=d+1$，其中d为维数。</p><p>要证明的话，只需分两步证明：</p><ul><li>$d_{vc}\geq d+1$</li><li>$d_{vc}\leq d+1$</li></ul><p><img src="http://img.blog.csdn.net/20170615082410013?" alt="这里写图片描述"></p><p>首先证明第一个不等式：$d_{vc}\geq d+1$。</p><p>在d维里，我们只要找到某一类的d+1个inputs可以被shatter的话，那么必然得到$d_{vc}\geq d+1$。所以，我们有意构造一个d维的矩阵$X$能够被shatter就行。$X$是d维的，有d+1个inputs，每个inputs加上第零个维度的常数项1，得到$X$的矩阵：</p><p><img src="http://img.blog.csdn.net/20170615084743432?" alt="这里写图片描述"></p><p>矩阵中，每一行代表一个inputs，每个inputs是d+1维的，共有d+1个inputs。这里构造的$X$很明显是可逆的。shatter的本质是假设空间H对$X$的所有情况的判断都是对的，即总能找到权重W，满足$X\ast W=y$，$W=X^{-1}\ast y$。由于这里我们构造的矩阵$X$的逆矩阵存在，那么d维的所有inputs都能被shatter，也就证明了第一个不等式。</p><p><img src="http://img.blog.csdn.net/20170615085447521?" alt="这里写图片描述"></p><p>然后证明第二个不等式：$d_{vc}\leq d+1$。</p><p>在d维里，如果对于任何的d+2个inputs，一定不能被shatter，则不等式成立。我们构造一个任意的矩阵$X$，其包含d+2个inputs，该矩阵有d+1列，d+2行。这d+2个向量的某一列一定可以被另外d+1个向量线性表示，例如对于向量$X_{d+2}$，可表示为：<br>$$X_{d+2}=a_1\ast X_1+a_2\ast X_2+\cdots+a_{d+1}\ast X_{d+1}$$</p><p>其中，假设$a_1&gt;0$，$a_2,\cdots,a_{d+1}&lt;0$.</p><p>那么如果$X_1$是正类，$X_2,\cdots,X_{d+1}$均为负类，则存在$W$，得到如下表达式：<br>$X_{d+2}\ast W=$<font color="#0000ff">$a_1\ast X_1\ast W$</font>+<font color="#ff0000">$a_2\ast X_2\ast W$</font>+$\cdots$+<font color="#ff0000">$a_{d+1}\ast X_{d+1}\ast W$</font>$&gt;0$</p><p>因为其中蓝色项大于0，代表正类；红色项小于0，代表负类。所有对于这种情况，$X_d+2$一定是正类，无法得到负类的情况。也就是说，d+2个inputs无法被shatter。证明完毕！</p><p><img src="http://img.blog.csdn.net/20170505135705345?" alt="这里写图片描述"></p><p>综上证明可得$d_{vc}=d+1$。</p><h3 id="Physical-Intuition-VC-Dimension"><a href="#Physical-Intuition-VC-Dimension" class="headerlink" title="Physical Intuition VC Dimension"></a>Physical Intuition VC Dimension</h3><p><img src="http://img.blog.csdn.net/20170505140028066?" alt="这里写图片描述"></p><p>上节公式中$W$又名features，即自由度。自由度是可以任意调节的，如同上图中的旋钮一样，可以调节。VC Dimension代表了假设空间的分类能力，即反映了H的自由度，产生dichotomy的数量，也就等于features的个数，但也不是绝对的。</p><p><img src="http://img.blog.csdn.net/20170505140713568?" alt="这里写图片描述"></p><p>例如，对2D Perceptrons，线性分类，$d_{vc}=3$，则$W={w_0,w_1,w_2}$，也就是说只要3个features就可以进行学习，自由度为3。</p><p>介绍到这，我们发现M与$d_{vc}$是成正比的，从而得到如下结论：</p><p><img src="http://img.blog.csdn.net/20170505141450682?" alt="这里写图片描述"></p><h3 id="Interpreting-VC-Dimension"><a href="#Interpreting-VC-Dimension" class="headerlink" title="Interpreting VC Dimension"></a>Interpreting VC Dimension</h3><p>下面，我们将更深入地探讨VC Dimension的意义。首先，把VC Bound重新写到这里：</p><p><img src="http://img.blog.csdn.net/20170505141928121?" alt="这里写图片描述"></p><p>根据之前的泛化不等式，如果$|E_{in}-E_{out}|&gt;\epsilon$，即出现bad坏的情况的概率最大不超过$\delta$。那么反过来，对于good好的情况发生的概率最小为$1-\delta$，则对上述不等式进行重新推导：</p><p><img src="http://img.blog.csdn.net/20170505142454586?" alt="这里写图片描述"></p><p>$\epsilon$表现了假设空间H的泛化能力，$\epsilon$越小，泛化能力越大。</p><p><img src="http://img.blog.csdn.net/20170505142745183?" alt="这里写图片描述"></p><p>至此，已经推导出泛化误差$E_{out}$的边界，因为我们更关心其上界（$E_{out}$可能的最大值），即：</p><p><img src="http://img.blog.csdn.net/20170505143029968?" alt="这里写图片描述"></p><p>上述不等式的右边第二项称为模型复杂度，其模型复杂度与样本数量N、假设空间H($d_{vc}$)、$\epsilon$有关。$E_{out}$由$E_{in}$共同决定。下面绘出$E_{out}$、model complexity、$E_{in}$随$d_{vc}$变化的关系：</p><p><img src="http://img.blog.csdn.net/20170505143707333?" alt="这里写图片描述"></p><p>通过该图可以得出如下结论：</p><ul><li><p><strong>$d_{vc}$越大，$E_{in}$越小，$\Omega$越大（复杂）</strong>。</p></li><li><p><strong>$d_{vc}$越小，$E_{in}$越大，$\Omega$越小（简单）</strong>。</p></li><li><p><strong>随着$d_{vc}$增大，$E_{out}$会先减小再增大</strong>。</p></li></ul><p>所以，为了得到最小的$E_{out}$，不能一味地增大$d_{vc}$以减小$E_{in}$，因为$E_{in}$太小的时候，模型复杂度会增加，造成$E_{out}$变大。也就是说，选择合适的$d_{vc}$，选择的features个数要合适。</p><p>下面介绍一个概念：样本复杂度（Sample Complexity）。如果选定$d_{vc}$，样本数据D选择多少合适呢？通过下面一个例子可以帮助我们理解：</p><p><img src="http://img.blog.csdn.net/20170505145501721?" alt="这里写图片描述"></p><p>通过计算得到N=29300，刚好满足$\delta=0.1$的条件。N大约是$d_{vc}$的10000倍。这个数值太大了，实际中往往不需要这么多的样本数量，大概只需要$d_{vc}$的10倍就够了。N的理论值之所以这么大是因为VC Bound 过于宽松了，我们得到的是一个比实际大得多的上界。</p><p><img src="http://img.blog.csdn.net/20170505145842865?" alt="这里写图片描述"></p><p>值得一提的是，VC Bound是比较宽松的，而如何收紧它却不是那么容易，这也是机器学习的一大难题。但是，令人欣慰的一点是，VC Bound基本上对所有模型的宽松程度是基本一致的，所以，不同模型之间还是可以横向比较。从而，VC Bound宽松对机器学习的可行性还是没有太大影响。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了VC Dimension的概念就是最大的non-break point。然后，我们得到了Perceptrons在d维度下的VC Dimension是d+1。接着，我们在物理意义上，将$d_{vc}$与自由度联系起来。最终得出结论$d_{vc}$不能过大也不能过小。选取合适的值，才能让$E_{out}$足够小，使假设空间H具有良好的泛化能力。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170505103811400?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记6 -- Theory of Generalization</title>
    <link href="https://redstonewill.github.io/2018/03/17/6/"/>
    <id>https://redstonewill.github.io/2018/03/17/6/</id>
    <published>2018-03-17T06:24:41.000Z</published>
    <updated>2018-03-17T06:30:32.483Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170614075730000?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上一节课，我们主要探讨了当M的数值大小对机器学习的影响。如果M很大，那么就不能保证机器学习有很好的泛化能力，所以问题转换为验证M有限，即最好是按照多项式成长。然后通过引入了成长函数$m_H(N)$和dichotomy以及break point的概念，提出2D perceptrons的成长函数$m_H(N)$是多项式级别的猜想。这就是本节课将要深入探讨和证明的内容。</p><h3 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h3><p>我们先回顾一下上节课的内容，四种成长函数与break point的关系：</p><p><img src="http://img.blog.csdn.net/20170614075730000?" alt="这里写图片描述"></p><p>下面引入一个例子，如果k=2，那么当N取不同值的时候，计算其成长函数$m_H(N)$是多少。很明显，当N=1时，$m_H(N)$=2,；当N=2时，由break point为2可知，任意两点都不能被shattered（shatter的意思是对N个点，能够分解为$2^N$种dichotomies）；$m_H(N)$最大值只能是3；当N=3时，简单绘图分析可得其$m_H(N)=4$，即最多只有4种dichotomies。</p><p><img src="http://img.blog.csdn.net/20170614081452868?" alt="这里写图片描述"></p><p>所以，我们发现当N&gt;k时，break point限制了$m_H(N)$值的大小，也就是说影响成长函数$m_H(N)$的因素主要有两个：</p><ul><li><p>抽样数据集N</p></li><li><p>break point k（这个变量确定了假设的类型）</p></li></ul><p>那么，如果给定N和k，能够证明其$m_H(N)$的最大值的上界是多项式的，则根据霍夫丁不等式，就能用$m_H(N)$代替M，得到机器学习是可行的。所以，证明$m_H(N)$的上界是poly(N)，是我们的目标。</p><p><img src="http://img.blog.csdn.net/20170614082443877?" alt="这里写图片描述"></p><h3 id="Bounding-Function-Basic-Cases"><a href="#Bounding-Function-Basic-Cases" class="headerlink" title="Bounding Function: Basic Cases"></a>Bounding Function: Basic Cases</h3><p>现在，我们引入一个新的函数：bounding function，B(N,k)。Bound Function指的是当break point为k的时候，成长函数$m_H(N)$可能的最大值。也就是说B(N,k)是$m_H(N)$的上界，对应$m_H(N)$最多有多少种dichotomy。那么，我们新的目标就是证明：</p><p>$$B(N,k)\leq poly(N)$$</p><p>这里值得一提的是，B(N,k)的引入不考虑是1D postive intrervals问题还是2D perceptrons问题，而只关心成长函数的上界是多少，从而简化了问题的复杂度。</p><p><img src="http://img.blog.csdn.net/20170614083709962?" alt="这里写图片描述"></p><p>求解B(N,k)的过程十分巧妙：</p><ul><li><p>当k=1时，B(N,1)恒为1。</p></li><li><p>当N &lt; k时，根据break point的定义，很容易得到$B(N,k)=2^N$。</p></li><li><p>当N = k时，此时N是第一次出现不能被shatter的值，所以最多只能有$2^N-1$个dichotomies，则$B(N,k)=2^N-1$。</p></li></ul><p><img src="http://img.blog.csdn.net/20170614085020015?" alt="这里写图片描述"></p><p>到此，bounding function的表格已经填了一半了，对于最常见的N&gt;k的情况比较复杂，推导过程下一小节再详细介绍。</p><h3 id="Bounding-Function-Inductive-Cases"><a href="#Bounding-Function-Inductive-Cases" class="headerlink" title="Bounding Function: Inductive Cases"></a>Bounding Function: Inductive Cases</h3><p>N &gt; k的情况较为复杂，下面给出推导过程：</p><p>以B(4,3)为例，首先想着能否构建B(4,3)与B(3,x)之间的关系。</p><p>首先，把B(4,3)所有情况写下来，共有11组。也就是说再加一种dichotomy，任意三点都能被shattered，11是极限。</p><p><img src="http://img.blog.csdn.net/20170614091230231?" alt="这里写图片描述"></p><p>对这11种dichotomy分组，目前分成两组，分别是orange和purple，orange的特点是，x1,x2和x3是一致的，x4不同并成对，例如1和5，2和8等，purple则是单一的，x1,x2,x3都不同，如6,7,9三组。</p><p><img src="http://img.blog.csdn.net/20170614091821543?" alt="这里写图片描述"></p><p>将Orange去掉x4后去重得到4个不同的vector并成为$\alpha$，相应的purple为$\beta$。那么$B(4,3) = 2\alpha + \beta$，这个是直接转化。紧接着，由定义，B(4,3)是不能允许任意三点shatter的，所以由$\alpha$和$\beta$构成的所有三点组合也不能shatter（alpha经过去重），即$\alpha + \beta\leq B(3,3)$。</p><p><img src="http://img.blog.csdn.net/20170614094602964?" alt="这里写图片描述"></p><p>另一方面，由于$\alpha$中x4是成对存在的，且$\alpha$是不能被任意三点shatter的，则能推导出$\alpha$是不能被任意两点shatter的。这是因为，如果$\alpha$是不能被任意两点shatter，而x4又是成对存在的，那么x1、x2、x3、x4组成的$\alpha$必然能被三个点shatter。这就违背了条件的设定。这个地方的推导非常巧妙，也解释了为什么会这样分组。此处得到的结论是$\alpha \leq B(3,2)$</p><p><img src="http://img.blog.csdn.net/20170614094707216?" alt="这里写图片描述"></p><p>由此得出B(4,3)与B(3,x)的关系为：</p><p><img src="http://img.blog.csdn.net/20170614094834154?" alt="这里写图片描述"></p><p>最后，推导出一般公式为：</p><p><img src="http://img.blog.csdn.net/20170614094924233?" alt="这里写图片描述"></p><p>根据推导公式，下表给出B(N,K)值</p><p><img src="http://img.blog.csdn.net/20170614095031483?" alt="这里写图片描述"></p><p>根据递推公式，推导出B(N,K)满足下列不等式：</p><p><img src="http://img.blog.csdn.net/20170614095335958?" alt="这里写图片描述"></p><p>上述不等式的右边是最高阶为k-1的N多项式，也就是说成长函数$m_H(N)$的上界B(N,K)的上界满足多项式分布poly(N)，这就是我们想要得到的结果。</p><p>得到了$m_H(N)$的上界B(N,K)的上界满足多项式分布poly(N)后，我们回过头来看看之前介绍的几种类型它们的$m_H(N)$与break point的关系：</p><p><img src="http://img.blog.csdn.net/20170614100013092?" alt="这里写图片描述"></p><p>我们得到的结论是，对于2D perceptrons，break point为k=4，$m_H(N)$的上界是$N^{k-1}$。推广一下，也就是说，如果能找到一个模型的break point，且是有限大的，那么就能推断出其成长函数$m_H(N)$有界。</p><h3 id="A-Pictorial-Proof"><a href="#A-Pictorial-Proof" class="headerlink" title="A Pictorial Proof"></a>A Pictorial Proof</h3><p>我们已经知道了成长函数的上界是poly(N)的，下一步，如果能将$m_H(N)$代替M，代入到Hoffding不等式中，就能得到$E_{out}\approx E_{in}$的结论：</p><p><img src="http://img.blog.csdn.net/20170614101927607?" alt="这里写图片描述"></p><p>实际上并不是简单的替换就可以了，正确的表达式为：</p><p><img src="http://img.blog.csdn.net/20170614102141749?" alt="这里写图片描述"></p><p>该推导的证明比较复杂，我们可以简单概括为三个步骤来证明：</p><p><img src="http://img.blog.csdn.net/20170614103551436?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170614103603584?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170614103615889?" alt="这里写图片描述"></p><p>这部分内容，我也只能听个大概内容，对具体的证明过程有兴趣的童鞋可以自行研究一下，研究的结果记得告诉一下我哦。</p><p>最终，我们通过引入成长函数$m_H$，得到了一个新的不等式，称为Vapnik-Chervonenkis(VC) bound：</p><p><img src="http://img.blog.csdn.net/20170614105404608?" alt="这里写图片描述"></p><p>对于2D perceptrons，它的break point是4，那么成长函数$m_H(N)=O(N^3)$。所以，我们可以说2D perceptrons是可以进行机器学习的，只要找到hypothesis能让$E_{in}\approx0$，就能保证$E_{in}\approx E_{out}$。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课我们主要介绍了只要存在break point，那么其成长函数$m_H(N)$就满足poly(N)。推导过程是先引入$m_H(N)$的上界B(N,k)，B(N,k)的上界是N的k-1阶多项式，从而得到$m_H(N)$的上界就是N的k-1阶多项式。然后，我们通过简单的三步证明，将$m_H(N)$代入了Hoffding不等式中，推导出了Vapnik-Chervonenkis(VC) bound，最终证明了只要break point存在，那么机器学习就是可行的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170614075730000?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记5 -- Training versus Testing</title>
    <link href="https://redstonewill.github.io/2018/03/16/5/"/>
    <id>https://redstonewill.github.io/2018/03/16/5/</id>
    <published>2018-03-16T14:45:12.000Z</published>
    <updated>2018-03-17T06:31:32.547Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170613075450559?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要介绍了机器学习的可行性。首先，由NFL定理可知，机器学习貌似是不可行的。但是，随后引入了统计学知识，如果样本数据足够大，且hypothesis个数有限，那么机器学习一般就是可行的。本节课将讨论机器学习的核心问题，严格证明为什么机器可以学习。从上节课最后的问题出发，即当hypothesis的个数是无限多的时候，机器学习的可行性是否仍然成立？</p><h3 id="Recap-and-Preview"><a href="#Recap-and-Preview" class="headerlink" title="Recap and Preview"></a>Recap and Preview</h3><p>我们先来看一下基于统计学的机器学习流程图：</p><p><img src="http://img.blog.csdn.net/20170613075450559?" alt="这里写图片描述"></p><p>该流程图中，训练样本D和最终测试h的样本都是来自同一个数据分布，这是机器能够学习的前提。另外，训练样本D应该足够大，且hypothesis set的个数是有限的，这样根据霍夫丁不等式，才不会出现Bad Data，保证$E_{in}\approx E_{out}$，即有很好的泛化能力。同时，通过训练，得到使$E_{in}$最小的h，作为模型最终的矩g，g接近于目标函数。</p><p>这里，我们总结一下前四节课的主要内容：第一节课，我们介绍了机器学习的定义，目标是找出最好的矩g，使$g\approx f$，保证$E_{out}(g)\approx 0$；第二节课，我们介绍了如何让$E_{in}\approx 0$，可以使用PLA、pocket等演算法来实现；第三节课，我们介绍了机器学习的分类，我们的训练样本是批量数据（batch），处理监督式（supervised）二元分类（binary classification）问题；第四节课，我们介绍了机器学习的可行性，通过统计学知识，把$E_{in}(g)$与$E_{out}(g)$联系起来，证明了在一些条件假设下，$E_{in}(g)\approx E_{out}(g)$成立。</p><p><img src="http://img.blog.csdn.net/20170613081415084?" alt="这里写图片描述"></p><p>这四节课总结下来，我们把机器学习的主要目标分成两个核心的问题：</p><ul><li><p>$E_{in}(g)\approx E_{out}(g)$</p></li><li><p>$E_{in}(g)$足够小</p></li></ul><p>上节课介绍的机器学习可行的一个条件是hypothesis set的个数M是有限的，那M跟上面这两个核心问题有什么联系呢？</p><p>我们先来看一下，当M很小的时候，由上节课介绍的霍夫丁不等式，得到$E_{in}(g)\approx E_{out}(g)$，即能保证第一个核心问题成立。但M很小时，演算法A可以选择的hypothesis有限，不一定能找到使$E_{in}(g)$足够小的hypothesis，即不能保证第二个核心问题成立。当M很大的时候，同样由霍夫丁不等式，$E_{in}(g)$与$E_{out}(g)$的差距可能比较大，第一个核心问题可能不成立。而M很大，使的演算法A的可以选择的hypothesis就很多，很有可能找到一个hypothesis，使$E_{in}(g)$足够小，第二个核心问题可能成立。</p><p><img src="http://img.blog.csdn.net/20170613083237643?" alt="这里写图片描述"></p><p>从上面的分析来看，M的选择直接影响机器学习两个核心问题是否满足，M不能太大也不能太小。那么如果M无限大的时候，是否机器就不可以学习了呢？例如PLA算法中直线是无数条的，但是PLA能够很好地进行机器学习，这又是为什么呢？如果我们能将无限大的M限定在一个有限的$m_H$内，问题似乎就解决了。</p><h3 id="Effective-Number-of-Line"><a href="#Effective-Number-of-Line" class="headerlink" title="Effective Number of Line"></a>Effective Number of Line</h3><p>我们先看一下上节课推导的霍夫丁不等式：</p><p>$$P[|E_{in}(g)-E_{out}(g)|&gt;\epsilon]\leq 2\cdot M\cdot exp(-2\epsilon^2N)$$</p><p>其中，M表示hypothesis的个数。每个hypothesis下的BAD events $B_m$级联的形式满足下列不等式：</p><p>$$P[B_1\ or\ B_2\ or\ \cdots B_M]\leq P[B_1]+P[B_2]+\cdots+P[B_M]$$</p><p>当$M=\infty$时，上面不等式右边值将会很大，似乎说明BAD events很大，$E_{in}(g)$与$E_{out}(g)$也并不接近。但是BAD events $B_m$级联的形式实际上是扩大了上界，union bound过大。这种做法假设各个hypothesis之间没有交集，这是最坏的情况，可是实际上往往不是如此，很多情况下，都是有交集的，也就是说M实际上没那么大，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170613092500157?" alt="这里写图片描述"></p><p>也就是说union bound被估计过高了（over-estimating）。所以，我们的目的是找出不同BAD events之间的重叠部分，也就是将无数个hypothesis分成有限个类别。</p><p>如何将无数个hypothesis分成有限类呢？我们先来看这样一个例子，假如平面上用直线将点分开，也就跟PLA一样。如果平面上只有一个点x1，那么直线的种类有两种：一种将x1划为+1，一种将x1划为-1：</p><p><img src="http://img.blog.csdn.net/20170613095709542?" alt="这里写图片描述"></p><p>如果平面上有两个点x1、x2，那么直线的种类共4种：x1、x2都为+1，x1、x2都为-1，x1为+1且x2为-1，x1为-1且x2为+1：</p><p><img src="http://img.blog.csdn.net/20170613103010274?" alt="这里写图片描述"></p><p>如果平面上有三个点x1、x2、x3，那么直线的种类共8种：</p><p><img src="http://img.blog.csdn.net/20170613103032665?" alt="这里写图片描述"></p><p>但是，在三个点的情况下，也会出现不能用一条直线划分的情况：</p><p><img src="http://img.blog.csdn.net/20170613102051356?" alt="这里写图片描述"></p><p>也就是说，对于平面上三个点，不能保证所有的8个类别都能被一条直线划分。那如果是四个点x1、x2、x3、x4，我们发现，平面上找不到一条直线能将四个点组成的16个类别完全分开，最多只能分开其中的14类，即直线最多只有14种：</p><p><img src="http://img.blog.csdn.net/20170613102539221?" alt="这里写图片描述"></p><p>经过分析，我们得到平面上线的种类是有限的，1个点最多有2种线，2个点最多有4种线，3个点最多有8种线，4个点最多有14（$&lt;2^4$）种线等等。我们发现，有效直线的数量总是满足$\leq 2^N$，其中，N是点的个数。所以，如果我们可以用effective(N)代替M，霍夫丁不等式可以写成：</p><p>$$P[|E_{in}(g)-E_{out}(g)|&gt;\epsilon]\leq 2\cdot effective(N)\cdot exp(-2\epsilon^2N)$$</p><p>已知effective(N)&lt;$2^N$，如果能够保证effective(N)&lt;&lt;$2^N$，即不等式右边接近于零，那么即使M无限大，直线的种类也很有限，机器学习也是可能的。</p><p><img src="http://img.blog.csdn.net/20170613110232379?" alt="这里写图片描述"></p><h3 id="Effective-Number-of-Hypotheses"><a href="#Effective-Number-of-Hypotheses" class="headerlink" title="Effective Number of Hypotheses"></a>Effective Number of Hypotheses</h3><p>接下来先介绍一个新名词：二分类（dichotomy）。dichotomy就是将空间中的点（例如二维平面）用一条直线分成正类（蓝色o）和负类（红色x）。令H是将平面上的点用直线分开的所有hypothesis h的集合，dichotomy H与hypotheses H的关系是：hypotheses H是平面上所有直线的集合，个数可能是无限个，而dichotomy H是平面上能将点完全用直线分开的直线种类，它的上界是$2^N$。接下来，我们要做的就是尝试用dichotomy代替M。</p><p><img src="http://img.blog.csdn.net/20170613112843268?" alt="这里写图片描述"></p><p>再介绍一个新的名词：成长函数（growth function），记为$m_H(H)$。成长函数的定义是：对于由N个点组成的不同集合中，某集合对应的dichotomy最大，那么这个dichotomy值就是$m_H(H)$，它的上界是$2^N$：</p><p><img src="http://img.blog.csdn.net/20170613113650318?" alt="这里写图片描述"></p><p>成长函数其实就是我们之前讲的effective lines的数量最大值。根据成长函数的定义，二维平面上，$m_H(H)$随N的变化关系是：</p><p><img src="http://img.blog.csdn.net/20170613113930381?" alt="这里写图片描述"></p><p>接下来，我们讨论如何计算成长函数。先看一个简单情况，一维的Positive Rays：</p><p><img src="http://img.blog.csdn.net/20170613115111665?" alt="这里写图片描述"></p><p>若有N个点，则整个区域可分为N+1段，很容易得到其成长函数$m_H(N)=N+1$。注意当N很大时，$(N+1)&lt;&lt;2^N$，这是我们希望看到的。</p><p>另一种情况是一维的Positive Intervals：</p><p><img src="http://img.blog.csdn.net/20170613133644825?" alt="这里写图片描述"></p><p>它的成长函数可以由下面推导得出：</p><p><img src="http://img.blog.csdn.net/20170613134246055?" alt="这里写图片描述"></p><p>这种情况下，$m_H(N)=\frac12N^2+\frac12N+1&lt;&lt;2^N$，在N很大的时候，仍然是满足的。</p><p>再来看这个例子，假设在二维空间里，如果hypothesis是凸多边形或类圆构成的封闭曲线，如下图所示，左边是convex的，右边不是convex的。那么，它的成长函数是多少呢？</p><p><img src="http://img.blog.csdn.net/20170613135200251?" alt="这里写图片描述"></p><p>当数据集D按照如下的凸分布时，我们很容易计算得到它的成长函数$m_H=2^N$。这种情况下，N个点所有可能的分类情况都能够被hypotheses set覆盖，我们把这种情形称为shattered。也就是说，如果能够找到一个数据分布集，hypotheses set对N个输入所有的分类情况都做得到，那么它的成长函数就是$2^N$。</p><p><img src="http://img.blog.csdn.net/20170613135918235?" alt="这里写图片描述"></p><h3 id="Break-Point"><a href="#Break-Point" class="headerlink" title="Break Point"></a>Break Point</h3><p>上一小节，我们介绍了四种不同的成长函数，分别是：</p><p><img src="http://img.blog.csdn.net/20170613140725974?" alt="这里写图片描述"></p><p>其中，positive rays和positive intervals的成长函数都是polynomial的，如果用$m_H$代替M的话，这两种情况是比较好的。而convex sets的成长函数是exponential的，即等于M，并不能保证机器学习的可行性。那么，对于2D perceptrons，它的成长函数究竟是polynomial的还是exponential的呢？</p><p>对于2D perceptrons，我们之前分析了3个点，可以做出8种所有的dichotomy，而4个点，就无法做出所有16个点的dichotomy了。所以，我们就把4称为2D perceptrons的break point（5、6、7等都是break point）。令有k个点，如果k大于等于break point时，它的成长函数一定小于2的k次方。</p><p>根据break point的定义，我们知道满足$m_H(k)\neq 2^k$的k的最小值就是break point。对于我们之前介绍的四种成长函数，他们的break point分别是：</p><p><img src="http://img.blog.csdn.net/20170613143127195?" alt="这里写图片描述"></p><p>通过观察，我们猜测成长函数可能与break point存在某种关系：对于convex sets，没有break point，它的成长函数是2的N次方；对于positive rays，break point k=2，它的成长函数是O(N)；对于positive intervals，break point k=3，它的成长函数是$O(N^2)$。则根据这种推论，我们猜测2D perceptrons，它的成长函数$m_H(N)=O(N^{k-1})$ 。如果成立，那么就可以用$m_H$代替M，就满足了机器能够学习的条件。关于上述猜测的证明，我们下节课再详细介绍。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课，我们更深入地探讨了机器学习的可行性。我们把机器学习拆分为两个核心问题：$E_{in}(g)\approx E_{out}(g)$和$E_{in}(g)\approx 0$。对于第一个问题，我们探讨了M个hypothesis到底可以划分为多少种，也就是成长函数$m_H$。并引入了break point的概念，给出了break point的计算方法。下节课，我们将详细论证对于2D perceptrons，它的成长函数与break point是否存在多项式的关系，如果是这样，那么机器学习就是可行的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170613075450559?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记4 -- Feasibility of Learning</title>
    <link href="https://redstonewill.github.io/2018/03/16/4/"/>
    <id>https://redstonewill.github.io/2018/03/16/4/</id>
    <published>2018-03-16T14:30:42.000Z</published>
    <updated>2018-03-17T06:32:57.753Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170612080753910?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要介绍了根据不同的设定，机器学习可以分为不同的类型。其中，监督式学习中的二元分类和回归分析是最常见的也是最重要的机器学习问题。本节课，我们将介绍机器学习的可行性，讨论问题是否可以使用机器学习来解决。</p><h3 id="Learning-is-Impossible"><a href="#Learning-is-Impossible" class="headerlink" title="Learning is Impossible"></a>Learning is Impossible</h3><p>首先，考虑这样一个例子，如下图所示，有3个label为-1的九宫格和3个label为+1的九宫格。根据这6个样本，提取相应label下的特征，预测右边九宫格是属于-1还是+1？结果是，如果依据对称性，我们会把它归为+1；如果依据九宫格左上角是否是黑色，我们会把它归为-1。除此之外，还有根据其它不同特征进行分类，得到不同结果的情况。而且，这些分类结果貌似都是正确合理的，因为对于6个训练样本来说，我们选择的模型都有很好的分类效果。</p><p><img src="http://img.blog.csdn.net/20170612080753910?" alt="这里写图片描述"></p><p>再来看一个比较数学化的二分类例子，输入特征x是二进制的、三维的，对应有8种输入，其中训练样本D有5个。那么，根据训练样本对应的输出y，假设有8个hypothesis，这8个hypothesis在D上，对5个训练样本的分类效果效果都完全正确。但是在另外3个测试数据上，不同的hypothesis表现有好有坏。在已知数据D上，$g\approx f$；但是在D以外的未知数据上，$g\approx f$不一定成立。而机器学习目的，恰恰是希望我们选择的模型能在未知数据上的预测与真实结果是一致的，而不是在已知的数据集D上寻求最佳效果。</p><p><img src="http://img.blog.csdn.net/20170612083115583?" alt="这里写图片描述"></p><p>这个例子告诉我们，我们想要在D以外的数据中更接近目标函数似乎是做不到的，只能保证对D有很好的分类结果。机器学习的这种特性被称为没有免费午餐（No Free Lunch）定理。NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。从这个例子来看，NFL说明了无法保证一个机器学习算法在D以外的数据集上一定能分类或预测正确，除非加上一些假设条件，我们以后会介绍。</p><h3 id="Probability-to-the-Rescue"><a href="#Probability-to-the-Rescue" class="headerlink" title="Probability to the Rescue"></a>Probability to the Rescue</h3><p>从上一节得出的结论是：在训练集D以外的样本上，机器学习的模型是很难，似乎做不到正确预测或分类的。那是否有一些工具或者方法能够对未知的目标函数f做一些推论，让我们的机器学习模型能够变得有用呢？</p><p>如果有一个装有很多（数量很大数不过来）橙色球和绿色球的罐子，我们能不能推断橙色球的比例u？统计学上的做法是，从罐子中随机取出N个球，作为样本，计算这N个球中橙色球的比例v，那么就估计出罐子中橙色球的比例约为v。</p><p><img src="http://img.blog.csdn.net/20170612094631233?" alt="这里写图片描述"></p><p>这种随机抽取的做法能否说明罐子里橙色球的比例一定是v呢？答案是否定的。但是从概率的角度来说，样本中的v很有可能接近我们未知的u。下面从数学推导的角度来看v与u是否相近。</p><p>已知u是罐子里橙色球的比例，v是N个抽取的样本中橙色球的比例。当N足够大的时候，v接近于u。这就是Hoeffding’s inequality：</p><p>$$P[|v-u|&gt;\epsilon]\leq 2exp(-2\epsilon^2N)$$</p><p>Hoeffding不等式说明当N很大的时候，v与u相差不会很大，它们之间的差值被限定在$\epsilon$之内。我们把结论v=u称为probably approximately correct(PAC)。</p><p><img src="http://img.blog.csdn.net/20170612100845352?" alt="这里写图片描述"></p><h3 id="Connection-to-Learning"><a href="#Connection-to-Learning" class="headerlink" title="Connection to Learning"></a>Connection to Learning</h3><p>下面，我们将罐子的内容对应到机器学习的概念上来。机器学习中hypothesis与目标函数相等的可能性，类比于罐子中橙色球的概率问题；罐子里的一颗颗弹珠类比于机器学习样本空间的x；橙色的弹珠类比于h(x)与f不相等；绿色的弹珠类比于h(x)与f相等；从罐子中抽取的N个球类比于机器学习的训练样本D，且这两种抽样的样本与总体样本之间都是独立同分布的。所以呢，如果样本N够大，且是独立同分布的，那么，从样本中$h(x)\neq f(x)$的概率就能推导在抽样样本外的所有样本中$h(x)\neq f(x)$的概率是多少。</p><p><img src="http://img.blog.csdn.net/20170612105733424?" alt="这里写图片描述"></p><p>映射中最关键的点是讲抽样中橙球的概率理解为样本数据集D上h(x)错误的概率，以此推算出在所有数据上h(x)错误的概率，这也是机器学习能够工作的本质，即我们为啥在采样数据上得到了一个假设，就可以推到全局呢？因为两者的错误率是PAC的，只要我们保证前者小，后者也就小了。</p><p> <img src="http://img.blog.csdn.net/20170612110350990?" alt="这里写图片描述"></p><p>这里我们引入两个值$E_{in}(h)$和$E_{out}(h)$。$E_{in}(h)$表示在抽样样本中，h(x)与$y_n$不相等的概率；$E_{out}(h)$表示实际所有样本中，h(x)与f(x)不相等的概率是多少。</p><p><img src="http://img.blog.csdn.net/20170612110744008?" alt="这里写图片描述"></p><p>同样，它的Hoeffding’s inequality可以表示为：</p><p>$$P[|E_{in}(h)-E_{out}(h)|&gt;\epsilon]\leq 2exp(-2\epsilon^2N)$$</p><p>该不等式表明，$E_{in}(h)=E_{out}(h)$也是PAC的。如果$E_{in}(h)\approx E_{out}(h)$，$E_{in}(h)$很小，那么就能推断出$E_{out}(h)$很小，也就是说在该数据分布P下，h与f非常接近，机器学习的模型比较准确。</p><p>一般地，h如果是固定的，N很大的时候，$E_{in}(h)\approx E_{out}(h)$，但是并不意味着$g\approx f$。因为h是固定的，不能保证$E_{in}(h)$足够小，即使$E_{in}(h)\approx E_{out}(h)$，也可能使$E_{out}(h)$偏大。所以，一般会通过演算法A，选择最好的h，使$E_{in}(h)$足够小，从而保证$E_{out}(h)$很小。固定的h，使用新数据进行测试，验证其错误率是多少。</p><p><img src="http://img.blog.csdn.net/20170612112902807?" alt="这里写图片描述"></p><h3 id="Connection-to-Real-Learning"><a href="#Connection-to-Real-Learning" class="headerlink" title="Connection to Real Learning"></a>Connection to Real Learning</h3><p><img src="http://img.blog.csdn.net/20170612135223361?" alt="这里写图片描述"></p><p>假设现在有很多罐子M个（即有M个hypothesis），如果其中某个罐子抽样的球全是绿色，那是不是应该选择这个罐子呢？我们先来看这样一个例子：150个人抛硬币，那么其中至少有一个人连续5次硬币都是正面朝上的概率是</p><p>$$1-(\frac{31}{32})^{150}&gt;99\%$$</p><p>可见这个概率是很大的，但是能否说明5次正面朝上的这个硬币具有代表性呢？答案是否定的！并不能说明该硬币单次正面朝上的概率很大，其实都是0.5。一样的道理，抽到全是绿色求的时候也不能一定说明那个罐子就全是绿色球。当罐子数目很多或者抛硬币的人数很多的时候，可能引发Bad Sample，Bad Sample就是$E_{in}$和$E_{out}$差别很大，即选择过多带来的负面影响，选择过多会恶化不好的情形。</p><p>根据许多次抽样的到的不同的数据集D，Hoeffding’s inequality保证了大多数的D都是比较好的情形（即对于某个h，保证$E_{in}\approx E_{out}$），但是也有可能出现Bad Data，即$E_{in}$和$E_{out}$差别很大的数据集D，这是小概率事件。</p><p><img src="http://img.blog.csdn.net/20170612140418003?" alt="这里写图片描述"></p><p>也就是说，不同的数据集$D_n$，对于不同的hypothesis，有可能成为Bad Data。只要$D_n$在某个hypothesis上是Bad Data，那么$D_n$就是Bad Data。只有当$D_n$在所有的hypothesis上都是好的数据，才说明$D_n$不是Bad Data，可以自由选择演算法A进行建模。那么，根据Hoeffding’s inequality，Bad Data的上界可以表示为连级（union bound）的形式：</p><p><img src="http://img.blog.csdn.net/20170612141520550?" alt="这里写图片描述"></p><p>其中，M是hypothesis的个数，N是样本D的数量，$\epsilon$是参数。该union bound表明，当M有限，且N足够大的时候，Bad Data出现的概率就更低了，即能保证D对于所有的h都有$E_{in}\approx E_{out}$，满足PAC，演算法A的选择不受限制。那么满足这种union bound的情况，我们就可以和之前一样，选取一个合理的演算法（PLA/pocket），选择使$E_{in}$最小的$h_m$作为矩g，一般能够保证$g\approx f$，即有不错的泛化能力。</p><p>所以，如果hypothesis的个数M是有限的，N足够大，那么通过演算法A任意选择一个矩g，都有$E_{in}\approx E_{out}$成立；同时，如果找到一个矩g，使$E_{in}\approx 0$，PAC就能保证$E_{out}\approx 0$。至此，就证明了机器学习是可行的。</p><p><img src="http://img.blog.csdn.net/20170612143915944?" alt="这里写图片描述"></p><p>但是，如上面的学习流程图右下角所示，如果M是无数个，例如之前介绍的PLA直线有无数条，是否这些推论就不成立了呢？是否机器就不能进行学习呢？这些内容和问题，我们下节课再介绍。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了机器学习的可行性。首先引入NFL定理，说明机器学习无法找到一个矩g能够完全和目标函数f一样。接着介绍了可以采用一些统计上的假设，例如Hoeffding不等式，建立$E_{in}$和$E_{out}$的联系，证明对于某个h，当N足够大的时候，$E_{in}$和$E_{out}$是PAC的。最后，对于h个数很多的情况，只要有h个数M是有限的，且N足够大，就能保证$E_{in}\approx E_{out}$，证明机器学习是可行的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170612080753910?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记3 -- Types of Learning</title>
    <link href="https://redstonewill.github.io/2018/03/16/3/"/>
    <id>https://redstonewill.github.io/2018/03/16/3/</id>
    <published>2018-03-16T13:52:20.000Z</published>
    <updated>2018-03-17T06:31:54.198Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170609080354964?imageView/2/w/500/q/100" alt="这里写图片描述"></p><a id="more"></a><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了解决线性分类问题的一个简单的方法：PLA。PLA能够在平面中选择一条直线将样本数据完全正确分类。而对于线性不可分的情况，可以使用Pocket Algorithm来处理。本节课将主要介绍一下机器学习有哪些种类，并进行归纳。</p><h3 id="Learning-with-Different-Output-Space-Y"><a href="#Learning-with-Different-Output-Space-Y" class="headerlink" title="Learning with Different Output Space Y"></a>Learning with Different Output Space Y</h3><p>我们在上节课引入的银行根据用户个人情况判断是否给他发信用卡的例子，这是一个典型的二元分类（binary classification）问题。也就是说输出只有两个，一般y={-1, +1}，-1代表不发信用卡（负类），+1代表发信用卡（正类）。</p><p>二元分类的问题很常见，包括信用卡发放、垃圾邮件判别、患者疾病诊断、答案正确性估计等等。二元分类是机器学习领域非常核心和基本的问题。二元分类有线性模型也有非线性模型，根据实际问题情况，选择不同的模型。</p><p><img src="http://img.blog.csdn.net/20170609080354964?" alt="这里写图片描述"></p><p>除了二元分类，也有多元分类（Multiclass Classification）问题。顾名思义，多元分类的输出多于两个，y={1, 2, … , K}, K&gt;2. 一般多元分类的应用有数字识别、图片内容识别等等。</p><p><img src="http://img.blog.csdn.net/20170609081211320?" alt="这里写图片描述"></p><p>二元分类和多元分类都属于分类问题，它们的输出都是离散值。二对于另外一种情况，比如训练模型，预测房屋价格、股票收益多少等，这类问题的输出y=R，即范围在整个实数空间，是连续的。这类问题，我们把它叫做回归（Regression）。最简单的线性回归是一种典型的回归模型。</p><p>除了分类和回归问题，在自然语言处理等领域中，还会用到一种机器学习问题：结构化学习（Structured Learning）。结构化学习的输出空间包含了某种结构在里面，它的一些解法通常是从多分类问题延伸而来的，比较复杂。本系列课程不会详细介绍Structured Learning，有兴趣的读者可以自行对它进行更深入的研究。</p><p>简单总结一下，机器学习按照输出空间划分的话，包括二元分类、多元分类、回归、结构化学习等不同的类型。其中二元分类和回归是最基础、最核心的两个类型，也是我们课程主要介绍的部分。</p><p><img src="http://img.blog.csdn.net/20170609083208763?" alt="这里写图片描述"></p><h3 id="Learning-with-Different-Data-Label-yn"><a href="#Learning-with-Different-Data-Label-yn" class="headerlink" title="Learning with Different Data Label yn"></a>Learning with Different Data Label yn</h3><p>如果我们拿到的训练样本D既有输入特征x，也有输出yn，那么我们把这种类型的学习称为监督式学习（Supervised Learning）。监督式学习可以是二元分类、多元分类或者是回归，最重要的是知道输出标签yn。与监督式学习相对立的另一种类型是非监督式学习（Unsupervised learning）。非监督式学习是没有输出标签yn的，典型的非监督式学习包括：聚类（clustering）问题，比如对网页上新闻的自动分类；密度估计，比如交通路况分析；异常检测，比如用户网络流量监测。通常情况下，非监督式学习更复杂一些，而且非监督的问题很多都可以使用监督式学习的一些算法思想来实现。</p><p><img src="http://img.blog.csdn.net/20170609084807897?" alt="这里写图片描述"></p><p>介于监督式和非监督式学习之间的叫做半监督式学习（Semi-supervised Learning）。顾名思义，半监督式学习就是说一部分数据有输出标签yn，而另一部分数据没有输出标签yn。在实际应用中，半监督式学习有时候是必须的，比如医药公司对某些药物进行检测，考虑到成本和实验人群限制等问题，只有一部分数据有输出标签yn。</p><p>监督式、非监督式、半监督式学习是机器学习领域三个主要类型。除此之外，还有一种非常重要的类型：增强学习（Reinforcement Learning）。增强学习中，我们给模型或系统一些输入，但是给不了我们希望的真实的输出y，根据模型的输出反馈，如果反馈结果良好，更接近真实输出，就给其正向激励，如果反馈结果不好，偏离真实输出，就给其反向激励。不断通过“反馈-修正”这种形式，一步一步让模型学习的更好，这就是增强学习的核心所在。增强学习可以类比成训练宠物的过程，比如我们要训练狗狗坐下，但是狗狗无法直接听懂我们的指令“sit down”。在训练过程中，我们给狗狗示意，如果它表现得好，我们就给他奖励，如果它做跟sit down完全无关的动作，我们就给它小小的惩罚。这样不断修正狗狗的动作，最终能让它按照我们的指令来行动。实际生活中，增强学习的例子也很多，比如根据用户点击、选择而不断改进的广告系统</p><p>简单总结一下，机器学习按照数据输出标签yn划分的话，包括监督式学习、非监督式学习、半监督式学习和增强学习等。其中，监督式学习应用最为广泛。</p><p><img src="http://img.blog.csdn.net/20170609093112388?" alt="这里写图片描述"></p><h3 id="Learning-with-Different-Protocol-f-xn-yn"><a href="#Learning-with-Different-Protocol-f-xn-yn" class="headerlink" title="Learning with Different Protocol f(xn,yn)"></a>Learning with Different Protocol f(xn,yn)</h3><p>按照不同的协议，机器学习可以分为三种类型：</p><ul><li><p>Batch Learning</p></li><li><p>Online</p></li><li><p>Active Learning</p></li></ul><p>batch learning是一种常见的类型。batch learning获得的训练数据D是一批的，即一次性拿到整个D，对其进行学习建模，得到我们最终的机器学习模型。batch learning在实际应用中最为广泛。</p><p>online是一种在线学习模型，数据是实时更新的，根据数据一个个进来，同步更新我们的算法。比如在线邮件过滤系统，根据一封一封邮件的内容，根据当前算法判断是否为垃圾邮件，再根据用户反馈，及时更新当前算法。这是一个动态的过程。之前我们介绍的PLA和增强学习都可以使用online模型。</p><p>active learning是近些年来新出现的一种机器学习类型，即让机器具备主动问问题的能力，例如手写数字识别，机器自己生成一个数字或者对它不确定的手写字主动提问。active learning优势之一是在获取样本label比较困难的时候，可以节约时间和成本，只对一些重要的label提出需求。</p><p>简单总结一下，按照不同的协议，机器学习可以分为batch, online, active。这三种学习类型分别可以类比为：填鸭式，老师教学以及主动问问题。</p><p><img src="http://img.blog.csdn.net/20170609101642114?" alt="这里写图片描述"></p><h3 id="Learning-with-Different-Input-Space-X"><a href="#Learning-with-Different-Input-Space-X" class="headerlink" title="Learning with Different Input Space X"></a>Learning with Different Input Space X</h3><p>上面几部分介绍的机器学习分类都是根据输出来分类的，比如根据输出空间进行分类，根据输出y的标记进行分类，根据取得数据和标记的方法进行分类。这部分，我们将谈谈输入X有哪些类型。</p><p>输入X的第一种类型就是concrete features。比如说硬币分类问题中硬币的尺寸、重量等；比如疾病诊断中的病人信息等具体特征。concrete features对机器学习来说最容易理解和使用。</p><p>第二种类型是raw features。比如说手写数字识别中每个数字所在图片的mxn维像素值；比如语音信号的频谱等。raw features一般比较抽象，经常需要人或者机器来转换为其对应的concrete features，这个转换的过程就是Feature Transform。</p><p>第三种类型是abstract features。比如某购物网站做购买预测时，提供给参赛者的是抽象加密过的资料编号或者ID，这些特征X完全是抽象的，没有实际的物理含义。所以对于机器学习来说是比较困难的，需要对特征进行更多的转换和提取。</p><p>简单总结一下，根据输入X类型不同，可以分为concetet, raw, abstract。将一些抽象的特征转换为具体的特征，是机器学习过程中非常重要的一个环节。在《机器学习技法》课程中，我们再详细介绍。</p><p><img src="http://img.blog.csdn.net/20170609104530535?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了机器学习的类型，包括Out Space、Data Label、Protocol、Input Space四种类型。</p><p><img src="http://img.blog.csdn.net/20170609104941159?" alt="这里写图片描述"></p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p><p><strong>更多AI资源请关注公众号：红色石头的机器学习之路（ID：redstonewill）</strong><br><img src="http://img.blog.csdn.net/20180305133756771?" alt="这里写图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170609080354964?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记2 -- Learning to Answer Yes or No</title>
    <link href="https://redstonewill.github.io/2018/03/14/2/"/>
    <id>https://redstonewill.github.io/2018/03/14/2/</id>
    <published>2018-03-14T01:39:54.000Z</published>
    <updated>2018-03-17T06:31:58.886Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170608082007683?imageView/2/w/500/q/100" alt="这里写图片描述"></p><a id="more"></a><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课，我们主要简述了机器学习的定义及其重要性，并用流程图的形式介绍了机器学习的整个过程：根据模型H，使用演算法A，在训练样本D上进行训练，得到最好的h，其对应的g就是我们最后需要的机器学习的模型函数，一般g接近于目标函数f。本节课将继续深入探讨机器学习问题，介绍感知机Perceptron模型，并推导课程的第一个机器学习算法：Perceptron Learning Algorithm（PLA）。</p><h3 id="Perceptron-Hypothesis-Set"><a href="#Perceptron-Hypothesis-Set" class="headerlink" title="Perceptron Hypothesis Set"></a>Perceptron Hypothesis Set</h3><p>引入这样一个例子：某银行要根据用户的年龄、性别、年收入等情况来判断是否给该用户发信用卡。现在有训练样本D，即之前用户的信息和是否发了信用卡。这是一个典型的机器学习问题，我们要根据D，通过A，在H中选择最好的h，得到g，接近目标函数f，也就是根据先验知识建立是否给用户发信用卡的模型。银行用这个模型对以后用户进行判断：发信用卡（+1），不发信用卡（-1）。</p><p>在这个机器学习的整个流程中，有一个部分非常重要：就是模型选择，即Hypothesis Set。选择什么样的模型，很大程度上会影响机器学习的效果和表现。下面介绍一个简单常用的Hypothesis Set：感知机（Perceptron）。</p><p>还是刚才银行是否给用户发信用卡的例子，我们把用户的个人信息作为特征向量x，令总共有d个特征，每个特征赋予不同的权重w，表示该特征对输出（是否发信用卡）的影响有多大。那所有特征的加权和的值与一个设定的阈值threshold进行比较：大于这个阈值，输出为+1，即发信用卡；小于这个阈值，输出为-1，即不发信用卡。感知机模型，就是当特征加权和与阈值的差大于或等于0，则输出h(x)=1；当特征加权和与阈值的差小于0，则输出h(x)=-1，而我们的目的就是计算出所有权值w和阈值threshold。</p><p><img src="http://img.blog.csdn.net/20170608082007683?" alt="这里写图片描述"></p><p>为了计算方便，通常我们将阈值threshold当做$w_0$，引入一个$x_0=1$的量与$w_0$相乘，这样就把threshold也转变成了权值$w_0$，简化了计算。h(x)的表达式做如下变换：</p><p><img src="http://img.blog.csdn.net/20170608083119699?" alt="这里写图片描述"></p><p>为了更清晰地说明感知机模型，我们假设Perceptrons在二维平面上，即$h(x)=sign(w_0+w_1x_1+w_2x_2)$。其中，$w_0+w_1x_1+w_2x_2=0$是平面上一条分类直线，直线一侧是正类（+1），直线另一侧是负类（-1）。权重w不同，对应于平面上不同的直线。</p><p><img src="http://img.blog.csdn.net/20170608084125366?" alt="这里写图片描述"></p><p>那么，我们所说的Perceptron，在这个模型上就是一条直线，称之为linear(binary) classifiers。注意一下，感知器线性分类不限定在二维空间中，在3D中，线性分类用平面表示，在更高维度中，线性分类用超平面表示，即只要是形如$w^Tx$的线性模型就都属于linear(binary) classifiers。</p><p>同时，需要注意的是，这里所说的linear(binary) classifiers是用简单的感知器模型建立的，线性分类问题还可以使用logistic regression来解决，后面将会介绍。</p><h3 id="Perceptron-Learning-Algorithm-PLA"><a href="#Perceptron-Learning-Algorithm-PLA" class="headerlink" title="Perceptron Learning Algorithm(PLA)"></a>Perceptron Learning Algorithm(PLA)</h3><p>根据上一部分的介绍，我们已经知道了hypothesis set由许多条直线构成。接下来，我们的目的就是如何设计一个演算法A，来选择一个最好的直线，能将平面上所有的正类和负类完全分开，也就是找到最好的g，使$g\approx f$。</p><p>如何找到这样一条最好的直线呢？我们可以使用逐点修正的思想，首先在平面上随意取一条直线，看看哪些点分类错误。然后开始对第一个错误点就行修正，即变换直线的位置，使这个错误点变成分类正确的点。接着，再对第二个、第三个等所有的错误分类点就行直线纠正，直到所有的点都完全分类正确了，就得到了最好的直线。这种“逐步修正”，就是PLA思想所在。</p><p><img src="http://img.blog.csdn.net/20170608095000165?" alt="这里写图片描述"></p><p>下面介绍一下PLA是怎么做的。首先随机选择一条直线进行分类。然后找到第一个分类错误的点，如果这个点表示正类，被误分为负类，即$w_t^Tx_{n(t)}&lt;0$，那表示w和x夹角大于90度，其中w是直线的法向量。所以，x被误分在直线的下侧（相对于法向量，法向量的方向即为正类所在的一侧），修正的方法就是使w和x夹角小于90度。通常做法是$w\leftarrow w+yx,\ y=1$，如图右上角所示，一次或多次更新后的$w+yx$与x夹角小于90度，能保证x位于直线的上侧，则对误分为负类的错误点完成了直线修正。</p><p>同理，如果是误分为正类的点，即$w_t^Tx_{n(t)}&gt;0$，那表示w和x夹角小于90度，其中w是直线的法向量。所以，x被误分在直线的上侧，修正的方法就是使w和x夹角大于90度。通常做法是$w\leftarrow w+yx,\ y=-1$，如图右下角所示，一次或多次更新后的$w+yx$与x夹角大于90度，能保证x位于直线的下侧，则对误分为正类的错误点也完成了直线修正。</p><p>按照这种思想，遇到个错误点就进行修正，不断迭代。要注意一点：每次修正直线，可能使之前分类正确的点变成错误点，这是可能发生的。但是没关系，不断迭代，不断修正，最终会将所有点完全正确分类（PLA前提是线性可分的）。这种做法的思想是“知错能改”，有句话形容它：“A fault confessed is half redressed.”</p><p>实际操作中，可以一个点一个点地遍历，发现分类错误的点就进行修正，直到所有点全部分类正确。这种被称为Cyclic PLA。</p><p><img src="http://img.blog.csdn.net/20170608102847562?" alt="这里写图片描述"></p><p>下面用图解的形式来介绍PLA的修正过程：</p><p><img src="http://img.blog.csdn.net/20170608104910590?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608104952086?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105013685?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105029404?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105044842?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105100764?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105122249?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105214946?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105230634?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105243181?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608105257829?" alt="这里写图片描述"></p><p>对PLA，我们需要考虑以下两个问题：</p><ul><li><p>PLA迭代一定会停下来吗？如果线性不可分怎么办？</p></li><li><p>PLA停下来的时候，是否能保证$f\approx g$？如果没有停下来，是否有$f\approx g$？</p></li></ul><h3 id="Guarantee-of-PLA"><a href="#Guarantee-of-PLA" class="headerlink" title="Guarantee of PLA"></a>Guarantee of PLA</h3><p>PLA什么时候会停下来呢？根据PLA的定义，当找到一条直线，能将所有平面上的点都分类正确，那么PLA就停止了。要达到这个终止条件，就必须保证D是线性可分（linear separable）。如果是非线性可分的，那么，PLA就不会停止。</p><p><img src="http://img.blog.csdn.net/20170608111542131?" alt="这里写图片描述"></p><p>对于线性可分的情况，如果有这样一条直线，能够将正类和负类完全分开，令这时候的目标权重为$w_f$，则对每个点，必然满足$y_n=sign(w_f^Tx_n)$，即对任一点：</p><p><img src="http://img.blog.csdn.net/20170608134312092?" alt="这里写图片描述"></p><p>PLA会对每次错误的点进行修正，更新权重$w_{t+1}$的值，如果$w_{t+1}$与$w_f$越来越接近，数学运算上就是内积越大，那表示$w_{t+1}$是在接近目标权重$w_f$，证明PLA是有学习效果的。所以，我们来计算$w_{t+1}$与$w_f$的内积：</p><p><img src="http://img.blog.csdn.net/20170608134340499?" alt="这里写图片描述"></p><p>从推导可以看出，$w_{t+1}$与$w_f$的内积跟$w_t$与$w_f$的内积相比更大了。似乎说明了$w_{t+1}$更接近$w_f$，但是内积更大，可能是向量长度更大了，不一定是向量间角度更小。所以，下一步，我们还需要证明$w_{t+1}$与$w_t$向量长度的关系：</p><p><img src="http://img.blog.csdn.net/20170608140302480?" alt="这里写图片描述"></p><p>$w_t$只会在分类错误的情况下更新，最终得到的$||w_{t+1}^2||$相比$||w_{t}^2||$的增量值不超过$max||x_n^2||$。也就是说，$w_t$的增长被限制了，$w_{t+1}$与$w_t$向量长度不会差别太大！</p><p>如果令初始权值$w_0=0$，那么经过T次错误修正后，有如下结论：</p><p>$$\frac{w_f^T}{||w_f||}\frac{w_T}{w_T}\geq \sqrt T\cdot constant$$</p><p>下面贴出来该结论的具体推导过程：</p><p><img src="http://img.blog.csdn.net/20170608143421951?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170608143438779?" alt="这里写图片描述"></p><p>上述不等式左边其实是$w_T$与$w_f$夹角的余弦值，随着T增大，该余弦值越来越接近1，即$w_T$与$w_f$越来越接近。同时，需要注意的是，$\sqrt T\cdot constant\leq 1$，也就是说，迭代次数T是有上界的。根据以上证明，我们最终得到的结论是：$w_{t+1}$与$w_f$的是随着迭代次数增加，逐渐接近的。而且，PLA最终会停下来（因为T有上界），实现对线性可分的数据集完全分类。</p><h3 id="Non-Separable-Data"><a href="#Non-Separable-Data" class="headerlink" title="Non-Separable Data"></a>Non-Separable Data</h3><p>上一部分，我们证明了线性可分的情况下，PLA是可以停下来并正确分类的，但对于非线性可分的情况，$w_f$实际上并不存在，那么之前的推导并不成立，PLA不一定会停下来。所以，PLA虽然实现简单，但也有缺点：</p><p><img src="http://img.blog.csdn.net/20170608145844603?" alt="这里写图片描述"></p><p>对于非线性可分的情况，我们可以把它当成是数据集D中掺杂了一下noise，事实上，大多数情况下我们遇到的D，都或多或少地掺杂了noise。这时，机器学习流程是这样的：</p><p><img src="http://img.blog.csdn.net/20170608150716294?" alt="这里写图片描述"></p><p>在非线性情况下，我们可以把条件放松，即不苛求每个点都分类正确，而是容忍有错误点，取错误点的个数最少时的权重w：</p><p><img src="http://img.blog.csdn.net/20170608151418751?" alt="这里写图片描述"></p><p>事实证明，上面的解是NP-hard问题，难以求解。然而，我们可以对在线性可分类型中表现很好的PLA做个修改，把它应用到非线性可分类型中，获得近似最好的g。</p><p>修改后的PLA称为Packet Algorithm。它的算法流程与PLA基本类似，首先初始化权重$w_0$，计算出在这条初始化的直线中，分类错误点的个数。然后对错误点进行修正，更新w，得到一条新的直线，在计算其对应的分类错误的点的个数，并与之前错误点个数比较，取个数较小的直线作为我们当前选择的分类直线。之后，再经过n次迭代，不断比较当前分类错误点个数与之前最少的错误点个数比较，选择最小的值保存。直到迭代次数完成后，选取个数最少的直线对应的w，即为我们最终想要得到的权重值。</p><p><img src="http://img.blog.csdn.net/20170608155259223?" alt="这里写图片描述"></p><p>如何判断数据集D是不是线性可分？对于二维数据来说，通常还是通过肉眼观察来判断的。一般情况下，Pocket Algorithm要比PLA速度慢一些。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了线性感知机模型，以及解决这类感知机分类问题的简单算法：PLA。我们详细证明了对于线性可分问题，PLA可以停下来并实现完全正确分类。对于不是线性可分的问题，可以使用PLA的修正算法Pocket Algorithm来解决。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170608082007683?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习基石课程学习笔记1 -- The Learning Problem</title>
    <link href="https://redstonewill.github.io/2018/03/13/1/"/>
    <id>https://redstonewill.github.io/2018/03/13/1/</id>
    <published>2018-03-13T08:59:11.000Z</published>
    <updated>2018-03-17T06:33:01.886Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170607145430382?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>最近在看NTU林轩田的《机器学习基石》课程，个人感觉讲的非常好。整个基石课程分成四个部分：</p><ul><li><p>When Can Machine Learn? </p></li><li><p>Why Can Machine Learn? </p></li><li><p>How Can Machine Learn?</p></li><li><p>How Can Machine Learn Better?</p></li></ul><p>每个部分由四节课组成，总共有16节课。那么，从这篇开始，我们将连续对这门课做课程笔记，共16篇，希望能对正在看这们课的童鞋有所帮助。下面开始第一节课的笔记：The Learning Problem。</p><h3 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning"></a>What is Machine Learning</h3><p>什么是“学习”？学习就是人类通过观察、积累经验，掌握某项技能或能力。就好像我们从小学习识别字母、认识汉字，就是学习的过程。而机器学习（Machine Learning），顾名思义，就是让机器（计算机）也能向人类一样，通过观察大量的数据和训练，发现事物规律，获得某种分析问题、解决问题的能力。</p><p><img src="http://img.blog.csdn.net/20170607145430382?" alt="这里写图片描述"></p><p>机器学习可以被定义为：Improving some performance measure with experence computed from data. 也就是机器从数据中总结经验，从数据中找出某种规律或者模型，并用它来解决实际问题。</p><p><img src="http://img.blog.csdn.net/20170607145937180?" alt="这里写图片描述"></p><p>什么情况下会使用机器学习来解决问题呢？其实，目前机器学习的应用非常广泛，基本上任何场合都能够看到它的身影。其应用场合大致可归纳为三个条件：</p><ul><li><p>事物本身存在某种潜在规律</p></li><li><p>某些问题难以使用普通编程解决</p></li><li><p>有大量的数据样本可供使用</p></li></ul><p><img src="http://img.blog.csdn.net/20170607151033657?" alt="这里写图片描述"></p><h3 id="Applications-of-Machine-Learning"><a href="#Applications-of-Machine-Learning" class="headerlink" title="Applications of Machine Learning"></a>Applications of Machine Learning</h3><p>机器学习在我们的衣、食、住、行、教育、娱乐等各个方面都有着广泛的应用，我们的生活处处都离不开机器学习。比如，打开购物网站，网站就会给我们自动推荐我们可能会喜欢的商品；电影频道会根据用户的浏览记录和观影记录，向不同用户推荐他们可能喜欢的电影等等，到处都有机器学习的影子。</p><h3 id="Components-of-Machine-Learning"><a href="#Components-of-Machine-Learning" class="headerlink" title="Components of Machine Learning"></a>Components of Machine Learning</h3><p>本系列的课程对机器学习问题有一些基本的术语需要注意一下：</p><ul><li><p>输入x</p></li><li><p>输出y</p></li><li><p>目标函数f，即最接近实际样本分布的规律</p></li><li><p>训练样本data</p></li><li><p>假设hypothesis，一个机器学习模型对应了很多不同的hypothesis，通过演算法A，选择一个最佳的hypothesis对应的函数称为矩g，g能最好地表示事物的内在规律，也是我们最终想要得到的模型表达式。</p></li></ul><p><img src="http://img.blog.csdn.net/20170607153054321?" alt="这里写图片描述"></p><p>实际中，机器学习的流程图可以表示为：</p><p><img src="http://img.blog.csdn.net/20170607153730795?" alt="这里写图片描述"></p><p>对于理想的目标函数f，我们是不知道的，我们手上拿到的是一些训练样本D，假设是监督式学习，其中有输入x，也有输出y。机器学习的过程，就是根据先验知识选择模型，该模型对应的hypothesis set（用H表示），H中包含了许多不同的hypothesis，通过演算法A，在训练样本D上进行训练，选择出一个最好的hypothes，对应的函数表达式g就是我们最终要求的。一般情况下，g能最接近目标函数f，这样，机器学习的整个流程就完成了。</p><h3 id="Machine-Learning-and-Other-Fields"><a href="#Machine-Learning-and-Other-Fields" class="headerlink" title="Machine Learning and Other Fields"></a>Machine Learning and Other Fields</h3><p>与机器学习相关的领域有：</p><ul><li><p>数据挖掘（Data Mining）</p></li><li><p>人工智能（Artificial Intelligence）</p></li><li><p>统计（Statistics）</p></li></ul><p>其实，机器学习与这三个领域是相通的，基本类似，但也不完全一样。机器学习是这三个领域中的有力工具，而同时，这三个领域也是机器学习可以广泛应用的领域，总得来说，他们之间没有十分明确的界线。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了什么是机器学习，什么样的场合下可以使用机器学习解决问题，然后用流程图的形式展示了机器学习的整个过程，最后把机器学习和数据挖掘、人工智能、统计这三个领域做个比较。本节课的内容主要是概述性的东西，比较简单，所以笔记也相对比较简略。</p><p>这里附上林轩田（Hsuan-Tien Lin）关于这门课的主页：<br><a href="http://www.csie.ntu.edu.tw/~htlin/" target="_blank" rel="noopener">http://www.csie.ntu.edu.tw/~htlin/</a></p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p><p><strong>关注公众号并回复jishi1获得本节课笔记的pdf文件哦～</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170607145430382?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习基石" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="基石" scheme="https://redstonewill.github.io/tags/%E5%9F%BA%E7%9F%B3/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
