<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>红色石头的机器学习之路</title>
  
  <subtitle>公众号ID：redstonewill</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://redstonewill.github.io/"/>
  <updated>2018-04-02T07:45:56.300Z</updated>
  <id>https://redstonewill.github.io/</id>
  
  <author>
    <name>红色石头</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Coursera吴恩达《构建机器学习项目》课程笔记（2）-- 机器学习策略（下）</title>
    <link href="https://redstonewill.github.io/2018/04/02/43/"/>
    <id>https://redstonewill.github.io/2018/04/02/43/</id>
    <published>2018-04-02T07:43:13.000Z</published>
    <updated>2018-04-02T07:45:56.300Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20171122223431927?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>这是Andrew Ng深度学习专项课程第三门课《构建机器学习项目》的第二节笔记，第一节笔记入口：<a href="http://blog.csdn.net/red_stone1/article/details/78519599" target="_blank" rel="noopener">Coursera吴恩达《构建机器学习项目》课程笔记（1）– 机器学习策略（上）</a></p><h3 id="Carrying-out-error-analysis"><a href="#Carrying-out-error-analysis" class="headerlink" title="Carrying out error analysis"></a>Carrying out error analysis</h3><p>对已经建立的机器学习模型进行错误分析（error analysis）十分必要，而且有针对性地、正确地进行error analysis更加重要。</p><p>举个例子，猫类识别问题，已经建立的模型的错误率为10%。为了提高正确率，我们发现该模型会将一些狗类图片错误分类成猫。一种常规解决办法是扩大狗类样本，增强模型对够类（负样本）的训练。但是，这一过程可能会花费几个月的时间，耗费这么大的时间成本到底是否值得呢？也就是说扩大狗类样本，重新训练模型，对提高模型准确率到底有多大作用？这时候我们就需要进行error analysis，帮助我们做出判断。</p><p>方法很简单，我们可以从分类错误的样本中统计出狗类的样本数量。根据狗类样本所占的比重，判断这一问题的重要性。假如狗类样本所占比重仅为5%，即时我们花费几个月的时间扩大狗类样本，提升模型对其识别率，改进后的模型错误率最多只会降低到9.5%。相比之前的10%，并没有显著改善。我们把这种性能限制称为ceiling on performance。相反，假如错误样本中狗类所占比重为50%，那么改进后的模型错误率有望降低到5%，性能改善很大。因此，值得去花费更多的时间扩大狗类样本。</p><p>这种error analysis虽然简单，但是能够避免花费大量的时间精力去做一些对提高模型性能收效甚微的工作，让我们专注解决影响模型正确率的主要问题，十分必要。</p><p>这种error analysis可以同时评估多个影响模型性能的因素，通过各自在错误样本中所占的比例来判断其重要性。例如，猫类识别模型中，可能有以下几个影响因素：</p><ul><li><p><strong>Fix pictures of dogs being recognized as cats</strong></p></li><li><p><strong>Fix great cats(lions, panthers, etc…) being misrecognized</strong></p></li><li><p><strong>Improve performance on blurry images</strong></p></li></ul><p>通常来说，比例越大，影响越大，越应该花费时间和精力着重解决这一问题。这种error analysis让我们改进模型更加有针对性，从而提高效率。</p><h3 id="Cleaning-up-incorrectly-labeled-data"><a href="#Cleaning-up-incorrectly-labeled-data" class="headerlink" title="Cleaning up incorrectly labeled data"></a>Cleaning up incorrectly labeled data</h3><p>监督式学习中，训练样本有时候会出现输出y标注错误的情况，即incorrectly labeled examples。如果这些label标错的情况是随机性的（random errors），DL算法对其包容性是比较强的，即健壮性好，一般可以直接忽略，无需修复。然而，如果是系统错误（systematic errors），这将对DL算法造成影响，降低模型性能。</p><p>刚才说的是训练样本中出现incorrectly labeled data，如果是dev/test sets中出现incorrectly labeled data，该怎么办呢？</p><p>方法很简单，利用上节内容介绍的error analysis，统计dev sets中所有分类错误的样本中incorrectly labeled data所占的比例。根据该比例的大小，决定是否需要修正所有incorrectly labeled data，还是可以忽略。举例说明，若：</p><ul><li><p><strong>Overall dev set error: 10%</strong></p></li><li><p><strong>Errors due incorrect labels: 0.6%</strong></p></li><li><p><strong>Errors due to other causes: 9.4%</strong></p></li></ul><p>上面数据表明Errors due incorrect labels所占的比例仅为0.6%，占dev set error的6%，而其它类型错误占dev set error的94%。因此，这种情况下，可以忽略incorrectly labeled data。</p><p>如果优化DL算法后，出现下面这种情况：</p><ul><li><p><strong>Overall dev set error: 2%</strong></p></li><li><p><strong>Errors due incorrect labels: 0.6%</strong></p></li><li><p><strong>Errors due to other causes: 1.4%</strong></p></li></ul><p>上面数据表明Errors due incorrect labels所占的比例依然为0.6%，但是却占dev set error的30%，而其它类型错误占dev set error的70%。因此，这种情况下，incorrectly labeled data不可忽略，需要手动修正。</p><p>我们知道，dev set的主要作用是在不同算法之间进行比较，选择错误率最小的算法模型。但是，如果有incorrectly labeled data的存在，当不同算法错误率比较接近的时候，我们无法仅仅根据Overall dev set error准确指出哪个算法模型更好，必须修正incorrectly labeled data。</p><p>关于修正incorrect dev/test set data，有几条建议：</p><ul><li><p><strong>Apply same process to your dev and test sets to make sure they continue to come from the same distribution</strong></p></li><li><p><strong>Consider examining examples your algorithm got right as well as ones it got wrong</strong></p></li><li><p><strong>Train and dev/test data may now come from slightly different distributions</strong></p></li></ul><h3 id="Build-your-first-system-quickly-then-iterate"><a href="#Build-your-first-system-quickly-then-iterate" class="headerlink" title="Build your first system quickly then iterate"></a>Build your first system quickly then iterate</h3><p>对于如何构建一个机器学习应用模型，Andrew给出的建议是先快速构建第一个简单模型，然后再反复迭代优化。</p><ul><li><p><strong>Set up dev/test set and metric</strong></p></li><li><p><strong>Build initial system quickly</strong></p></li><li><p><strong>Use Bias/Variance analysis &amp; Error analysis to prioritize next steps</strong></p></li></ul><h3 id="Training-and-testing-on-different-distribution"><a href="#Training-and-testing-on-different-distribution" class="headerlink" title="Training and testing on different distribution"></a>Training and testing on different distribution</h3><p>当train set与dev/test set不来自同一个分布的时候，我们应该如何解决这一问题，构建准确的机器学习模型呢？</p><p>以猫类识别为例，train set来自于网络下载（webpages），图片比较清晰；dev/test set来自用户手机拍摄（mobile app），图片比较模糊。假如train set的大小为200000，而dev/test set的大小为10000，显然train set要远远大于dev/test set。</p><p><img src="http://img.blog.csdn.net/20171122223431927?" alt="这里写图片描述"></p><p>虽然dev/test set质量不高，但是模型最终主要应用在对这些模糊的照片的处理上。面对train set与dev/test set分布不同的情况，有两种解决方法。</p><p>第一种方法是将train set和dev/test set完全混合，然后在随机选择一部分作为train set，另一部分作为dev/test set。例如，混合210000例样本，然后随机选择205000例样本作为train set，2500例作为dev set，2500例作为test set。这种做法的优点是实现train set和dev/test set分布一致，缺点是dev/test set中webpages图片所占的比重比mobile app图片大得多。例如dev set包含2500例样本，大约有2381例来自webpages，只有119例来自mobile app。这样，dev set的算法模型对比验证，仍然主要由webpages决定，实际应用的mobile app图片所占比重很小，达不到验证效果。因此，这种方法并不是很好。</p><p>第二种方法是将原来的train set和一部分dev/test set组合当成train set，剩下的dev/test set分别作为dev set和test set。例如，200000例webpages图片和5000例mobile app图片组合成train set，剩下的2500例mobile app图片作为dev set，2500例mobile app图片作为test set。其关键在于dev/test set全部来自于mobile app。这样保证了验证集最接近实际应用场合。这种方法较为常用，而且性能表现比较好。</p><h3 id="Bias-and-Variance-with-mismatched-data-distributions"><a href="#Bias-and-Variance-with-mismatched-data-distributions" class="headerlink" title="Bias and Variance with mismatched data distributions"></a>Bias and Variance with mismatched data distributions</h3><p>我们之前介绍过，根据human-level error、training error和dev error的相对值可以判定是否出现了bias或者variance。但是，需要注意的一点是，如果train set和dev/test set来源于不同分布，则无法直接根据相对值大小来判断。例如某个模型human-level error为0%，training error为1%，dev error为10%。根据我们之前的理解，显然该模型出现了variance。但是，training error与dev error之间的差值9%可能来自算法本身（variance），也可能来自于样本分布不同。比如dev set都是很模糊的图片样本，本身就难以识别，跟算法模型关系不大。因此不能简单认为出现了variance。</p><p>在可能伴有train set与dev/test set分布不一致的情况下，定位是否出现variance的方法是设置train-dev set。Andrew给train-dev set的定义是：“Same distribution as training set, but not used for training.”也就是说，从原来的train set中分割出一部分作为train-dev set，train-dev set不作为训练模型使用，而是与dev set一样用于验证。</p><p>这样，我们就有training error、training-dev error和dev error三种error。其中，training error与training-dev error的差值反映了variance；training-dev error与dev error的差值反映了data mismatch problem，即样本分布不一致。</p><p>举例说明，如果training error为1%，training-dev error为9%，dev error为10%，则variance问题比较突出。如果training error为1%，training-dev error为1.5%，dev error为10%，则data mismatch problem比较突出。通过引入train-dev set，能够比较准确地定位出现了variance还是data mismatch。</p><p>总结一下human-level error、training error、training-dev error、dev error以及test error之间的差值关系和反映的问题：</p><p><img src="http://img.blog.csdn.net/20171123102507240?" alt="这里写图片描述"></p><p>一般情况下，human-level error、training error、training-dev error、dev error以及test error的数值是递增的，但是也会出现dev error和test error下降的情况。这主要可能是因为训练样本比验证/测试样本更加复杂，难以训练。</p><h3 id="Addressing-data-mismatch"><a href="#Addressing-data-mismatch" class="headerlink" title="Addressing data mismatch"></a>Addressing data mismatch</h3><p>关于如何解决train set与dev/test set样本分布不一致的问题，有两条建议：</p><ul><li><p><strong>Carry out manual error analysis to try to understand difference between training dev/test sets</strong></p></li><li><p><strong>Make training data more similar; or collect more data similar to dev/test sets</strong></p></li></ul><p>为了让train set与dev/test set类似，我们可以使用人工数据合成的方法（artificial data synthesis）。例如说话人识别问题，实际应用场合（dev/test set）是包含背景噪声的，而训练样本train set很可能没有背景噪声。为了让train set与dev/test set分布一致，我们可以在train set上人工添加背景噪声，合成类似实际场景的声音。这样会让模型训练的效果更准确。但是，需要注意的是，我们不能给每段语音都增加同一段背景噪声，这样会出现对背景噪音的过拟合，效果不佳。这就是人工数据合成需要注意的地方。</p><h3 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h3><p>深度学习非常强大的一个功能之一就是有时候你可以将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。比如我们已经训练好一个猫类识别的神经网络模型，那么我们可以直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去。这种学习方法被称为迁移学习（Transfer Learning）。</p><p>如果我们已经有一个训练好的神经网络，用来做图像识别。现在，我们想要构建另外一个通过X光片进行诊断的模型。迁移学习的做法是无需重新构建新的模型，而是利用之前的神经网络模型，只改变样本输入、输出以及输出层的权重系数$W^{[L]},\ b^{[L]}$。也就是说对新的样本(X,Y)，重新训练输出层权重系数$W^{[L]},\ b^{[L]}$，而其它层所有的权重系数$W^{[l]},\ b^{[l]}$保持不变。</p><p><img src="http://img.blog.csdn.net/20171123160012172?" alt="这里写图片描述"></p><p>迁移学习，重新训练权重系数，如果需要构建新模型的样本数量较少，那么可以像刚才所说的，只训练输出层的权重系数$W^{[L]},\ b^{[L]}$，保持其它层所有的权重系数$W^{[l]},\ b^{[l]}$不变。这种做法相对来说比较简单。如果样本数量足够多，那么也可以只保留网络结构，重新训练所有层的权重系数。这种做法使得模型更加精确，因为毕竟样本对模型的影响最大。选择哪种方法通常由数据量决定。</p><p>顺便提一下，如果重新训练所有权重系数，初始$W^{[l]},\ b^{[l]}$由之前的模型训练得到，这一过程称为pre-training。之后，不断调试、优化$W^{[l]},\ b^{[l]}$的过程称为fine-tuning。pre-training和fine-tuning分别对应上图中的黑色箭头和红色箭头。</p><p>迁移学习之所以能这么做的原因是，神经网络浅层部分能够检测出许多图片固有特征，例如图像边缘、曲线等。使用之前训练好的神经网络部分结果有助于我们更快更准确地提取X光片特征。二者处理的都是图片，而图片处理是有相同的地方，第一个训练好的神经网络已经帮我们实现如何提取图片有用特征了。 因此，即便是即将训练的第二个神经网络样本数目少，仍然可以根据第一个神经网络结构和权重系数得到健壮性好的模型。</p><p>迁移学习可以保留原神经网络的一部分，再添加新的网络层。具体问题，具体分析，可以去掉输出层后再增加额外一些神经层。</p><p><img src="http://img.blog.csdn.net/20171124141447596?" alt="这里写图片描述"></p><p>总体来说，迁移学习的应用场合主要包括三点：</p><ul><li><p><strong>Task A and B have the same input x.</strong></p></li><li><p><strong>You have a lot more data for Task A than Task B.</strong></p></li><li><p><strong>Low level features from A could be helpful for learning B.</strong></p></li></ul><h3 id="Multi-task-learning"><a href="#Multi-task-learning" class="headerlink" title="Multi-task learning"></a>Multi-task learning</h3><p>顾名思义，多任务学习（multi-task learning）就是构建神经网络同时执行多个任务。这跟二元分类或者多元分类都不同，多任务学习类似将多个神经网络融合在一起，用一个网络模型来实现多种分类效果。如果有C个，那么输出y的维度是$(C,1)$。例如汽车自动驾驶中，需要实现的多任务为行人、车辆、交通标志和信号灯。如果检测出汽车和交通标志，则y为：</p><p>$$y=<br>\left[<br> \begin{matrix}<br>   0\<br>   1\<br>   1\<br>   0<br>  \end{matrix}<br>  \right]<br>$$</p><p>多任务学习模型的cost function为：</p><p>$$\frac1m\sum_{i=1}^m\sum_{j=1}^cL(\hat y_j^{(i)},y_j^{(i)})$$</p><p>其中，j表示任务下标，总有c个任务。对应的loss function为：</p><p>$$L(\hat y_j^{(i)},y_j^{(i)})=-y_j^{(i)}log\ \hat y_j^{(i)}-(1-y_j^{(i)})log\ (1-\hat y_j^{(i)})$$</p><p>值得一提的是，Multi-task learning与Softmax regression的区别在于Softmax regression是single label的，即输出向量y只有一个元素为1；而Multi-task learning是multiple labels的，即输出向量y可以有多个元素为1。</p><p>多任务学习是使用单个神经网络模型来实现多个任务。实际上，也可以分别构建多个神经网络来实现。但是，如果各个任务之间是相似问题（例如都是图片类别检测），则可以使用多任务学习模型。另外，多任务学习中，可能存在训练样本Y某些label空白的情况，这并不影响多任务模型的训练。</p><p>总体来说，多任务学习的应用场合主要包括三点：</p><ul><li><p><strong>Training on a set of tasks that could benefit from having shared lower-level features.</strong></p></li><li><p><strong>Usually: Amount of data you have for each task is quite similar.</strong></p></li><li><p><strong>Can train a big enough neural network to do well on all the tasks.</strong></p></li></ul><p>顺便提一下，迁移学习和多任务学习在实际应用中，迁移学习使用得更多一些。</p><h3 id="What-is-end-to-end-deep-learning"><a href="#What-is-end-to-end-deep-learning" class="headerlink" title="What is end-to-end deep learning"></a>What is end-to-end deep learning</h3><p>端到端（end-to-end）深度学习就是将所有不同阶段的数据处理系统或学习系统模块组合在一起，用一个单一的神经网络模型来实现所有的功能。它将所有模块混合在一起，只关心输入和输出。</p><p>以语音识别为例，传统的算法流程和end-to-end模型的区别如下：</p><p><img src="http://img.blog.csdn.net/20171127134439086?" alt="这里写图片描述"></p><p>如果训练样本足够大，神经网络模型足够复杂，那么end-to-end模型性能比传统机器学习分块模型更好。实际上，end-to-end让神经网络模型内部去自我训练模型特征，自我调节，增加了模型整体契合度。</p><h3 id="Whether-to-use-end-to-end-deep-learning"><a href="#Whether-to-use-end-to-end-deep-learning" class="headerlink" title="Whether to use end-to-end deep learning"></a>Whether to use end-to-end deep learning</h3><p>end-to-end深度学习有优点也有缺点。</p><p>优点：</p><ul><li><p><strong>Let the data speak</strong></p></li><li><p><strong>Less hand-designing of components needed</strong></p></li></ul><p>缺点：</p><ul><li><p><strong>May need large amount of data</strong></p></li><li><p><strong>Excludes potentially useful hand-designed</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20171122223431927?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达构建机器学习项目" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9E%84%E5%BB%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《构建机器学习项目》课程笔记（1）-- 机器学习策略（上）</title>
    <link href="https://redstonewill.github.io/2018/04/02/42/"/>
    <id>https://redstonewill.github.io/2018/04/02/42/</id>
    <published>2018-04-02T07:38:22.000Z</published>
    <updated>2018-04-02T07:41:21.824Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20171113204424247?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>《Structuring Machine Learning Projects》（构建机器学习项目）这门课是Andrw Ng深度学习专项课程中的第三门课。这门课主要介绍机器学习中的一些策略和方法，让我们能够更快更有效地让机器学习系统工作，该门课共有两周的课时。</p><h3 id="Why-ML-Strategy"><a href="#Why-ML-Strategy" class="headerlink" title="Why ML Strategy"></a>Why ML Strategy</h3><p>当我们最初得到一个深度神经网络模型时，我们可能希望从很多方面来对它进行优化，例如：</p><ul><li><p><strong>Collect more data</strong></p></li><li><p><strong>Collect more diverse training set</strong></p></li><li><p><strong>Train algorithm longer with gradient descent</strong></p></li><li><p><strong>Try Adam instead of gradient descent</strong></p></li><li><p><strong>Try bigger network</strong></p></li><li><p><strong>Try smaller network</strong></p></li><li><p><strong>Try dropout</strong></p></li><li><p><strong>Add L2 regularization</strong></p></li><li><p><strong>Network architecture: Activation functions, #hidden units…</strong></p></li></ul><p>可选择的方法很多，也很复杂、繁琐。盲目选择、尝试不仅耗费时间而且可能收效甚微。因此，使用快速、有效的策略来优化机器学习模型是非常必要的。</p><h3 id="Orthogonalization"><a href="#Orthogonalization" class="headerlink" title="Orthogonalization"></a>Orthogonalization</h3><p>机器学习中有许多参数、超参数需要调试。通过每次只调试一个参数，保持其它参数不变，而得到的模型某一性能改变是一种最常用的调参策略，我们称之为正交化方法（Orthogonalization）。</p><p>Orthogonalization的核心在于每次调试一个参数只会影响模型的某一个性能。例如老式电视机旋钮，每个旋钮就对应一个功能，调整旋钮会调整对应的功能，而不会影响其它功能。也就是说彼此旋钮之间是互不影响的，是正交的，这也是Orthogonalization名称的由来。这种方法能够让我们更快更有效地进行机器学习模型的调试和优化。</p><p>对应到机器学习监督式学习模型中，可以大致分成四个独立的“功能”，每个“功能”对应一些可调节的唯一的旋钮。四个“功能”如下：</p><ul><li><p><strong>Fit training set well on cost function</strong></p></li><li><p><strong>Fit dev set well on cost function</strong></p></li><li><p><strong>Fit test set well on cost function</strong></p></li><li><p><strong>Performs well in real world</strong></p></li></ul><p>其中，第一条优化训练集可以通过使用更复杂NN，使用Adam等优化算法来实现；第二条优化验证集可以通过正则化，采用更多训练样本来实现；第三条优化测试集可以通过使用更多的验证集样本来实现；第四条提升实际应用模型可以通过更换验证集，使用新的cost function来实现。概括来说，每一种“功能”对应不同的调节方法。而这些调节方法（旋钮）只会对应一个“功能”，是正交的。</p><p>顺便提一下，early stopping在模型功能调试中并不推荐使用。因为early stopping在提升验证集性能的同时降低了训练集的性能。也就是说early stopping同时影响两个“功能”，不具有独立性、正交性。</p><h3 id="Single-number-evaluation-metric"><a href="#Single-number-evaluation-metric" class="headerlink" title="Single number evaluation metric"></a>Single number evaluation metric</h3><p>构建、优化机器学习模型时，单值评价指标非常必要。有了量化的单值评价指标后，我们就能根据这一指标比较不同超参数对应的模型的优劣，从而选择最优的那个模型。</p><p>举个例子，比如有A和B两个模型，它们的准确率（Precision）和召回率（Recall）分别如下：</p><p><img src="http://img.blog.csdn.net/20171113160716628?" alt="这里写图片描述"></p><p>如果只看Precision的话，B模型更好。如果只看Recall的话，A模型更好。实际应用中，我们通常使用单值评价指标F1 Score来评价模型的好坏。F1 Score综合了Precision和Recall的大小，计算方法如下：</p><p>$$F1=\frac{2\cdot P\cdot R}{P+R}$$</p><p>然后得到了A和B模型各自的F1 Score：</p><p><img src="http://img.blog.csdn.net/20171113161842574?" alt="这里写图片描述"></p><p>从F1 Score来看，A模型比B模型更好一些。通过引入单值评价指标F1 Score，很方便对不同模型进行比较。</p><p>除了F1 Score之外，我们还可以使用平均值作为单值评价指标来对模型进行评估。如下图所示，A, B, C, D, E, F六个模型对不同国家样本的错误率不同，可以计算其平均性能，然后选择平均错误率最小的那个模型（C模型）。</p><p><img src="http://img.blog.csdn.net/20171113163112581?" alt="这里写图片描述"></p><h3 id="Satisficing-and-Optimizing-metic"><a href="#Satisficing-and-Optimizing-metic" class="headerlink" title="Satisficing and Optimizing metic"></a>Satisficing and Optimizing metic</h3><p>有时候，要把所有的性能指标都综合在一起，构成单值评价指标是比较困难的。解决办法是，我们可以把某些性能作为优化指标（Optimizing metic），寻求最优化值；而某些性能作为满意指标（Satisficing metic），只要满足阈值就行了。</p><p>举个猫类识别的例子，有A，B，C三个模型，各个模型的Accuracy和Running time如下表中所示：</p><p><img src="http://img.blog.csdn.net/20171113171230472?" alt="这里写图片描述"></p><p>Accuracy和Running time这两个性能不太合适综合成单值评价指标。因此，我们可以将Accuracy作为优化指标（Optimizing metic），将Running time作为满意指标（Satisficing metic）。也就是说，给Running time设定一个阈值，在其满足阈值的情况下，选择Accuracy最大的模型。如果设定Running time必须在100ms以内，那么很明显，模型C不满足阈值条件，首先剔除；模型B相比较模型A而言，Accuracy更高，性能更好。</p><p>概括来说，性能指标（Optimizing metic）是需要优化的，越优越好；而满意指标（Satisficing metic）只要满足设定的阈值就好了。</p><h3 id="Train-dev-test-distributions"><a href="#Train-dev-test-distributions" class="headerlink" title="Train/dev/test distributions"></a>Train/dev/test distributions</h3><p>Train/dev/test sets如何设置对机器学习的模型训练非常重要，合理设置能够大大提高模型训练效率和模型质量。</p><p>原则上应该尽量保证dev sets和test sets来源于同一分布且都反映了实际样本的情况。如果dev sets和test sets不来自同一分布，那么我们从dev sets上选择的“最佳”模型往往不能够在test sets上表现得很好。这就好比我们在dev sets上找到最接近一个靶的靶心的箭，但是我们test sets提供的靶心却远远偏离dev sets上的靶心，结果这支肯定无法射中test sets上的靶心位置。</p><p><img src="http://img.blog.csdn.net/20171113204424247?" alt="这里写图片描述"></p><h3 id="Size-of-the-dev-and-test-sets"><a href="#Size-of-the-dev-and-test-sets" class="headerlink" title="Size of the dev and test sets"></a>Size of the dev and test sets</h3><p>在之前的课程中我们已经介绍过，当样本数量不多（小于一万）的时候，通常将Train/dev/test sets的比例设为60%/20%/20%，在没有dev sets的情况下，Train/test sets的比例设为70%/30%。当样本数量很大（百万级别）的时候，通常将相应的比例设为98%/1%/1%或者99%/1%。</p><p>对于dev sets数量的设置，应该遵循的准则是通过dev sets能够检测不同算法或模型的区别，以便选择出更好的模型。</p><p>对于test sets数量的设置，应该遵循的准则是通过test sets能够反映出模型在实际中的表现。</p><p>实际应用中，可能只有train/dev sets，而没有test sets。这种情况也是允许的，只要算法模型没有对dev sets过拟合。但是，条件允许的话，最好是有test sets，实现无偏估计。</p><h3 id="When-to-change-dev-test-sets-and-metrics"><a href="#When-to-change-dev-test-sets-and-metrics" class="headerlink" title="When to change dev/test sets and metrics"></a>When to change dev/test sets and metrics</h3><p>算法模型的评价标准有时候需要根据实际情况进行动态调整，目的是让算法模型在实际应用中有更好的效果。</p><p>举个猫类识别的例子。初始的评价标准是错误率，算法A错误率为3%，算法B错误率为5%。显然，A更好一些。但是，实际使用时发现算法A会通过一些色情图片，但是B没有出现这种情况。从用户的角度来说，他们可能更倾向选择B模型，虽然B的错误率高一些。这时候，我们就需要改变之前单纯只是使用错误率作为评价标准，而考虑新的情况进行改变。例如增加色情图片的权重，增加其代价。</p><p>原来的cost function：</p><p>$$J=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})$$</p><p>更改评价标准后的cost function：</p><p>$$J=\frac{1}{w^{(i)}}\sum_{i=1}^mw^{(i)}L(\hat y^{(i)},y^{(i)})$$</p><p>$$w^{(i)}=\begin{cases}<br>        1, &amp; x^{(i)}\ is\ non-porn\<br>        10, &amp; x^{(i)}\ is\ porn<br>    \end{cases}$$</p><p>概括来说，机器学习可分为两个过程：</p><ul><li><p><strong>Define a metric to evaluate classifiers</strong></p></li><li><p><strong>How to do well on this metric</strong></p></li></ul><p>也就是说，第一步是找靶心，第二步是通过训练，射中靶心。但是在训练的过程中可能会根据实际情况改变算法模型的评价标准，进行动态调整。</p><p>另外一个需要动态改变评价标准的情况是dev/test sets与实际使用的样本分布不一致。比如猫类识别样本图像分辨率差异。</p><p><img src="http://img.blog.csdn.net/20171114094708310?" alt="这里写图片描述"></p><h3 id="Why-human-level-performance"><a href="#Why-human-level-performance" class="headerlink" title="Why human-level performance"></a>Why human-level performance</h3><p>机器学习模型的表现通常会跟人类水平表现作比较，如下图所示：</p><p><img src="http://img.blog.csdn.net/20171115090646865?" alt="这里写图片描述"></p><p>图中，横坐标是训练时间，纵坐标是准确性。机器学习模型经过训练会不断接近human-level performance甚至超过它。但是，超过human-level performance之后，准确性会上升得比较缓慢，最终不断接近理想的最优情况，我们称之为bayes optimal error。理论上任何模型都不能超过它，bayes optimal error代表了最佳表现。</p><p>实际上，human-level performance在某些方面有不俗的表现。例如图像识别、语音识别等领域，人类是很擅长的。所以，让机器学习模型性能不断接近human-level performance非常必要也做出很多努力：</p><ul><li><p><strong>Get labeled data from humans.</strong></p></li><li><p><strong>Gain insight from manual error analysis: Why did a person get this right?</strong></p></li><li><p><strong>Better analysis of bias/variance.</strong></p></li></ul><h3 id="Avoidable-bias"><a href="#Avoidable-bias" class="headerlink" title="Avoidable bias"></a>Avoidable bias</h3><p>实际应用中，要看human-level error，training error和dev error的相对值。例如猫类识别的例子中，如果human-level error为1%，training error为8%，dev error为10%。由于training error与human-level error相差7%，dev error与training error只相差2%，所以目标是尽量在训练过程中减小training error，即减小偏差bias。如果图片很模糊，肉眼也看不太清，human-level error提高到7.5%。这时，由于training error与human-level error只相差0.5%，dev error与training error只相差2%，所以目标是尽量在训练过程中减小dev error，即方差variance。这是相对而言的。</p><p>对于物体识别这类CV问题，human-level error是很低的，很接近理想情况下的bayes optimal error。因此，上面例子中的1%和7.5%都可以近似看成是两种情况下对应的bayes optimal error。实际应用中，我们一般会用human-level error代表bayes optimal error。</p><p>通常，我们把training error与human-level error之间的差值称为bias，也称作avoidable bias；把dev error与training error之间的差值称为variance。根据bias和variance值的相对大小，可以知道算法模型是否发生了欠拟合或者过拟合。</p><h3 id="Understanding-human-level-performance"><a href="#Understanding-human-level-performance" class="headerlink" title="Understanding human-level performance"></a>Understanding human-level performance</h3><p>我们说过human-level performance能够代表bayes optimal error。但是，human-level performance如何定义呢？举个医学图像识别的例子，不同人群的error有所不同：</p><ul><li><p><strong>Typical human : 3% error</strong></p></li><li><p><strong>Typical doctor : 1% error </strong></p></li><li><p><strong>Experienced doctor : 0.7% error</strong></p></li><li><p><strong>Team of experienced doctors : 0.5% error</strong></p></li></ul><p>不同人群他们的错误率不同。一般来说，我们将表现最好的那一组，即Team of experienced doctors作为human-level performance。那么，这个例子中，human-level error就为0.5%。但是实际应用中，不同人可能选择的human-level performance基准是不同的，这会带来一些影响。</p><p>假如该模型training error为0.7%，dev error为0.8。如果选择Team of experienced doctors，即human-level error为0.5%，则bias比variance更加突出。如果选择Experienced doctor，即human-level error为0.7%，则variance更加突出。也就是说，选择什么样的human-level error，有时候会影响bias和variance值的相对变化。当然这种情况一般只会在模型表现很好，接近bayes optimal error的时候出现。越接近bayes optimal error，模型越难继续优化，因为这时候的human-level performance可能是比较模糊难以准确定义的。</p><h3 id="Surpassing-human-level-performance"><a href="#Surpassing-human-level-performance" class="headerlink" title="Surpassing human-level performance"></a>Surpassing human-level performance</h3><p>对于自然感知类问题，例如视觉、听觉等，机器学习的表现不及人类。但是在很多其它方面，机器学习模型的表现已经超过人类了，包括：</p><ul><li><p><strong>Online advertising</strong></p></li><li><p><strong>Product recommendations</strong></p></li><li><p><strong>Logistics(predicting transit time)</strong></p></li><li><p><strong>Loan approvals</strong></p></li></ul><p>实际上，机器学习模型超过human-level performance是比较困难的。但是只要提供足够多的样本数据，训练复杂的神经网络，模型预测准确性会大大提高，很有可能接近甚至超过human-level performance。值得一提的是当算法模型的表现超过human-level performance时，很难再通过人的直觉来解决如何继续提高算法模型性能的问题。</p><h3 id="Improving-your-model-performance"><a href="#Improving-your-model-performance" class="headerlink" title="Improving your model performance"></a>Improving your model performance</h3><p>提高机器学习模型性能主要要解决两个问题：avoidable bias和variance。我们之前介绍过，training error与human-level error之间的差值反映的是avoidable bias，dev error与training error之间的差值反映的是variance。</p><p>解决avoidable bias的常用方法包括：</p><ul><li><p><strong>Train bigger model</strong></p></li><li><p><strong>Train longer/better optimization algorithms: momentum, RMSprop, Adam</strong></p></li><li><p><strong>NN architecture/hyperparameters search</strong></p></li></ul><p>解决variance的常用方法包括：</p><ul><li><p><strong>More data</strong></p></li><li><p><strong>Regularization: L2, dropout, data augmentation</strong></p></li><li><p><strong>NN architecture/hyperparameters search</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20171113204424247?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达构建机器学习项目" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9E%84%E5%BB%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《优化深度神经网络》课程笔记（3）-- 超参数调试、Batch正则化和编程框架</title>
    <link href="https://redstonewill.github.io/2018/03/29/41/"/>
    <id>https://redstonewill.github.io/2018/03/29/41/</id>
    <published>2018-03-29T13:21:30.000Z</published>
    <updated>2018-03-29T13:55:23.340Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20171031230142816?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了深度神经网络的优化算法。包括对原始数据集进行分割，使用mini-batch gradient descent。然后介绍了指数加权平均（Exponentially weighted averages）的概念以及偏移校正（bias correction）方法。接着，我们着重介绍了三种常用的加速神经网络学习速度的三种算法：动量梯度下降、RMSprop和Adam算法。其中，Adam结合了动量梯度下降和RMSprop各自的优点，实际应用中表现更好。然后，我们介绍了另外一种提高学习速度的方法：learning rate decay，通过不断减小学习因子，减小步进长度，来减小梯度振荡。最后，我们对深度学习中local optima的概念作了更深入的解释。本节课，我们将重点介绍三个方面的内容：超参数调试、Batch正则化和深度学习编程框架。</p><h3 id="Tuning-Process"><a href="#Tuning-Process" class="headerlink" title="Tuning Process"></a>Tuning Process</h3><p>深度神经网络需要调试的超参数（Hyperparameters）较多，包括：</p><ul><li><p><strong>$\alpha$：学习因子</strong></p></li><li><p><strong>$\beta$：动量梯度下降因子</strong></p></li><li><p><strong>$\beta_1,\beta_2,\varepsilon$：Adam算法参数</strong></p></li><li><p><strong>#layers：神经网络层数</strong></p></li><li><p><strong>#hidden units：各隐藏层神经元个数</strong></p></li><li><p><strong>learning rate decay：学习因子下降参数</strong></p></li><li><p><strong>mini-batch size：批量训练样本包含的样本个数</strong></p></li></ul><p>超参数之间也有重要性差异。通常来说，学习因子$\alpha$是最重要的超参数，也是需要重点调试的超参数。动量梯度下降因子$\beta$、各隐藏层神经元个数#hidden units和mini-batch size的重要性仅次于$\alpha$。然后就是神经网络层数#layers和学习因子下降参数learning rate decay。最后，Adam算法的三个参数$\beta_1,\beta_2,\varepsilon$一般常设置为0.9，0.999和$10^{-8}$，不需要反复调试。当然，这里超参数重要性的排名并不是绝对的，具体情况，具体分析。</p><p>如何选择和调试超参数？传统的机器学习中，我们对每个参数等距离选取任意个数的点，然后，分别使用不同点对应的参数组合进行训练，最后根据验证集上的表现好坏，来选定最佳的参数。例如有两个待调试的参数，分别在每个参数上选取5个点，这样构成了5x5=25中参数组合，如下图所示：</p><p><img src="http://img.blog.csdn.net/20171031162750019?" alt="这里写图片描述"></p><p>这种做法在参数比较少的时候效果较好。但是在深度神经网络模型中，我们一般不采用这种均匀间隔取点的方法，比较好的做法是使用随机选择。也就是说，对于上面这个例子，我们随机选择25个点，作为待调试的超参数，如下图所示：</p><p><img src="http://img.blog.csdn.net/20171031162811224?" alt="这里写图片描述"></p><p>随机化选择参数的目的是为了尽可能地得到更多种参数组合。还是上面的例子，如果使用均匀采样的话，每个参数只有5种情况；而使用随机采样的话，每个参数有25种可能的情况，因此更有可能得到最佳的参数组合。</p><p>这种做法带来的另外一个好处就是对重要性不同的参数之间的选择效果更好。假设hyperparameter1为$\alpha$，hyperparameter2为$\varepsilon$，显然二者的重要性是不一样的。如果使用第一种均匀采样的方法，$\varepsilon$的影响很小，相当于只选择了5个$\alpha$值。而如果使用第二种随机采样的方法，$\varepsilon$和$\alpha$都有可能选择25种不同值。这大大增加了$\alpha$调试的个数，更有可能选择到最优值。其实，在实际应用中完全不知道哪个参数更加重要的情况下，随机采样的方式能有效解决这一问题，但是均匀采样做不到这点。</p><p>在经过随机采样之后，我们可能得到某些区域模型的表现较好。然而，为了得到更精确的最佳参数，我们应该继续对选定的区域进行由粗到细的采样（coarse to fine sampling scheme）。也就是放大表现较好的区域，再对此区域做更密集的随机采样。例如，对下图中右下角的方形区域再做25点的随机采样，以获得最佳参数。</p><p><img src="http://img.blog.csdn.net/20171031230142816?" alt="这里写图片描述"></p><h3 id="Using-an-appropriate-scale-to-pick-hyperparameters"><a href="#Using-an-appropriate-scale-to-pick-hyperparameters" class="headerlink" title="Using an appropriate scale to pick hyperparameters"></a>Using an appropriate scale to pick hyperparameters</h3><p>上一部分讲的调试参数使用随机采样，对于某些超参数是可以进行尺度均匀采样的，但是某些超参数需要选择不同的合适尺度进行随机采样。</p><p>什么意思呢？例如对于超参数#layers和#hidden units，都是正整数，是可以进行均匀随机采样的，即超参数每次变化的尺度都是一致的（如每次变化为1，犹如一个刻度尺一样，刻度是均匀的）。</p><p>但是，对于某些超参数，可能需要非均匀随机采样（即非均匀刻度尺）。例如超参数$\alpha$，待调范围是[0.0001, 1]。如果使用均匀随机采样，那么有90%的采样点分布在[0.1, 1]之间，只有10%分布在[0.0001, 0.1]之间。这在实际应用中是不太好的，因为最佳的$\alpha$值可能主要分布在[0.0001, 0.1]之间，而[0.1, 1]范围内$\alpha$值效果并不好。因此我们更关注的是区间[0.0001, 0.1]，应该在这个区间内细分更多刻度。</p><p>通常的做法是将linear scale转换为log scale，将均匀尺度转化为非均匀尺度，然后再在log scale下进行均匀采样。这样，[0.0001, 0.001]，[0.001, 0.01]，[0.01, 0.1]，[0.1, 1]各个区间内随机采样的超参数个数基本一致，也就扩大了之前[0.0001, 0.1]区间内采样值个数。</p><p><img src="http://img.blog.csdn.net/20171101094754376?" alt="这里写图片描述"></p><p>一般解法是，如果线性区间为[a, b]，令m=log(a)，n=log(b)，则对应的log区间为[m,n]。对log区间的[m,n]进行随机均匀采样，然后得到的采样值r，最后反推到线性区间，即$10^r$。$10^r$就是最终采样的超参数。相应的Python语句为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = np.log10(a)</span><br><span class="line">n = np.log10(b)</span><br><span class="line">r = np.random.rand()</span><br><span class="line">r = m + (n-m)*r</span><br><span class="line">r = np.power(<span class="number">10</span>,r)</span><br></pre></td></tr></table></figure><p>除了$\alpha$之外，动量梯度因子$\beta$也是一样，在超参数调试的时候也需要进行非均匀采样。一般$\beta$的取值范围在[0.9, 0.999]之间，那么$1-\beta$的取值范围就在[0.001, 0.1]之间。那么直接对$1-\beta$在[0.001, 0.1]区间内进行log变换即可。</p><p>这里解释下为什么$\beta$也需要向$\alpha$那样做非均匀采样。假设$\beta$从0.9000变化为0.9005，那么$\frac{1}{1-\beta}$基本没有变化。但假设$\beta$从0.9990变化为0.9995，那么$\frac{1}{1-\beta}$前后差别1000。$\beta$越接近1，指数加权平均的个数越多，变化越大。所以对$\beta$接近1的区间，应该采集得更密集一些。</p><h3 id="Hyperparameters-tuning-in-practice-Pandas-vs-Caviar"><a href="#Hyperparameters-tuning-in-practice-Pandas-vs-Caviar" class="headerlink" title="Hyperparameters tuning in practice: Pandas vs. Caviar"></a>Hyperparameters tuning in practice: Pandas vs. Caviar</h3><p>经过调试选择完最佳的超参数并不是一成不变的，一段时间之后（例如一个月），需要根据新的数据和实际情况，再次调试超参数，以获得实时的最佳模型。</p><p>在训练深度神经网络时，一种情况是受计算能力所限，我们只能对一个模型进行训练，调试不同的超参数，使得这个模型有最佳的表现。我们称之为Babysitting one model。另外一种情况是可以对多个模型同时进行训练，每个模型上调试不同的超参数，根据表现情况，选择最佳的模型。我们称之为Training many models in parallel。</p><p><img src="http://img.blog.csdn.net/20171101171005196?" alt="这里写图片描述"></p><p>因为第一种情况只使用一个模型，所以类比做Panda approach；第二种情况同时训练多个模型，类比做Caviar approach。使用哪种模型是由计算资源、计算能力所决定的。一般来说，对于非常复杂或者数据量很大的模型，使用Panda approach更多一些。</p><h3 id="Normalizing-activations-in-a-network"><a href="#Normalizing-activations-in-a-network" class="headerlink" title="Normalizing activations in a network"></a>Normalizing activations in a network</h3><p>Sergey Ioffe和Christian Szegedy两位学者提出了Batch Normalization方法。Batch Normalization不仅可以让调试超参数更加简单，而且可以让神经网络模型更加“健壮”。也就是说较好模型可接受的超参数范围更大一些，包容性更强，使得更容易去训练一个深度神经网络。接下来，我们就来介绍什么是Batch Normalization，以及它是如何工作的。</p><p>之前，我们在<a href="http://blog.csdn.net/red_stone1/article/details/78208851" target="_blank" rel="noopener">Coursera吴恩达《优化深度神经网络》课程笔记（1）– 深度学习的实用层面</a>中提到过在训练神经网络时，标准化输入可以提高训练的速度。方法是对训练数据集进行归一化的操作，即将原始数据减去其均值$\mu$后，再除以其方差$\sigma^2$。但是标准化输入只是对输入进行了处理，那么对于神经网络，又该如何对各隐藏层的输入进行标准化处理呢？</p><p>其实在神经网络中，第$l$层隐藏层的输入就是第$l-1$层隐藏层的输出$A^{[l-1]}$。对$A^{[l-1]}$进行标准化处理，从原理上来说可以提高$W^{[l]}$和$b^{[l]}$的训练速度和准确度。这种对各隐藏层的标准化处理就是Batch Normalization。值得注意的是，实际应用中，一般是对$Z^{[l-1]}$进行标准化处理而不是$A^{[l-1]}$，其实差别不是很大。</p><p>Batch Normalization对第$l$层隐藏层的输入$Z^{[l-1]}$做如下标准化处理，忽略上标$[l-1]$：</p><p>$$\mu=\frac1m\sum_iz^{(i)}$$</p><p>$$\sigma^2=\frac1m\sum_i(z_i-\mu)^2$$</p><p>$$z^{(i)}_{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}}$$</p><p>其中，m是单个mini-batch包含样本个数，$\varepsilon$是为了防止分母为零，可取值$10^{-8}$。这样，使得该隐藏层的所有输入$z^{(i)}$均值为0，方差为1。</p><p>但是，大部分情况下并不希望所有的$z^{(i)}$均值都为0，方差都为1，也不太合理。通常需要对$z^{(i)}$进行进一步处理：</p><p>$$\tilde z^{(i)}=\gamma\cdot z^{(i)}_{norm}+\beta$$</p><p>上式中，$\gamma$和$\beta$是learnable parameters，类似于W和b一样，可以通过梯度下降等算法求得。这里，$\gamma$和$\beta$的作用是让$\tilde z^{(i)}$的均值和方差为任意值，只需调整其值就可以了。例如，令：</p><p>$$\gamma=\sqrt{\sigma^2+\varepsilon},\ \ \beta=u$$</p><p>则$\tilde z^{(i)}=z^{(i)}$，即identity function。可见，设置$\gamma$和$\beta$为不同的值，可以得到任意的均值和方差。</p><p>这样，通过Batch Normalization，对隐藏层的各个$z^{[l](i)}$进行标准化处理，得到$\tilde z^{[l](i)}$，替代$z^{[l](i)}$。</p><p>值得注意的是，输入的标准化处理Normalizing inputs和隐藏层的标准化处理Batch Normalization是有区别的。Normalizing inputs使所有输入的均值为0，方差为1。而Batch Normalization可使各隐藏层输入的均值和方差为任意值。实际上，从激活函数的角度来说，如果各隐藏层的输入均值在靠近0的区域即处于激活函数的线性区域，这样不利于训练好的非线性神经网络，得到的模型效果也不会太好。这也解释了为什么需要用$\gamma$和$\beta$来对$z^{<a href="i">l</a>}$作进一步处理。</p><h3 id="Fitting-Batch-Norm-into-a-neural-network"><a href="#Fitting-Batch-Norm-into-a-neural-network" class="headerlink" title="Fitting Batch Norm into a neural network"></a>Fitting Batch Norm into a neural network</h3><p>我们已经知道了如何对某单一隐藏层的所有神经元进行Batch Norm，接下来将研究如何把Bath Norm应用到整个神经网络中。</p><p>对于L层神经网络，经过Batch Norm的作用，整体流程如下：</p><p><img src="http://img.blog.csdn.net/20171102090304433?" alt="这里写图片描述"></p><p>实际上，Batch Norm经常使用在mini-batch上，这也是其名称的由来。</p><p>值得注意的是，因为Batch Norm对各隐藏层$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$有去均值的操作，所以这里的常数项$b^{[l]}$可以消去，其数值效果完全可以由$\tilde Z^{[l]}$中的$\beta$来实现。因此，我们在使用Batch Norm的时候，可以忽略各隐藏层的常数项$b^{[l]}$。在使用梯度下降算法时，分别对$W^{[l]}$，$\beta^{[l]}$和$\gamma^{[l]}$进行迭代更新。除了传统的梯度下降算法之外，还可以使用我们之前介绍过的动量梯度下降、RMSprop或者Adam等优化算法。</p><h3 id="Why-does-Batch-Norm-work？"><a href="#Why-does-Batch-Norm-work？" class="headerlink" title="Why does Batch Norm work？"></a>Why does Batch Norm work？</h3><p>我们可以把输入特征做均值为0，方差为1的规范化处理，来加快学习速度。而Batch Norm也是对隐藏层各神经元的输入做类似的规范化处理。总的来说，Batch Norm不仅能够提高神经网络训练速度，而且能让神经网络的权重W的更新更加“稳健”，尤其在深层神经网络中更加明显。比如神经网络很后面的W对前面的W包容性更强，即前面的W的变化对后面W造成的影响很小，整体网络更加健壮。</p><p>举个例子来说明，假如用一个浅层神经网络（类似逻辑回归）来训练识别猫的模型。如下图所示，提供的所有猫的训练样本都是黑猫。然后，用这个训练得到的模型来对各种颜色的猫样本进行测试，测试的结果可能并不好。其原因是训练样本不具有一般性（即不是所有的猫都是黑猫），这种训练样本（黑猫）和测试样本（猫）分布的变化称之为covariate shift。</p><p><img src="http://img.blog.csdn.net/20171108204845005?" alt="这里写图片描述"></p><p>对于这种情况，如果实际应用的样本与训练样本分布不同，即发生了covariate shift，则一般是要对模型重新进行训练的。在神经网络，尤其是深度神经网络中，covariate shift会导致模型预测效果变差，重新训练的模型各隐藏层的$W^{[l]}$和$B^{[l]}$均产生偏移、变化。而Batch Norm的作用恰恰是减小covariate shift的影响，让模型变得更加健壮，鲁棒性更强。Batch Norm减少了各层$W^{[l]}$、$B^{[l]}$之间的耦合性，让各层更加独立，实现自我训练学习的效果。也就是说，如果输入发生covariate shift，那么因为Batch Norm的作用，对个隐藏层输出$Z^{[l]}$进行均值和方差的归一化处理，$W^{[l]}$和$B^{[l]}$更加稳定，使得原来的模型也有不错的表现。针对上面这个黑猫的例子，如果我们使用深层神经网络，使用Batch Norm，那么该模型对花猫的识别能力应该也是不错的。</p><p>从另一个方面来说，Batch Norm也起到轻微的正则化（regularization）效果。具体表现在：</p><ul><li><p><strong>每个mini-batch都进行均值为0，方差为1的归一化操作</strong></p></li><li><p><strong>每个mini-batch中，对各个隐藏层的$Z^{[l]}$添加了随机噪声，效果类似于Dropout</strong></p></li><li><p><strong>mini-batch越小，正则化效果越明显</strong></p></li></ul><p>但是，Batch Norm的正则化效果比较微弱，正则化也不是Batch Norm的主要功能。</p><h3 id="Batch-Norm-at-test-time"><a href="#Batch-Norm-at-test-time" class="headerlink" title="Batch Norm at test time"></a>Batch Norm at test time</h3><p>训练过程中，Batch Norm是对单个mini-batch进行操作的，但在测试过程中，如果是单个样本，该如何使用Batch Norm进行处理呢？</p><p>首先，回顾一下训练过程中Batch Norm的主要过程：</p><p>$$\mu=\frac1m\sum_iz^{(i)}$$</p><p>$$\sigma^2=\frac1m\sum_i(z^{(i)}-\mu)^2$$</p><p>$$z_{norm}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}}$$</p><p>$$\tilde z^{(i)}=\gamma\cdot z^{(i)}_{norm}+\beta$$</p><p>其中，$\mu$和$\sigma^2$是对单个mini-batch中所有m个样本求得的。在测试过程中，如果只有一个样本，求其均值和方差是没有意义的，就需要对$\mu$和$\sigma^2$进行估计。估计的方法有很多，理论上我们可以将所有训练集放入最终的神经网络模型中，然后将每个隐藏层计算得到的$\mu^{[l]}$和$\sigma^{2[l]}$直接作为测试过程的$\mu$和$\sigma^2$来使用。但是，实际应用中一般不使用这种方法，而是使用我们之前介绍过的指数加权平均（exponentially weighted average）的方法来预测测试过程单个样本的$\mu$和$\sigma^2$。</p><p>指数加权平均的做法很简单，对于第$l$层隐藏层，考虑所有mini-batch在该隐藏层下的$\mu^{[l]}$和$\sigma^{2[l]}$，然后用指数加权平均的方式来预测得到当前单个样本的$\mu^{[l]}$和$\sigma^{2[l]}$。这样就实现了对测试过程单个样本的均值和方差估计。最后，再利用训练过程得到的$\gamma$和$\beta$值计算出各层的$\tilde z^{(i)}$值。</p><h3 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h3><p>目前我们介绍的都是二分类问题，神经网络输出层只有一个神经元，表示预测输出$\hat y$是正类的概率$P(y=1|x)$，$\hat y&gt;0.5$则判断为正类，$\hat y&lt;0.5$则判断为负类。</p><p>对于多分类问题，用C表示种类个数，神经网络中输出层就有C个神经元，即$n^{[L]}=C$。其中，每个神经元的输出依次对应属于该类的概率，即$P(y=c|x)$。为了处理多分类问题，我们一般使用Softmax回归模型。Softmax回归模型输出层的激活函数如下所示：</p><p>$$z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]}$$</p><p>$$a^{[L]}_i=\frac{e^{z^{[L]}<em>i}}{\sum</em>{i=1}^Ce^{z^{[L]}_i}}$$</p><p>输出层每个神经元的输出$a^{[L]}_i$对应属于该类的概率，满足：</p><p>$$\sum_{i=1}^Ca^{[L]}_i=1$$</p><p>所有的$a^{[L]}_i$，即$\hat y$，维度为(C, 1)。</p><p>下面给出几个简单的线性多分类的例子：</p><p><img src="http://img.blog.csdn.net/20171109212043740?" alt="这里写图片描述"></p><p>如果使用神经网络，特别是深层神经网络，可以得到更复杂、更精确的非线性模型。</p><h3 id="Training-a-softmax-classifier"><a href="#Training-a-softmax-classifier" class="headerlink" title="Training a softmax classifier"></a>Training a softmax classifier</h3><p>Softmax classifier的训练过程与我们之前介绍的二元分类问题有所不同。先来看一下softmax classifier的loss function。举例来说，假如C=4，某个样本的预测输出$\hat y$和真实输出$y$为：</p><p>$$<br> \hat y=\left[<br> \begin{matrix}<br>   0.3 \<br>   0.2 \<br>   0.1 \<br>   0.4<br>  \end{matrix}<br>  \right]<br>$$</p><p>$$<br> y=\left[<br> \begin{matrix}<br>   0 \<br>   1 \<br>   0 \<br>   0<br>  \end{matrix}<br>  \right]<br>$$</p><p>从$\hat y$值来看，$P(y=4|x)=0.4$，概率最大，而真实样本属于第2类，因此该预测效果不佳。我们定义softmax classifier的loss function为：</p><p>$$L(\hat y,y)=-\sum_{j=1}^4y_j\cdot log\ \hat y_j$$</p><p>然而，由于只有当$j=2$时，$y_2=1$，其它情况下，$y_j=0$。所以，上式中的$L(\hat y,y)$可以简化为：</p><p>$$L(\hat y,y)=-y_2\cdot log\ \hat y_2=-log\ \hat y_2$$</p><p>要让$L(\hat y,y)$更小，就应该让$\hat y_2$越大越好。$\hat y_2$反映的是概率，完全符合我们之前的定义。</p><p>所有m个样本的cost function为：</p><p>$$J=\frac1m\sum_{i=1}^mL(\hat y,y)$$</p><p>其预测输出向量$A^{[L]}$即$\hat Y$的维度为(4, m)。</p><p>softmax classifier的反向传播过程仍然使用梯度下降算法，其推导过程与二元分类有一点点不一样。因为只有输出层的激活函数不一样，我们先推导$dZ^{[L]}$：</p><p>$$da^{[L]}=-\frac{1}{a^{[L]}}$$</p><p>$$\frac{\partial a^{[L]}}{\partial z^{[L]}}=\frac{\partial}{\partial z^{[L]}}\cdot (\frac{e^{z^{[L]}_i}}{\sum_i^Ce^{z^{[L]}_i}})=a^{[L]}\cdot (1-a^{[L]})$$</p><p>$$dz^{[L]}=da^{[L]}\cdot \frac{\partial a^{[L]}}{\partial z^{[L]}}=a^{[L]}-1=a^{[L]}-y$$</p><p>对于所有m个训练样本：</p><p>$$dZ^{[L]}=A^{[L]}-Y$$</p><p>可见$dZ^{[L]}$的表达式与二元分类结果是一致的，虽然推导过程不太一样。然后就可以继续进行反向传播过程的梯度下降算法了，推导过程与二元分类神经网络完全一致。</p><h3 id="Deep-learning-frameworks"><a href="#Deep-learning-frameworks" class="headerlink" title="Deep learning frameworks"></a>Deep learning frameworks</h3><p>深度学习框架有很多，例如：</p><ul><li><p><strong>Caffe/Caffe2</strong></p></li><li><p><strong>CNTK</strong></p></li><li><p><strong>DL4J</strong></p></li><li><p><strong>Keras</strong></p></li><li><p><strong>Lasagne</strong></p></li><li><p><strong>mxnet</strong></p></li><li><p><strong>PaddlePaddle</strong></p></li><li><p><strong>TensorFlow</strong></p></li><li><p><strong>Theano</strong></p></li><li><p><strong>Torch</strong></p></li></ul><p>一般选择深度学习框架的基本准则是：</p><ul><li><p><strong>Ease of programming(development and deployment)</strong></p></li><li><p><strong>Running speed</strong></p></li><li><p><strong>Truly open(open source with good governance)</strong></p></li></ul><p>实际应用中，我们应该根据自己的需求选择最合适的深度学习框架。</p><h3 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h3><p>这里简单介绍一下最近几年比较火的一个深度学习框架：TensorFlow。</p><p>举个例子来说明，例如cost function是参数w的函数：</p><p>$$J=w^2-10w+25$$</p><p>如果使用TensorFlow对cost function进行优化，求出最小值对应的w，程序如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype=tf.float32)</span><br><span class="line"><span class="comment">#cost = tf.add(tf.add(w**2,tf.multiply(-10,w)),25)</span></span><br><span class="line">cost = w**<span class="number">2</span> - <span class="number">10</span>*w +<span class="number">25</span></span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure><p>>&gt;0.0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session.run(train)</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure><p>>&gt;0.1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">session.run(train)</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure><p>>&gt;4.99999</p><p>TensorFlow框架内可以直接调用梯度下降优化算法，不需要我们自己再写程序了，大大提高了效率。在运行1000次梯度下降算法后，w的解为4.99999，已经非常接近w的最优值5了。</p><p>针对上面这个例子，如果对w前的系数用变量x来代替，程序如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">cofficients = np.array([[<span class="number">1.</span>],[<span class="number">-10.</span>],[<span class="number">25.</span>]])</span><br><span class="line"></span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype=tf.float32)</span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment">#cost = tf.add(tf.add(w**2,tf.multiply(-10,w)),25)</span></span><br><span class="line"><span class="comment">#cost = w**2 - 10*w +25</span></span><br><span class="line">cost = x[<span class="number">0</span>][<span class="number">0</span>]*w**<span class="number">2</span> + x[<span class="number">1</span>][<span class="number">0</span>]*w + x[<span class="number">2</span>][<span class="number">0</span>]</span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure><p>>&gt;0.0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session.run(train, feed_dict=(x:coefficients))</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure><p>>&gt;0.1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">session.run(train, feed_dict=(x:coefficients))</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure><p>>&gt;4.99999</p><p>结果跟之前是一样的。除此之外，我们还可以更改x即cofficients的值，而得到不同的优化结果w。</p><p>另外，上段程序中的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure><p>有另外一种写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">session.run(init)</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure><p>TensorFlow的最大优点就是采用数据流图（data flow graphs）来进行数值运算。图中的节点（Nodes）表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。而且它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。</p><p>关于TensorFlow更多的原理和编程技巧这里就不在赘述了，感兴趣的朋友可以关注更详细的TensorFlow相关文档。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20171031230142816?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达优化深度神经网络" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《优化深度神经网络》课程笔记（2）-- 优化算法</title>
    <link href="https://redstonewill.github.io/2018/03/29/40/"/>
    <id>https://redstonewill.github.io/2018/03/29/40/</id>
    <published>2018-03-29T12:58:17.000Z</published>
    <updated>2018-03-29T13:18:36.308Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20171028232352807?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了如何建立一个实用的深度学习神经网络。包括Train/Dev/Test sets的比例选择，Bias和Variance的概念和区别：Bias对应欠拟合，Variance对应过拟合。接着，我们介绍了防止过拟合的两种方法：L2 regularization和Dropout。然后，介绍了如何进行规范化输入，以加快梯度下降速度和精度。然后，我们介绍了梯度消失和梯度爆炸的概念和危害，并提出了如何使用梯度初始化来降低这种风险。最后，我们介绍了梯度检查，来验证梯度下降算法是否正确。本节课，我们将继续讨论深度神经网络中的一些优化算法，通过使用这些技巧和方法来提高神经网络的训练速度和精度。</p><h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><p>之前我们介绍的神经网络训练过程是对所有m个样本，称为batch，通过向量化计算方式，同时进行的。如果m很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。我们将这种梯度下降算法称为Batch Gradient Descent。</p><p>为了解决这一问题，我们可以把m个训练样本分成若干个子集，称为mini-batches，这样每个子集包含的数据量就小了，例如只有1000，然后每次在单一子集上进行神经网络训练，速度就会大大提高。这种梯度下降算法叫做Mini-batch Gradient Descent。</p><p>假设总的训练样本个数m=5000000，其维度为$(n_x,m)$。将其分成5000个子集，每个mini-batch含有1000个样本。我们将每个mini-batch记为$X^{\{t\}}$，其维度为$(n_x,1000)$。相应的每个mini-batch的输出记为$Y^{\{t\}}$，其维度为$(1,1000)$，且$t=1,2,\cdots,5000$。</p><p>这里顺便总结一下我们遇到的神经网络中几类字母的上标含义：</p><ul><li><p><strong>$X^{(i)}$ ：第i个样本</strong></p></li><li><p><strong>$Z^{[l]}$：神经网络第$l$层网络的线性输出</strong></p></li><li><p><strong>$X^{\{t\}},Y^{\{t\}}$：第t组mini-batch</strong></p></li></ul><p>Mini-batches Gradient Descent的实现过程是先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕。</p><p>$for\ \ t=1,\cdots,T\ \ \{$</p><p>$\ \ \ \ Forward\ Propagation$</p><p>$\ \ \ \ Compute Cost Function$</p><p>$\ \ \ \ Backward Propagation$</p><p>$\ \ \ \ W:=W-\alpha\cdot dW$</p><p>$\ \ \ \ b:=b-\alpha\cdot db$</p><p>$\}$</p><p>经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个epoch。对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法。</p><p>值得一提的是，对于Mini-Batches Gradient Descent，可以进行多次epoch训练。而且，每次epoch，最好是将总体训练数据重新打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型。</p><h3 id="Understanding-mini-batch-gradient-descent"><a href="#Understanding-mini-batch-gradient-descent" class="headerlink" title="Understanding mini-batch gradient descent"></a>Understanding mini-batch gradient descent</h3><p>Batch gradient descent和Mini-batch gradient descent的cost曲线如下图所示：</p><p><img src="http://img.blog.csdn.net/20171026113219156?" alt="这里写图片描述"></p><p>对于一般的神经网络模型，使用Batch gradient descent，随着迭代次数增加，cost是不断减小的。然而，使用Mini-batch gradient descent，随着在不同的mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值。</p><p>之所以出现细微振荡的原因是不同的mini-batch之间是有差异的。例如可能第一个子集$(X^1,Y^1)$是好的子集，而第二个子集$(X^2,Y^2)$包含了一些噪声noise。出现细微振荡是正常的。</p><p>如何选择每个mini-batch的大小，即包含的样本个数呢？有两个极端：如果mini-batch size=m，即为Batch gradient descent，只包含一个子集为$(X^1,Y^1)=(X,Y)$；如果mini-batch size=1，即为Stachastic gradient descent，每个样本就是一个子集$(X^1,Y^1)=(x^{(i)},y^{(i)})$，共有m个子集。</p><p>我们来比较一下Batch gradient descent和Stachastic gradient descent的梯度下降曲线。如下图所示，蓝色的线代表Batch gradient descent，紫色的线代表Stachastic gradient descent。Batch gradient descent会比较平稳地接近全局最小值，但是因为使用了所有m个样本，每次前进的速度有些慢。Stachastic gradient descent每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。</p><p><img src="http://img.blog.csdn.net/20171026135131370?" alt="这里写图片描述"></p><p>实际使用中，mini-batch size不能设置得太大（Batch gradient descent），也不能设置得太小（Stachastic gradient descent）。这样，相当于结合了Batch gradient descent和Stachastic gradient descent各自的优点，既能使用向量化优化算法，又能叫快速地找到最小值。mini-batch gradient descent的梯度下降曲线如下图绿色所示，每次前进速度较快，且振荡较小，基本能接近全局最小值。</p><p><img src="http://img.blog.csdn.net/20171026140312033?" alt="这里写图片描述"></p><p>一般来说，如果总体样本数量m不太大时，例如$m\leq2000$，建议直接使用Batch gradient descent。如果总体样本数量m很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch size为64,128,256,512。这些都是2的幂。之所以这样设置的原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度。</p><h3 id="Exponentially-weighted-averages"><a href="#Exponentially-weighted-averages" class="headerlink" title="Exponentially weighted averages"></a>Exponentially weighted averages</h3><p>该部分我们将介绍指数加权平均（Exponentially weighted averages）的概念。</p><p>举个例子，记录半年内伦敦市的气温变化，并在二维平面上绘制出来，如下图所示：</p><p><img src="http://img.blog.csdn.net/20171026145118050?" alt="这里写图片描述"></p><p>看上去，温度数据似乎有noise，而且抖动较大。如果我们希望看到半年内气温的整体变化趋势，可以通过移动平均（moving average）的方法来对每天气温进行平滑处理。</p><p>例如我们可以设$V_0=0$，当成第0天的气温值。</p><p>第一天的气温与第0天的气温有关：</p><p>$$V_1=0.9V_0+0.1\theta_1$$</p><p>第二天的气温与第一天的气温有关：</p><p>$$\begin{eqnarray}V_2<br>&amp;=&amp;0.9V_1+0.1\theta_2\<br>&amp;=&amp;0.9(0.9V_0+0.1\theta_1)+0.1\theta_2\<br>&amp;=&amp;0.9^2V_0+0.9\cdot0.1\theta_1+0.1\theta_2<br>\end{eqnarray}$$</p><p>第三天的气温与第二天的气温有关：</p><p>$$\begin{eqnarray}V_3<br>&amp;=&amp;0.9V_2+0.1\theta_3\<br>&amp;=&amp;0.9(0.9^2V_0+0.9\cdot0.1\theta_1+0.1\theta_2)+0.1\theta_3\<br>&amp;=&amp;0.9^3V_0+0.9^2\cdot 0.1\theta_1+0.9\cdot 0.1\theta_2+0.1\theta_3<br>\end{eqnarray}$$</p><p>即第t天与第t-1天的气温迭代关系为：</p><p>$$\begin{eqnarray}V_t<br>&amp;=&amp;0.9V_{t-1}+0.1\theta_t\<br>&amp;=&amp;0.9^tV_0+0.9^{t-1}\cdot0.1\theta_1+0.9^{t-2}\cdot 0.1\theta_2+\cdots+0.9\cdot0.1\theta_{t-1}+0.1\theta_t<br>\end{eqnarray}$$</p><p>经过移动平均处理得到的气温如下图红色曲线所示：</p><p><img src="http://img.blog.csdn.net/20171027142418881?" alt="这里写图片描述"></p><p>这种滑动平均算法称为指数加权平均（exponentially weighted average）。根据之前的推导公式，其一般形式为：</p><p>$$V_t=\beta V_{t-1}+(1-\beta)\theta_t$$</p><p>上面的例子中，$\beta=0.9$。$\beta$值决定了指数加权平均的天数，近似表示为：</p><p>$$\frac{1}{1-\beta}$$</p><p>例如，当$\beta=0.9$，则$\frac{1}{1-\beta}=10$，表示将前10天进行指数加权平均。当$\beta=0.98$，则$\frac{1}{1-\beta}=50$，表示将前50天进行指数加权平均。$\beta$值越大，则指数加权平均的天数越多，平均后的趋势线就越平缓，但是同时也会向右平移。下图绿色曲线和黄色曲线分别表示了$\beta=0.98$和$\beta=0.5$时，指数加权平均的结果。</p><p><img src="http://img.blog.csdn.net/20171027150509634?" alt="这里写图片描述"></p><p>这里简单解释一下公式$\frac{1}{1-\beta}$是怎么来的。准确来说，指数加权平均算法跟之前所有天的数值都有关系，根据之前的推导公式就能看出。但是指数是衰减的，一般认为衰减到$\frac1e$就可以忽略不计了。因此，根据之前的推导公式，我们只要证明</p><p>$$\beta^{\frac{1}{1-\beta}}=\frac1e$$</p><p>就好了。</p><p>令$\frac{1}{1-\beta}=N$，$N&gt;0$，则$\beta=1-\frac{1}{N}$，$\frac1N&lt;1$。即证明转化为：</p><p>$$(1-\frac1N)^N=\frac1e$$</p><p>显然，当$N&gt;&gt;0$时，上述等式是近似成立的。</p><p>至此，简单解释了为什么指数加权平均的天数的计算公式为$\frac{1}{1-\beta}$。</p><h3 id="Understanding-exponetially-weighted-averages"><a href="#Understanding-exponetially-weighted-averages" class="headerlink" title="Understanding exponetially weighted averages"></a>Understanding exponetially weighted averages</h3><p>我们将指数加权平均公式的一般形式写下来：</p><p>$$\begin{eqnarray}V_t<br>&amp;=&amp;\beta V_{t-1}+(1-\beta)\theta_t\<br>&amp;=&amp;(1-\beta)\theta_t+(1-\beta)\cdot\beta\cdot\theta_{t-1}+(1-\beta)\cdot \beta^2\cdot\theta_{t-2}+\cdots\<br>&amp;&amp;+(1-\beta)\cdot \beta^{t-1}\cdot \theta_1+\beta^t\cdot V_0<br>\end{eqnarray}$$</p><p>观察上面这个式子，$\theta_t,\theta_{t-1},\theta_{t-2},\cdots,\theta_1$是原始数据值，$(1-\beta),(1-\beta)\beta,(1-\beta)\beta^2,\cdots,(1-\beta)\beta^{t-1}$是类似指数曲线，从右向左，呈指数下降的。$V_t$的值就是这两个子式的点乘，将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害。</p><p><img src="http://img.blog.csdn.net/20171027155944527?" alt="这里写图片描述"></p><p>我们已经知道了指数加权平均的递推公式。实际应用中，为了减少内存的使用，我们可以使用这样的语句来实现指数加权平均算法：</p><p>$V_{\theta}=0$</p><p>$Repeat\ \{$</p><p>$\ \ \ \ Get\ next\ \theta_t$</p><p>$\ \ \ \ V_{\theta}:=\beta V_{\theta}+(1-\beta)\theta_t$</p><p>$\}$</p><h3 id="Bias-correction-in-exponentially-weighted-average"><a href="#Bias-correction-in-exponentially-weighted-average" class="headerlink" title="Bias correction in exponentially weighted average"></a>Bias correction in exponentially weighted average</h3><p>上文中提到当$\beta=0.98$时，指数加权平均结果如下图绿色曲线所示。但是实际上，真实曲线如紫色曲线所示。</p><p><img src="http://img.blog.csdn.net/20171028095447301?" alt="这里写图片描述"></p><p>我们注意到，紫色曲线与绿色曲线的区别是，紫色曲线开始的时候相对较低一些。这是因为开始时我们设置$V_0=0$，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常。</p><p>修正这种问题的方法是进行偏移校正（bias correction），即在每次计算完$V_t$后，对$V_t$进行下式处理：</p><p>$$\frac{V_t}{1-\beta^t}$$</p><p>在刚开始的时候，t比较小，$(1-\beta^t)&lt;1$，这样就将$V_t$修正得更大一些，效果是把紫色曲线开始部分向上提升一些，与绿色曲线接近重合。随着t增大，$(1-\beta^t)\approx1$，$V_t$基本不变，紫色曲线与绿色曲线依然重合。这样就实现了简单的偏移校正，得到我们希望的绿色曲线。</p><p>值得一提的是，机器学习中，偏移校正并不是必须的。因为，在迭代一次次数后（t较大），$V_t$受初始值影响微乎其微，紫色曲线与绿色曲线基本重合。所以，一般可以忽略初始迭代过程，等到一定迭代之后再取值，这样就不需要进行偏移校正了。</p><h3 id="Gradient-descent-with-momentum"><a href="#Gradient-descent-with-momentum" class="headerlink" title="Gradient descent with momentum"></a>Gradient descent with momentum</h3><p>该部分将介绍动量梯度下降算法，其速度要比传统的梯度下降算法快很多。做法是在每次训练时，对梯度进行指数加权平均处理，然后用得到的梯度值更新权重W和常数项b。下面介绍具体的实现过程。</p><p><img src="http://img.blog.csdn.net/20171028142838569?" alt="这里写图片描述"></p><p>原始的梯度下降算法如上图蓝色折线所示。在梯度下降过程中，梯度下降的振荡较大，尤其对于W、b之间数值范围差别较大的情况。此时每一点处的梯度只与当前方向有关，产生类似折线的效果，前进缓慢。而如果对梯度进行指数加权平均，这样使当前梯度不仅与当前方向有关，还与之前的方向有关，这样处理让梯度前进方向更加平滑，减少振荡，能够更快地到达最小值处。</p><p>权重W和常数项b的指数加权平均表达式如下：</p><p>$$V_{dW}=\beta\cdot V_{dW}+(1-\beta)\cdot dW$$</p><p>$$V_{db}=\beta\cdot V_{db}+(1-\beta)\cdot db$$</p><p>从动量的角度来看，以权重W为例，$V_{dW}$可以成速度V，$dW$可以看成是加速度a。指数加权平均实际上是计算当前的速度，当前速度由之前的速度和现在的加速度共同影响。而$\beta&lt;1$，又能限制速度$V_{dW}$过大。也就是说，当前的速度是渐变的，而不是瞬变的，是动量的过程。这保证了梯度下降的平稳性和准确性，减少振荡，较快地达到最小值处。</p><p>动量梯度下降算法的过程如下：</p><p>$On\ iteration\ t:$</p><p>$\ \ \ \ Compute\ dW,\ db\ on\ the\ current\ mini-batch$</p><p>$\ \ \ \ V_{dW}=\beta V_{dW}+(1-\beta)dW$</p><p>$\ \ \ \ V_{db}=\beta V_{db}+(1-\beta)db$</p><p>$\ \ \ \ W=W-\alpha V_{dW},\ b=b-\alpha V_{db}$</p><p>初始时，令$V_{dW}=0,V_{db}=0$。一般设置$\beta=0.9$，即指数加权平均前10天的数据，实际应用效果较好。</p><p>另外，关于偏移校正，可以不使用。因为经过10次迭代后，随着滑动平均的过程，偏移情况会逐渐消失。</p><p>补充一下，在其它文献资料中，动量梯度下降还有另外一种写法：</p><p>$$V_{dW}=\beta V_{dW}+dW$$</p><p>$$V_{db}=\beta V_{db}+db$$</p><p>即消去了$dW$和$db$前的系数$(1-\beta)$。这样简化了表达式，但是学习因子$\alpha$相当于变成了$\frac{\alpha}{1-\beta}$，表示$\alpha$也受$\beta$的影响。从效果上来说，这种写法也是可以的，但是不够直观，且调参涉及到$\alpha$，不够方便。所以，实际应用中，推荐第一种动量梯度下降的表达式。</p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>RMSprop是另外一种优化梯度下降速度的算法。每次迭代训练过程中，其权重W和常数项b的更新表达式为：</p><p>$$S_W=\beta S_{dW}+(1-\beta)dW^2$$</p><p>$$S_b=\beta S_{db}+(1-\beta)db^2$$</p><p>$$W:=W-\alpha \frac{dW}{\sqrt{S_W}},\ b:=b-\alpha \frac{db}{\sqrt{S_b}}$$</p><p>下面简单解释一下RMSprop算法的原理，仍然以下图为例，为了便于分析，令水平方向为W的方向，垂直方向为b的方向。</p><p><img src="http://img.blog.csdn.net/20171028163526337?" alt="这里写图片描述"></p><p>从图中可以看出，梯度下降（蓝色折线）在垂直方向（b）上振荡较大，在水平方向（W）上振荡较小，表示在b方向上梯度较大，即$db$较大，而在W方向上梯度较小，即$dW$较小。因此，上述表达式中$S_b$较大，而$S_W$较小。在更新W和b的表达式中，变化值$\frac{dW}{\sqrt{S_W}}$较大，而$\frac{db}{\sqrt{S_b}}$较小。也就使得W变化得多一些，b变化得少一些。即加快了W方向的速度，减小了b方向的速度，减小振荡，实现快速梯度下降算法，其梯度下降过程如绿色折线所示。总得来说，就是如果哪个方向振荡大，就减小该方向的更新速度，从而减小振荡。</p><p>还有一点需要注意的是为了避免RMSprop算法中分母为零，通常可以在分母增加一个极小的常数$\varepsilon$：</p><p>$$W:=W-\alpha \frac{dW}{\sqrt{S_W}+\varepsilon},\ b:=b-\alpha \frac{db}{\sqrt{S_b}+\varepsilon}$$</p><p>其中，$\varepsilon=10^{-8}$，或者其它较小值。</p><h3 id="Adam-optimization-algorithm"><a href="#Adam-optimization-algorithm" class="headerlink" title="Adam optimization algorithm"></a>Adam optimization algorithm</h3><p>Adam（Adaptive Moment Estimation）算法结合了动量梯度下降算法和RMSprop算法。其算法流程为：</p><p>$V_{dW}=0,\ S_{dW},\ V_{db}=0,\ S_{db}=0$</p><p>$On\ iteration\ t:$</p><p>$\ \ \ \ Cimpute\ dW,\ db$</p><p>$\ \ \ \ V_{dW}=\beta_1V_{dW}+(1-\beta_1)dW,\ V_{db}=\beta_1V_{db}+(1-\beta_1)db$</p><p>$\ \ \ \ S_{dW}=\beta_2S_{dW}+(1-\beta_2)dW^2,\ S_{db}=\beta_2S_{db}+(1-\beta_2)db^2$</p><p>$\ \ \ \ V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta_1^t},\ V_{db}^{corrected}=\frac{V_{db}}{1-\beta_1^t}$</p><p>$\ \ \ \ S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta_2^t},\ S_{db}^{corrected}=\frac{S_{db}}{1-\beta_2^t}$</p><p>$\ \ \ \ W:=W-\alpha\frac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\varepsilon},\ b:=b-\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}$</p><p>Adam算法包含了几个超参数，分别是：$\alpha,\beta_1,\beta_2,\varepsilon$。其中，$\beta_1$通常设置为0.9，$\beta_2$通常设置为0.999，$\varepsilon$通常设置为$10^{-8}$。一般只需要对$\beta_1$和$\beta_2$进行调试。</p><p>实际应用中，Adam算法结合了动量梯度下降和RMSprop各自的优点，使得神经网络训练速度大大提高。</p><h3 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h3><p>减小学习因子$\alpha$也能有效提高神经网络训练速度，这种方法被称为learning rate decay。</p><p>Learning rate decay就是随着迭代次数增加，学习因子$\alpha$逐渐减小。下面用图示的方式来解释这样做的好处。下图中，蓝色折线表示使用恒定的学习因子$\alpha$，由于每次训练$\alpha$相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远。绿色折线表示使用不断减小的$\alpha$，随着训练次数增加，$\alpha$逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。相比较恒定的$\alpha$来说，learning rate decay更接近最优值。</p><p><img src="http://img.blog.csdn.net/20171028212226321?" alt="这里写图片描述"></p><p>Learning rate decay中对$\alpha$可由下列公式得到：</p><p>$$\alpha=\frac{1}{1+decay_rate*epoch}\alpha_0$$</p><p>其中，deacy_rate是参数（可调），epoch是训练完所有样本的次数。随着epoch增加，$\alpha$会不断变小。</p><p>除了上面计算$\alpha$的公式之外，还有其它可供选择的计算公式：</p><p>$$\alpha=0.95^{epoch}\cdot \alpha_0$$</p><p>$$\alpha=\frac{k}{\sqrt{epoch}}\cdot \alpha_0\ \ \ \ or\ \ \ \ \frac{k}{\sqrt{t}}\cdot \alpha_0$$</p><p>其中，k为可调参数，t为mini-bach number。</p><p>除此之外，还可以设置$\alpha$为关于t的离散值，随着t增加，$\alpha$呈阶梯式减小。当然，也可以根据训练情况灵活调整当前的$\alpha$值，但会比较耗时间。</p><h3 id="The-problem-of-local-optima"><a href="#The-problem-of-local-optima" class="headerlink" title="The problem of local optima"></a>The problem of local optima</h3><p>在使用梯度下降算法不断减小cost function时，可能会得到局部最优解（local optima）而不是全局最优解（global optima）。之前我们对局部最优解的理解是形如碗状的凹槽，如下图左边所示。但是在神经网络中，local optima的概念发生了变化。准确地来说，大部分梯度为零的“最优点”并不是这些凹槽处，而是形如右边所示的马鞍状，称为saddle point。也就是说，梯度为零并不能保证都是convex（极小值），也有可能是concave（极大值）。特别是在神经网络中参数很多的情况下，所有参数梯度为零的点很可能都是右边所示的马鞍状的saddle point，而不是左边那样的local optimum。</p><p><img src="http://img.blog.csdn.net/20171028232352807?" alt="这里写图片描述"></p><p>类似马鞍状的plateaus会降低神经网络学习速度。Plateaus是梯度接近于零的平缓区域，如下图所示。在plateaus上梯度很小，前进缓慢，到达saddle point需要很长时间。到达saddle point后，由于随机扰动，梯度一般能够沿着图中绿色箭头，离开saddle point，继续前进，只是在plateaus上花费了太多时间。</p><p><img src="http://img.blog.csdn.net/20171029094627014?" alt="这里写图片描述"></p><p>总的来说，关于local optima，有两点总结：</p><ul><li><p><strong>只要选择合理的强大的神经网络，一般不太可能陷入local optima</strong></p></li><li><p><strong>Plateaus可能会使梯度下降变慢，降低学习速度</strong></p></li></ul><p>值得一提的是，上文介绍的动量梯度下降，RMSprop，Adam算法都能有效解决plateaus下降过慢的问题，大大提高神经网络的学习速度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20171028232352807?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达优化深度神经网络" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《优化深度神经网络》课程笔记（1）-- 深度学习的实用层面</title>
    <link href="https://redstonewill.github.io/2018/03/29/39/"/>
    <id>https://redstonewill.github.io/2018/03/29/39/</id>
    <published>2018-03-29T12:20:35.000Z</published>
    <updated>2018-03-29T12:52:33.838Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20171016144014868?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>在接下来的几次笔记中，我们将对第二门课《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》进行笔记总结和整理。我们在第一门课中已经学习了如何建立一个神经网络，或者浅层的，或者深度的。而这第二门课，我们将着重讨论和研究如何优化神经网络模型，例如调整超参数，提高算法运行速度等等。开始吧~</p><h3 id="Train-Dev-Test-sets"><a href="#Train-Dev-Test-sets" class="headerlink" title="Train/Dev/Test sets"></a>Train/Dev/Test sets</h3><p>选择最佳的训练集（Training sets）、验证集（Development sets）、测试集（Test sets）对神经网络的性能影响非常重要。除此之外，在构建一个神经网络的时候，我们需要设置许多参数，例如神经网络的层数、每个隐藏层包含的神经元个数、学习因子（学习速率）、激活函数的选择等等。实际上很难在第一次设置的时候就选择到这些最佳的参数，而是需要通过不断地迭代更新来获得。这个循环迭代的过程是这样的：我们先有个想法Idea，先选择初始的参数值，构建神经网络模型结构；然后通过代码Code的形式，实现这个神经网络；最后，通过实验Experiment验证这些参数对应的神经网络的表现性能。根据验证结果，我们对参数进行适当的调整优化，再进行下一次的Idea-&gt;Code-&gt;Experiment循环。通过很多次的循环，不断调整参数，选定最佳的参数值，从而让神经网络性能最优化。</p><p><img src="http://img.blog.csdn.net/20171012091853037?" alt="这里写图片描述"></p><p>深度学习已经应用于许多领域中，比如NLP，CV，Speech Recognition等等。通常来说，最适合某个领域的深度学习网络往往不能直接应用在其它问题上。解决不同问题的最佳选择是根据样本数量、输入特征数量和电脑配置信息（GPU或者CPU）等，来选择最合适的模型。即使是最有经验的深度学习专家也很难第一次就找到最合适的参数。因此，应用深度学习是一个反复迭代的过程，需要通过反复多次的循环训练得到最优化参数。决定整个训练过程快慢的关键在于单次循环所花费的时间，单次循环越快，训练过程越快。而设置合适的Train/Dev/Test sets数量，能有效提高训练效率。</p><p>一般地，我们将所有的样本数据分成三个部分：Train/Dev/Test sets。Train sets用来训练你的算法模型；Dev sets用来验证不同算法的表现情况，从中选择最好的算法模型；Test sets用来测试最好算法的实际表现，作为该算法的无偏估计。</p><p>之前人们通常设置Train sets和Test sets的数量比例为70%和30%。如果有Dev sets，则设置比例为60%、20%、20%，分别对应Train/Dev/Test sets。这种比例分配在样本数量不是很大的情况下，例如100,1000,10000，是比较科学的。但是如果数据量很大的时候，例如100万，这种比例分配就不太合适了。科学的做法是要将Dev sets和Test sets的比例设置得很低。因为Dev sets的目标是用来比较验证不同算法的优劣，从而选择更好的算法模型就行了。因此，通常不需要所有样本的20%这么多的数据来进行验证。对于100万的样本，往往只需要10000个样本来做验证就够了。Test sets也是一样，目标是测试已选算法的实际表现，无偏估计。对于100万的样本，往往也只需要10000个样本就够了。因此，对于大数据样本，Train/Dev/Test sets的比例通常可以设置为98%/1%/1%，或者99%/0.5%/0.5%。样本数据量越大，相应的Dev/Test sets的比例可以设置的越低一些。</p><p>现代深度学习还有个重要的问题就是训练样本和测试样本分布上不匹配，意思是训练样本和测试样本来自于不同的分布。举个例子，假设你开发一个手机app，可以让用户上传图片，然后app识别出猫的图片。在app识别算法中，你的训练样本可能来自网络下载，而你的验证和测试样本可能来自不同用户的上传。从网络下载的图片一般像素较高而且比较正规，而用户上传的图片往往像素不稳定，且图片质量不一。因此，训练样本和验证/测试样本可能来自不同的分布。解决这一问题的比较科学的办法是尽量保证Dev sets和Test sets来自于同一分布。值得一提的是，训练样本非常重要，通常我们可以将现有的训练样本做一些处理，例如图片的翻转、假如随机噪声等，来扩大训练样本的数量，从而让该模型更加强大。即使Train sets和Dev/Test sets不来自同一分布，使用这些技巧也能提高模型性能。</p><p>最后提一点的是如果没有Test sets也是没有问题的。Test sets的目标主要是进行无偏估计。我们可以通过Train sets训练不同的算法模型，然后分别在Dev sets上进行验证，根据结果选择最好的算法模型。这样也是可以的，不需要再进行无偏估计了。如果只有Train sets和Dev sets，通常也有人把这里的Dev sets称为Test sets，我们要注意加以区别。</p><h3 id="Bias-Variance"><a href="#Bias-Variance" class="headerlink" title="Bias/Variance"></a>Bias/Variance</h3><p>偏差（Bias）和方差（Variance）是机器学习领域非常重要的两个概念和需要解决的问题。在传统的机器学习算法中，Bias和Variance是对立的，分别对应着欠拟合和过拟合，我们常常需要在Bias和Variance之间进行权衡。而在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。</p><p>如下图所示，显示了二维平面上，high bias，just right，high variance的例子。可见，high bias对应着欠拟合，而high variance对应着过拟合。</p><p><img src="http://img.blog.csdn.net/20171016144014868?" alt="这里写图片描述"></p><p>上图这个例子中输入特征是二维的，high bias和high variance可以直接从图中分类线看出来。而对于输入特征是高维的情况，如何来判断是否出现了high bias或者high variance呢？</p><p>例如猫识别问题，输入是一幅图像，其特征维度很大。这种情况下，我们可以通过两个数值Train set error和Dev set error来理解bias和variance。假设Train set error为1%，而Dev set error为11%，即该算法模型对训练样本的识别很好，但是对验证集的识别却不太好。这说明了该模型对训练样本可能存在过拟合，模型泛化能力不强，导致验证集识别率低。这恰恰是high variance的表现。假设Train set error为15%，而Dev set error为16%，虽然二者error接近，即该算法模型对训练样本和验证集的识别都不是太好。这说明了该模型对训练样本存在欠拟合。这恰恰是high bias的表现。假设Train set error为15%，而Dev set error为30%，说明了该模型既存在high bias也存在high variance（深度学习中最坏的情况）。再假设Train set error为0.5%，而Dev set error为1%，即low bias和low variance，是最好的情况。值得一提的是，以上的这些假设都是建立在base error是0的基础上，即人类都能正确识别所有猫类图片。base error不同，相应的Train set error和Dev set error会有所变化，但没有相对变化。</p><p>一般来说，Train set error体现了是否出现bias，Dev set error体现了是否出现variance（正确地说，应该是Dev set error与Train set error的相对差值）。</p><p>我们已经通过二维平面展示了high bias或者high variance的模型，下图展示了high bias and high variance的模型：</p><p><img src="http://img.blog.csdn.net/20171016151639944?" alt="这里写图片描述"></p><p>模型既存在high bias也存在high variance，可以理解成某段区域是欠拟合的，某段区域是过拟合的。</p><h3 id="Basic-Recipe-for-Machine-Learning"><a href="#Basic-Recipe-for-Machine-Learning" class="headerlink" title="Basic Recipe for Machine Learning"></a>Basic Recipe for Machine Learning</h3><p>机器学习中基本的一个诀窍就是避免出现high bias和high variance。首先，减少high bias的方法通常是增加神经网络的隐藏层个数、神经元个数，训练时间延长，选择其它更复杂的NN模型等。在base error不高的情况下，一般都能通过这些方式有效降低和避免high bias，至少在训练集上表现良好。其次，减少high variance的方法通常是增加训练样本数据，进行正则化Regularization，选择其他更复杂的NN模型等。</p><p>这里有几点需要注意的。第一，解决high bias和high variance的方法是不同的。实际应用中通过Train set error和Dev set error判断是否出现了high bias或者high variance，然后再选择针对性的方法解决问题。</p><p>第二，Bias和Variance的折中tradeoff。传统机器学习算法中，Bias和Variance通常是对立的，减小Bias会增加Variance，减小Variance会增加Bias。而在现在的深度学习中，通过使用更复杂的神经网络和海量的训练样本，一般能够同时有效减小Bias和Variance。这也是深度学习之所以如此强大的原因之一。</p><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>如果出现了过拟合，即high variance，则需要采用正则化regularization来解决。虽然扩大训练样本数量也是减小high variance的一种方法，但是通常获得更多训练样本的成本太高，比较困难。所以，更可行有效的办法就是使用regularization。</p><p>我们先来回顾一下之前介绍的Logistic regression。采用L2 regularization，其表达式为：</p><p>$$J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_2^2$$</p><p>$$\mid\mid w\mid\mid_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw$$</p><p>这里有个问题：为什么只对w进行正则化而不对b进行正则化呢？其实也可以对b进行正则化。但是一般w的维度很大，而b只是一个常数。相比较来说，参数很大程度上由w决定，改变b值对整体模型影响较小。所以，一般为了简便，就忽略对b的正则化了。</p><p>除了L2 regularization之外，还有另外一只正则化方法：L1 regularization。其表达式为：</p><p>$$J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_1$$</p><p>$$\mid\mid w\mid\mid_1=\sum_{j=1}^{n_x}|w_j|$$</p><p>与L2 regularization相比，L1 regularization得到的w更加稀疏，即很多w为零值。其优点是节约存储空间，因为大部分w为0。然而，实际上L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且，L1的在微分求导方面比较复杂。所以，一般L2 regularization更加常用。</p><p>L1、L2 regularization中的$\lambda$就是正则化参数（超参数的一种）。可以设置$\lambda$为不同的值，在Dev set中进行验证，选择最佳的$\lambda$。顺便提一下，在python中，由于lambda是保留字，所以为了避免冲突，我们使用lambd来表示$\lambda$。</p><p>在深度学习模型中，L2 regularization的表达式为：</p><p>$$J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2$$</p><p>$$||w^{[l]}||^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2$$</p><p>通常，我们把$||w^{[l]}||^2$称为Frobenius范数，记为$||w^{[l]}||_F^2$。一个矩阵的Frobenius范数就是计算所有元素平方和再开方，如下所示：</p><p>$$\mid\mid A\mid\mid_F=\sqrt {\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2}$$</p><p>值得注意的是，由于加入了正则化项，梯度下降算法中的$dw^{[l]}$计算表达式需要做如下修改：</p><p>$$dw^{[l]}=dw^{[l]}_{before}+\frac{\lambda}{m}w^{[l]}$$</p><p>$$w^{[l]}:=w^{[l]}-\alpha\cdot dw^{[l]}$$</p><p>L2 regularization也被称做weight decay。这是因为，由于加上了正则项，$dw^{[l]}$有个增量，在更新$w^{[l]}$的时候，会多减去这个增量，使得$w^{[l]}$比没有正则项的值要小一些。不断迭代更新，不断地减小。</p><p>$$w^{[l]}:=w^{[l]}-\alpha\cdot dw^{[l]}$$</p><p>$$w^{[l]}:=w^{[l]}-\alpha\cdot(dw^{[l]}_{before}+\frac{\lambda}{m}w^{[l]})$$</p><p>$$w^{[l]}:=(1-\alpha\frac{\lambda}{m})w^{[l]}-\alpha\cdot dw^{[l]}_{before}$$</p><p>其中，$(1-\alpha\frac{\lambda}{m})&lt;1$。</p><h3 id="Why-regularization-reduces-overfitting"><a href="#Why-regularization-reduces-overfitting" class="headerlink" title="Why regularization reduces overfitting"></a>Why regularization reduces overfitting</h3><p>为什么正则化能够有效避免high variance，防止过拟合呢？下面我们通过几个例子说明。</p><p>还是之前那张图，从左到右，分别表示了欠拟合，刚好拟合，过拟合三种情况。</p><p><img src="http://img.blog.csdn.net/20171017202339866?" alt="这里写图片描述"></p><p>假如我们选择了非常复杂的神经网络模型，如上图左上角所示。在未使用正则化的情况下，我们得到的分类超平面可能是类似上图右侧的过拟合。但是，如果使用L2 regularization，当$\lambda$很大时，$w^{[l]}\approx0$。$w^{[l]}$近似为零，意味着该神经网络模型中的某些神经元实际的作用很小，可以忽略。从效果上来看，其实是将某些神经元给忽略掉了。这样原本过于复杂的神经网络模型就变得不那么复杂了，而变得非常简单化了。如下图所示，整个简化的神经网络模型变成了一个逻辑回归模型。问题就从high variance变成了high bias了。</p><p><img src="http://img.blog.csdn.net/20171017204414580?" alt="这里写图片描述"></p><p>因此，选择合适大小的$\lambda$值，就能够同时避免high bias和high variance，得到最佳模型。</p><p>还有另外一个直观的例子来解释为什么正则化能够避免发生过拟合。假设激活函数是tanh函数。tanh函数的特点是在z接近零的区域，函数近似是线性的，而当|z|很大的时候，函数非线性且变化缓慢。当使用正则化，$\lambda$较大，即对权重$w^{[l]}$的惩罚较大，$w^{[l]}$减小。因为$z^{[l]}=w^{[l]}a^{[l]}+b^{[l]}$。当$w^{[l]}$减小的时候，$z^{[l]}$也会减小。则此时的$z^{[l]}$分布在tanh函数的近似线性区域。那么这个神经元起的作用就相当于是linear regression。如果每个神经元对应的权重$w^{[l]}$都比较小，那么整个神经网络模型相当于是多个linear regression的组合，即可看成一个linear network。得到的分类超平面就会比较简单，不会出现过拟合现象。</p><p><img src="http://img.blog.csdn.net/20171017205552864?" alt="这里写图片描述"></p><h3 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a>Dropout Regularization</h3><p>除了L2 regularization之外，还有另外一种防止过拟合的有效方法：Dropout。</p><p>Dropout是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。</p><p><img src="http://img.blog.csdn.net/20171018083824024?" alt="这里写图片描述"></p><p>Dropout有不同的实现方法，接下来介绍一种常用的方法：Inverted dropout。假设对于第$l$层神经元，设定保留神经元比例概率keep_prob=0.8，即该层有20%的神经元停止工作。$dl$为dropout向量，设置$dl$为随机vector，其中80%的元素为1，20%的元素为0。在python中可以使用如下语句生成dropout vector：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>],al.shape[<span class="number">1</span>])&lt;keep_prob</span><br></pre></td></tr></table></figure><p>然后，第$l$层经过dropout，随机删减20%的神经元，只保留80%的神经元，其输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">al = np.multiply(al,dl)</span><br></pre></td></tr></table></figure><p>最后，还要对$al$进行scale up处理，即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">al /= keep_prob</span><br></pre></td></tr></table></figure><p>以上就是Inverted dropout的方法。之所以要对$al$进行scale up是为了保证在经过dropout后，$al$作为下一层神经元的输入值尽量保持不变。假设第$l$层有50个神经元，经过dropout后，有10个神经元停止工作，这样只有40神经元有作用。那么得到的$al$只相当于原来的80%。scale up后，能够尽可能保持$al$的期望值相比之前没有大的变化。</p><p>Inverted dropout的另外一个好处就是在对该dropout后的神经网络进行测试时能够减少scaling问题。因为在训练时，使用scale up保证$al$的期望值没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作了。</p><p>对于m个样本，单次迭代训练时，随机删除掉隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上正向和反向更新权重w和常数项b；接着，下一次迭代中，再恢复之前删除的神经元，重新随机删除一定数量的神经元，进行正向和反向更新w和b。不断重复上述过程，直至迭代训练完成。</p><p>值得注意的是，使用dropout训练结束后，在测试和实际应用模型时，不需要进行dropout和随机删减神经元，所有的神经元都在工作。</p><h3 id="Understanding-Dropout"><a href="#Understanding-Dropout" class="headerlink" title="Understanding Dropout"></a>Understanding Dropout</h3><p>Dropout通过每次迭代训练时，随机选择不同的神经元，相当于每次都在不同的神经网络上进行训练，类似机器学习中Bagging的方法（三个臭皮匠，赛过诸葛亮），能够防止过拟合。</p><p>除此之外，还可以从权重w的角度来解释为什么dropout能够有效防止过拟合。对于某个神经元来说，某次训练时，它的某些输入在dropout的作用被过滤了。而在下一次训练时，又有不同的某些输入被过滤。经过多次训练后，某些输入被过滤，某些输入被保留。这样，该神经元就不会受某个输入非常大的影响，影响被均匀化了。也就是说，对应的权重w不会很大。这从从效果上来说，与L2 regularization是类似的，都是对权重w进行“惩罚”，减小了w的值。</p><p><img src="http://img.blog.csdn.net/20171019173811963?" alt="这里写图片描述"></p><p>总结一下，对于同一组训练数据，利用不同的神经网络训练之后，求其输出的平均值可以减少overfitting。Dropout就是利用这个原理，每次丢掉一定数量的隐藏层神经元，相当于在不同的神经网络上进行训练，这样就减少了神经元之间的依赖性，即每个神经元不能依赖于某几个其他的神经元（指层与层之间相连接的神经元），使神经网络更加能学习到与其他神经元之间的更加健壮robust的特征。</p><p>在使用dropout的时候，有几点需要注意。首先，不同隐藏层的dropout系数keep_prob可以不同。一般来说，神经元越多的隐藏层，keep_out可以设置得小一些.，例如0.5；神经元越少的隐藏层，keep_out可以设置的大一些，例如0.8，设置是1。另外，实际应用中，不建议对输入层进行dropout，如果输入层维度很大，例如图片，那么可以设置dropout，但keep_out应设置的大一些，例如0.8，0.9。总体来说，就是越容易出现overfitting的隐藏层，其keep_prob就设置的相对小一些。没有准确固定的做法，通常可以根据validation进行选择。</p><p>Dropout在电脑视觉CV领域应用比较广泛，因为输入层维度较大，而且没有足够多的样本数量。值得注意的是dropout是一种regularization技巧，用来防止过拟合的，最好只在需要regularization的时候使用dropout。</p><p>使用dropout的时候，可以通过绘制cost function来进行debug，看看dropout是否正确执行。一般做法是，将所有层的keep_prob全设置为1，再绘制cost function，即涵盖所有神经元，看J是否单调下降。下一次迭代训练时，再将keep_prob设置为其它值。</p><h3 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a>Other regularization methods</h3><p>除了L2 regularization和dropout regularization之外，还有其它减少过拟合的方法。</p><p>一种方法是增加训练样本数量。但是通常成本较高，难以获得额外的训练样本。但是，我们可以对已有的训练样本进行一些处理来“制造”出更多的样本，称为data augmentation。例如图片识别问题中，可以对已有的图片进行水平翻转、垂直翻转、任意角度旋转、缩放或扩大等等。如下图所示，这些处理都能“制造”出新的训练样本。虽然这些是基于原有样本的，但是对增大训练样本数量还是有很有帮助的，不需要增加额外成本，却能起到防止过拟合的效果。</p><p><img src="http://img.blog.csdn.net/20171020211332022?" alt="这里写图片描述"></p><p>在数字识别中，也可以将原有的数字图片进行任意旋转或者扭曲，或者增加一些noise，如下图所示：</p><p><img src="http://img.blog.csdn.net/20171021095330052?" alt="这里写图片描述"></p><p>还有另外一种防止过拟合的方法：early stopping。一个神经网络模型随着迭代训练次数增加，train set error一般是单调减小的，而dev set error 先减小，之后又增大。也就是说训练次数过多时，模型会对训练样本拟合的越来越好，但是对验证集拟合效果逐渐变差，即发生了过拟合。因此，迭代训练次数不是越多越好，可以通过train set error和dev set error随着迭代次数的变化趋势，选择合适的迭代次数，即early stopping。</p><p><img src="http://img.blog.csdn.net/20171021101801394?" alt="这里写图片描述"></p><p>然而，Early stopping有其自身缺点。通常来说，机器学习训练模型有两个目标：一是优化cost function，尽量减小J；二是防止过拟合。这两个目标彼此对立的，即减小J的同时可能会造成过拟合，反之亦然。我们把这二者之间的关系称为正交化orthogonalization。该节课开始部分就讲过，在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。但是，Early stopping的做法通过减少得带训练次数来防止过拟合，这样J就不会足够小。也就是说，early stopping将上述两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。</p><p>与early stopping相比，L2 regularization可以实现“分而治之”的效果：迭代训练足够多，减小J，而且也能有效防止过拟合。而L2 regularization的缺点之一是最优的正则化参数$\lambda$的选择比较复杂。对这一点来说，early stopping比较简单。总的来说，L2 regularization更加常用一些。</p><h3 id="Normalizing-inputs"><a href="#Normalizing-inputs" class="headerlink" title="Normalizing inputs"></a>Normalizing inputs</h3><p>在训练神经网络时，标准化输入可以提高训练的速度。标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值$\mu$后，再除以其方差$\sigma^2$：</p><p>$$\mu=\frac1m\sum_{i=1}^mX^{(i)}$$</p><p>$$\sigma^2=\frac1m\sum_{i=1}^m(X^{(i)})^2$$</p><p>$$X:=\frac{X-\mu}{\sigma^2}$$</p><p>以二维平面为例，下图展示了其归一化过程：</p><p><img src="http://img.blog.csdn.net/20171021154503181?" alt="这里写图片描述"></p><p>值得注意的是，由于训练集进行了标准化处理，那么对于测试集或在实际应用时，应该使用同样的$\mu$和$\sigma^2$对其进行标准化处理。这样保证了训练集合测试集的标准化操作一致。</p><p>之所以要对输入进行标准化操作，主要是为了让所有输入归一化同样的尺度上，方便进行梯度下降算法时能够更快更准确地找到全局最优解。假如输入特征是二维的，且x1的范围是[1,1000]，x2的范围是[0,1]。如果不进行标准化处理，x1与x2之间分布极不平衡，训练得到的w1和w2也会在数量级上差别很大。这样导致的结果是cost function与w和b的关系可能是一个非常细长的椭圆形碗。对其进行梯度下降算法时，由于w1和w2数值差异很大，只能选择很小的学习因子$\alpha$，来避免J发生振荡。一旦$\alpha$较大，必然发生振荡，J不再单调下降。如下左图所示。</p><p>然而，如果进行了标准化操作，x1与x2分布均匀，w1和w2数值差别不大，得到的cost function与w和b的关系是类似圆形碗。对其进行梯度下降算法时，$\alpha$可以选择相对大一些，且J一般不会发生振荡，保证了J是单调下降的。如下右图所示。</p><p><img src="http://img.blog.csdn.net/20171021161550926?" alt="这里写图片描述"></p><p>另外一种情况，如果输入特征之间的范围本来就比较接近，那么不进行标准化操作也是没有太大影响的。但是，标准化处理在大多数场合下还是值得推荐的。</p><h3 id="Vanishing-and-Exploding-gradients"><a href="#Vanishing-and-Exploding-gradients" class="headerlink" title="Vanishing and Exploding gradients"></a>Vanishing and Exploding gradients</h3><p>在神经网络尤其是深度神经网络中存在可能存在这样一个问题：梯度消失和梯度爆炸。意思是当训练一个 层数非常多的神经网络时，计算得到的梯度可能非常小或非常大，甚至是指数级别的减小或增大。这样会让训练过程变得非常困难。</p><p>举个例子来说明，假设一个多层的每层只包含两个神经元的深度神经网络模型，如下图所示：</p><p><img src="http://img.blog.csdn.net/20171021170831470?" alt="这里写图片描述"></p><p>为了简化复杂度，便于分析，我们令各层的激活函数为线性函数，即$g(Z)=Z$。且忽略各层常数项b的影响，令b全部为零。那么，该网络的预测输出$\hat Y$为：</p><p>$$\hat Y=W^{[L]}W^{[L-1]}W^{[L-2]}\cdots W^{[3]}W^{[2]}W^{[1]}X$$</p><p>如果各层权重$W^{[l]}$的元素都稍大于1，例如1.5，则预测输出$\hat Y$将正比于$1.5^L$。L越大，$\hat Y$越大，且呈指数型增长。我们称之为数值爆炸。相反，如果各层权重$W^{[l]}$的元素都稍小于1，例如0.5，则预测输出$\hat Y$将正比于$0.5^L$。网络层数L越多，$\hat Y$呈指数型减小。我们称之为数值消失。</p><p>也就是说，如果各层权重$W^{[l]}$都大于1或者都小于1，那么各层激活函数的输出将随着层数$l$的增加，呈指数型增大或减小。当层数很大时，出现数值爆炸或消失。同样，这种情况也会引起梯度呈现同样的指数型增大或减小的变化。L非常大时，例如L=150，则梯度会非常大或非常小，引起每次更新的步进长度过大或者过小，这让训练过程十分困难。</p><h3 id="Weight-Initialization-for-Deep-Networks"><a href="#Weight-Initialization-for-Deep-Networks" class="headerlink" title="Weight Initialization for Deep Networks"></a>Weight Initialization for Deep Networks</h3><p>下面介绍如何改善Vanishing and Exploding gradients这类问题，方法是对权重w进行一些初始化处理。</p><p>深度神经网络模型中，以单个神经元为例，该层（$l$）的输入个数为n，其输出为：</p><p>$$z=w_1x_1+w_2x_2+\cdots+w_nx_n$$</p><p>$$a=g(z)$$</p><p><img src="http://img.blog.csdn.net/20171021204641118?" alt="这里写图片描述"></p><p>这里忽略了常数项b。为了让z不会过大或者过小，思路是让w与n有关，且n越大，w应该越小才好。这样能够保证z不会过大。一种方法是在初始化w时，令其方差为$\frac1n$。相应的python伪代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">1</span>/n[l<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><p>如果激活函数是tanh，一般选择上面的初始化方法。</p><p>如果激活函数是ReLU，权重w的初始化一般令其方差为$\frac2n$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/n[l<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><p>除此之外，Yoshua Bengio提出了另外一种初始化w的方法，令其方差为$\frac{2}{n^{[l-1]}n^{[l]}}$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/n[l<span class="number">-1</span>]*n[l])</span><br></pre></td></tr></table></figure><p>至于选择哪种初始化方法因人而异，可以根据不同的激活函数选择不同方法。另外，我们可以对这些初始化方法中设置某些参数，作为超参数，通过验证集进行验证，得到最优参数，来优化神经网络。</p><h3 id="Numerical-approximation-of-gradients"><a href="#Numerical-approximation-of-gradients" class="headerlink" title="Numerical approximation of gradients"></a>Numerical approximation of gradients</h3><p>Back Propagation神经网络有一项重要的测试是梯度检查（gradient checking）。其目的是检查验证反向传播过程中梯度下降算法是否正确。该小节将先介绍如何近似求出梯度值。</p><p><img src="http://img.blog.csdn.net/20171021214350472?" alt="这里写图片描述"></p><p>利用微分思想，函数f在点$\theta$处的梯度可以表示成：</p><p>$$g(\theta)=\frac{f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2\varepsilon}$$</p><p>其中，$\varepsilon&gt;0$，且足够小。</p><h3 id="Gradient-checking"><a href="#Gradient-checking" class="headerlink" title="Gradient checking"></a>Gradient checking</h3><p>介绍完如何近似求出梯度值后，我们将介绍如何进行梯度检查，来验证训练过程中是否出现bugs。</p><p>梯度检查首先要做的是分别将$W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[L]}$这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维向量$\theta$。这样cost function $J(W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[L]})$就可以表示成$J(\theta)$。</p><p>然后将反向传播过程通过梯度下降算法得到的$dW^{[1]},db^{[1]},\cdots,dW^{[L]},db^{[L]}$按照一样的顺序构造成一个一维向量$d\theta$。$d\theta$的维度与$\theta$一致。</p><p>接着利用$J(\theta)$对每个$\theta_i$计算近似梯度，其值与反向传播算法得到的$d\theta_i$相比较，检查是否一致。例如，对于第i个元素，近似梯度为：</p><p>$$d\theta_{approx}[i]=\frac{J(\theta_1,\theta_2,\cdots,\theta_i+\varepsilon,\cdots)-J(\theta_1,\theta_2,\cdots,\theta_i-\varepsilon,\cdots)}{2\varepsilon}$$</p><p>计算完所有$\theta_i$的近似梯度后，可以计算$d\theta_{approx}$与$d\theta$的欧氏（Euclidean）距离来比较二者的相似度。公式如下：</p><p>$$\frac{\mid\mid d\theta_{approx}-d\theta\mid\mid_2}{\mid\mid d\theta_{approx}\mid\mid_2+\mid\mid d\theta\mid\mid_2}$$</p><p>一般来说，如果欧氏距离越小，例如$10^{-7}$，甚至更小，则表明$d\theta_{approx}$与$d\theta$越接近，即反向梯度计算是正确的，没有bugs。如果欧氏距离较大，例如$10^{-5}$，则表明梯度计算可能出现问题，需要再次检查是否有bugs存在。如果欧氏距离很大，例如$10^{-3}$，甚至更大，则表明$d\theta_{approx}$与$d\theta$差别很大，梯度下降计算过程有bugs，需要仔细检查。</p><h3 id="Gradient-Checking-Implementation-Notes"><a href="#Gradient-Checking-Implementation-Notes" class="headerlink" title="Gradient Checking Implementation Notes"></a>Gradient Checking Implementation Notes</h3><p>在进行梯度检查的过程中有几点需要注意的地方：</p><ul><li><p><strong>不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。</strong></p></li><li><p><strong>如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。</strong></p></li><li><p><strong>注意不要忽略正则化项，计算近似梯度的时候要包括进去。</strong></p></li><li><p><strong>梯度检查时关闭dropout，检查完毕后再打开dropout。</strong></p></li><li><p><strong>随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用）。</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20171016144014868?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达优化深度神经网络" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《神经网络与深度学习》课程笔记（5）-- 深层神经网络</title>
    <link href="https://redstonewill.github.io/2018/03/27/38/"/>
    <id>https://redstonewill.github.io/2018/03/27/38/</id>
    <published>2018-03-27T12:27:13.000Z</published>
    <updated>2018-03-27T12:31:16.285Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170927110934360?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重W，不同神经元的W不能初始化为同一零值。本节课是对上节课的延伸和扩展，讨论更深层的神经网络。</p><h3 id="Deep-L-layer-neural-network"><a href="#Deep-L-layer-neural-network" class="headerlink" title="Deep L-layer neural network"></a>Deep L-layer neural network</h3><p>深层神经网络其实就是包含更多的隐藏层神经网络。如下图所示，分别列举了逻辑回归、1个隐藏层的神经网络、2个隐藏层的神经网络和5个隐藏层的神经网络它们的模型结构。</p><p><img src="http://img.blog.csdn.net/20170927110907680?" alt="这里写图片描述"></p><p>命名规则上，一般只参考隐藏层个数和输出层。例如，上图中的逻辑回归又叫1 layer NN，1个隐藏层的神经网络叫做2 layer NN，2个隐藏层的神经网络叫做3 layer NN，以此类推。如果是L-layer NN，则包含了L-1个隐藏层，最后的L层是输出层。</p><p>下面以一个4层神经网络为例来介绍关于神经网络的一些标记写法。如下图所示，首先，总层数用L表示，L=4。输入层是第0层，输出层是第L层。$n^{[l]}$表示第$l$层包含的单元个数，$l=0,1,\cdots,L$。这个模型中，$n^{[0]}=n_x=3$，表示三个输入特征$x_1,x_2,x_3$。$n^{[1]}=5$，$n^{[2]}=5$，$n^{[3]}=3$，$n^{[4]}=n^{[L]}=1$。第$l$层的激活函数输出用$a^{[l]}$表示，$a^{[l]}=g^{[l]}(z^{[l]})$。$W^{[l]}$表示第$l$层的权重，用于计算$z^{[l]}$。另外，我们把输入x记为$a^{[0]}$，把输出层$\hat y$记为$a^{[L]}$。</p><p>注意，$a^{[l]}$和$W^{[l]}$中的上标$l$都是从1开始的，$l=1,\cdots,L$。</p><p><img src="http://img.blog.csdn.net/20170927110934360?" alt="这里写图片描述"></p><h3 id="Forward-Propagation-in-a-Deep-Network"><a href="#Forward-Propagation-in-a-Deep-Network" class="headerlink" title="Forward Propagation in a Deep Network"></a>Forward Propagation in a Deep Network</h3><p>接下来，我们来推导一下深层神经网络的正向传播过程。仍以上面讲过的4层神经网络为例，对于单个样本：</p><p>第1层，$l=1$：</p><p>$$z^{[1]}=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}$$</p><p>$$a^{[1]}=g^{[1]}(z^{[1]})$$</p><p>第2层，$l=2$：</p><p>$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$</p><p>$$a^{[2]}=g^{[2]}(z^{[2]})$$</p><p>第3层，$l=3$：</p><p>$$z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}$$</p><p>$$a^{[3]}=g^{[3]}(z^{[3]})$$</p><p>第4层，$l=4$：</p><p>$$z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}$$</p><p>$$a^{[4]}=g^{[4]}(z^{[4]})$$</p><p>如果有m个训练样本，其向量化矩阵形式为：</p><p>第1层，$l=1$：</p><p>$$Z^{[1]}=W^{[1]}X+b^{[1]}=W^{[1]}A^{[0]}+b^{[1]}$$</p><p>$$A^{[1]}=g^{[1]}(Z^{[1]})$$</p><p>第2层，$l=2$：</p><p>$$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$$</p><p>$$A^{[2]}=g^{[2]}(Z^{[2]})$$</p><p>第3层，$l=3$：</p><p>$$Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}$$</p><p>$$A^{[3]}=g^{[3]}(Z^{[3]})$$</p><p>第4层，$l=4$：</p><p>$$Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}$$</p><p>$$A^{[4]}=g^{[4]}(Z^{[4]})$$</p><p>综上所述，对于第$l$层，其正向传播过程的$Z^{[l]}$和$A^{[l]}$可以表示为：</p><p>$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$$</p><p>$$A^{[l]}=g^{[l]}(Z^{[l]})$$</p><p>其中$l=1,\cdots,L$</p><h3 id="Getting-your-matrix-dimensions-right"><a href="#Getting-your-matrix-dimensions-right" class="headerlink" title="Getting your matrix dimensions right"></a>Getting your matrix dimensions right</h3><p>对于单个训练样本，输入x的维度是（$n^{[0]},1$）神经网络的参数$W^{[l]}$和$b^{[l]}$的维度分别是：</p><p>$$W^{[l]}:\ (n^{[l]},n^{[l-1]})$$</p><p>$$b^{[l]}:\ (n^{[l]},1)$$</p><p>其中，$l=1,\cdots,L$，$n^{[l]}$和$n^{[l-1]}$分别表示第$l$层和$l-1$层的所含单元个数。$n^{[0]}=n_x$，表示输入层特征数目。</p><p>顺便提一下，反向传播过程中的$dW^{[l]}$和$db^{[l]}$的维度分别是：</p><p>$$dW^{[l]}:\ (n^{[l]},n^{[l-1]})$$</p><p>$$db^{[l]}:\ (n^{[l]},1)$$</p><p>注意到，$W^{[l]}$与$dW^{[l]}$维度相同，$b^{[l]}$与$db^{[l]}$维度相同。这很容易理解。</p><p>正向传播过程中的$z^{[l]}$和$a^{[l]}$的维度分别是：</p><p>$$z^{[l]}:\ (n^{[l]},1)$$</p><p>$$a^{[l]}:\ (n^{[l]},1)$$</p><p>$z^{[l]}$和$a^{[l]}$的维度是一样的，且$dz^{[l]}$和$da^{[l]}$的维度均与$z^{[l]}$和$a^{[l]}$的维度一致。</p><p>对于m个训练样本，输入矩阵X的维度是（$n^{[0]},m$）。需要注意的是$W^{[l]}$和$b^{[l]}$的维度与只有单个样本是一致的：</p><p>$$W^{[l]}:\ (n^{[l]},n^{[l-1]})$$</p><p>$$b^{[l]}:\ (n^{[l]},1)$$</p><p>只不过在运算$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$中，$b^{[l]}$会被当成（$n^{[l]},m$）矩阵进行运算，这是因为python的广播性质，且$b^{[l]}$每一列向量都是一样的。$dW^{[l]}$和$db^{[l]}$的维度分别与$W^{[l]}$和$b^{[l]}$的相同。</p><p>但是，$Z^{[l]}$和$A^{[l]}$的维度发生了变化：</p><p>$$Z^{[l]}:\ (n^{[l]},m)$$</p><p>$$A^{[l]}:\ (n^{[l]},m)$$</p><p>$dZ^{[l]}$和$dA^{[l]}$的维度分别与$Z^{[l]}$和$A^{[l]}$的相同。</p><h3 id="Why-deep-representations"><a href="#Why-deep-representations" class="headerlink" title="Why deep representations?"></a>Why deep representations?</h3><p>我们都知道神经网络能处理很多问题，而且效果显著。其强大能力主要源自神经网络足够“深”，也就是说网络层数越多，神经网络就更加复杂和深入，学习也更加准确。接下来，我们从几个例子入手，看一下为什么深度网络能够如此强大。</p><p>先来看人脸识别的例子，如下图所示。经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。可以看出，随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。</p><p>语音识别模型也是这个道理。浅层的神经元能够检测一些简单的音调，然后较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。记住一点，神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。</p><p><img src="http://img.blog.csdn.net/20170927111045673?" alt="这里写图片描述"></p><p>除了从提取特征复杂度的角度来说明深层网络的优势之外，深层网络还有另外一个优点，就是能够减少神经元个数，从而减少计算量。例如下面这个例子，使用电路理论，计算逻辑输出：</p><p>$$y=x_1\oplus x_2\oplus x_3\oplus\cdots\oplus x_n$$</p><p>其中，$\oplus$表示异或操作。对于这个逻辑运算，如果使用深度网络，深度网络的结构是每层将前一层的两两单元进行异或，最后到一个输出，如下图左边所示。这样，整个深度网络的层数是$log_2(n)$，不包含输入层。总共使用的神经元个数为：</p><p>$$1+2+\cdots+2^{log_2(n)-1}=1\cdot\frac{1-2^{log_2(n)}}{1-2}=2^{log_2(n)}-1=n-1$$</p><p>可见，输入个数是n，这种深层网络所需的神经元个数仅仅是n-1个。</p><p>如果不用深层网络，仅仅使用单个隐藏层，那么需要的神经元个数将是指数级别那么大。Andrew指出，由于包含了所有的逻辑位（0和1），则需要$2^{n-1}$个神经元。<strong>这里笔者推导的是$2^n$个神经元，为啥是$2^{n-1}$请哪位高手解释下。</strong></p><p>比较下来，处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。这也是深层神经网络的优点之一。</p><p>尽管深度学习有着非常显著的优势，Andrew还是建议对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合奥卡姆剃刀定律（Occam’s Razor）。对于比较复杂的问题，再使用较深的神经网络模型。</p><h3 id="Building-blocks-of-deep-neural-networks"><a href="#Building-blocks-of-deep-neural-networks" class="headerlink" title="Building blocks of deep neural networks"></a>Building blocks of deep neural networks</h3><p>下面用流程块图来解释神经网络正向传播和反向传播过程。如下图所示，对于第$l$层来说，正向传播过程中：</p><p>输入：$a^{[l-1]}$</p><p>输出：$a^{[l]}$</p><p>参数：$W^{[l]},b^{[l]}$</p><p>缓存变量：$z^{[l]}$</p><p>反向传播过程中：</p><p>输入：$da^{[l]}$</p><p>输出：$da^{[l-1]},dW^{[l]},db^{[l]}$</p><p>参数：$W^{[l]},b^{[l]}$</p><p><img src="http://img.blog.csdn.net/20170927111735834?" alt="这里写图片描述"></p><p>刚才这是第$l$层的流程块图，对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：</p><p><img src="http://img.blog.csdn.net/20171019095613497?" alt="这里写图片描述"></p><h3 id="Forward-and-Backward-Propagation"><a href="#Forward-and-Backward-Propagation" class="headerlink" title="Forward and Backward Propagation"></a>Forward and Backward Propagation</h3><p>我们继续接着上一部分流程块图的内容，推导神经网络正向传播过程和反向传播过程的具体表达式。</p><p>首先是正向传播过程，令层数为第$l$层，输入是$a^{[l-1]}$，输出是$a^{[l]}$，缓存变量是$z^{[l]}$。其表达式如下：</p><p>$$z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$$</p><p>$$a^{[l]}=g^{[l]}(z^{[l]})$$</p><p>m个训练样本，向量化形式为：</p><p>$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$$</p><p>$$A^{[l]}=g^{[l]}(Z^{[l]})$$</p><p>然后是反向传播过程，输入是$da^{[l]}$，输出是$da^{[l-1]},dw^{[l]},db^{[l]}$。其表达式如下：</p><p>$$dz^{[l]}=da^{[l]}\ast g^{[l]’}(z^{[l]})$$</p><p>$$dW^{[l]}=dz^{[l]}\cdot a^{[l-1]}$$</p><p>$$db^{[l]}=dz^{[l]}$$</p><p>$$da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}$$</p><p>由上述第四个表达式可得$da^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}$，将$da^{[l]}$代入第一个表达式中可以得到：</p><p>$$dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}\ast g^{[l]’}(z^{[l]})$$</p><p>该式非常重要，反映了$dz^{[l+1]}$与$dz^{[l]}$的递推关系。</p><p>m个训练样本，向量化形式为：</p><p>$$dZ^{[l]}=dA^{[l]}\ast g^{[l]’}(Z^{[l]})$$</p><p>$$dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T}$$</p><p>$$db^{[l]}=\frac1mnp.sum(dZ^{[l]},axis=1,keepdim=True)$$</p><p>$$dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}$$</p><p>$$dZ^{[l]}=W^{[l+1]T}\cdot dZ^{[l+1]}\ast g^{[l]’}(Z^{[l]})$$</p><h3 id="Parameters-vs-Hyperparameters"><a href="#Parameters-vs-Hyperparameters" class="headerlink" title="Parameters vs Hyperparameters"></a>Parameters vs Hyperparameters</h3><p>该部分介绍神经网络中的参数（parameters）和超参数（hyperparameters）的概念。</p><p>神经网络中的参数就是我们熟悉的$W^{[l]}$和$b^{[l]}$。而超参数则是例如学习速率$\alpha$，训练迭代次数N，神经网络层数L，各层神经元个数$n^{[l]}$，激活函数$g(z)$等。之所以叫做超参数的原因是它们决定了参数$W^{[l]}$和$b^{[l]}$的值。在后面的第二门课我们还将学习其它的超参数，这里先不讨论。</p><p>如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法。</p><h3 id="What-does-this-have-to-do-with-the-brain"><a href="#What-does-this-have-to-do-with-the-brain" class="headerlink" title="What does this have to do with the brain?"></a>What does this have to do with the brain?</h3><p>那么，神经网络跟人脑机制到底有什么联系呢？究竟有多少的相似程度？神经网络实际上可以分成两个部分：正向传播过程和反向传播过程。神经网络的每个神经元采用激活函数的方式，类似于感知机模型。这种模型与人脑神经元是类似的，可以说是一种非常简化的人脑神经元模型。如下图所示，人脑神经元可分为树突、细胞体、轴突三部分。树突接收外界电刺激信号（类比神经网络中神经元输入），传递给细胞体进行处理（类比神经网络中神经元激活函数运算），最后由轴突传递给下一个神经元（类比神经网络中神经元输出）。</p><p>值得一提的是，人脑神经元的结构和处理方式要复杂的多，神经网络模型只是非常简化的模型。人脑如何进行学习？是否也是通过反向传播和梯度下降算法现在还不清楚，可能会更加复杂。这是值得生物学家探索的事情。也许发现重要的新的人脑学习机制后，让我们的神经网络模型抛弃反向传播和梯度下降算法，能够实现更加准确和强大的神经网络模型！</p><p><img src="http://img.blog.csdn.net/20170927111316192?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了深层神经网络，是上一节浅层神经网络的拓展和归纳。首先，我们介绍了建立神经网络模型一些常用的标准的标记符号。然后，用流程块图的方式详细推导正向传播过程和反向传播过程的输入输出和参数表达式。我们也从提取特征复杂性和计算量的角度分别解释了深层神经网络为什么优于浅层神经网络。接着，我们介绍了超参数的概念，解释了超参数与参数的区别。最后，我们将神经网络与人脑做了类别，人工神经网络是简化的人脑模型。</p><p><strong>至此，Andew深度学习专项课程第一门课《神经网络与深度学习》结束。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170927110934360?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达神经网络与深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《神经网络与深度学习》课程笔记（4）-- 浅层神经网络</title>
    <link href="https://redstonewill.github.io/2018/03/26/37/"/>
    <id>https://redstonewill.github.io/2018/03/26/37/</id>
    <published>2018-03-26T13:06:31.000Z</published>
    <updated>2018-03-27T03:03:07.669Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170927081305727?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的个人网站：<a href="https://redstonewill.github.io/">红色石头的机器学习之路</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="http://weibo.com/redstonewill" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）</p></blockquote><p>上节课我们主要介绍了向量化、矩阵计算的方法和python编程的相关技巧。并以逻辑回归为例，将其算法流程包括梯度下降转换为向量化的形式，从而大大提高了程序运算速度。本节课我们将从浅层神经网络入手，开始真正的神经网络模型的学习。</p><h3 id="Neural-Networks-Overview"><a href="#Neural-Networks-Overview" class="headerlink" title="Neural Networks Overview"></a>Neural Networks Overview</h3><p>首先，我们从整体结构上来大致看一下神经网络模型。</p><p>前面的课程中，我们已经使用计算图的方式介绍了逻辑回归梯度下降算法的正向传播和反向传播两个过程。如下图所示。神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为隐藏层或中间层。这样从计算上来说，神经网络的正向传播和反向传播过程只是比逻辑回归多了一次重复的计算。正向传播过程分成两层，第一层是输入层到隐藏层，用上标[1]来表示：</p><p>$$z^{[1]}=W^{[1]}x+b^{[1]}$$</p><p>$$a^{[1]}=\sigma(z^{[1]})$$</p><p>第二层是隐藏层到输出层，用上标[2]来表示：</p><p>$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$</p><p>$$a^{[2]}=\sigma(z^{[2]})$$</p><p>在写法上值得注意的是，方括号上标[i]表示当前所处的层数；圆括号上标(i)表示第i个样本。</p><p>同样，反向传播过程也分成两层。第一层是输出层到隐藏层，第二层是隐藏层到输入层。其细节部分我们之后再来讨论。</p><p><img src="http://img.blog.csdn.net/20170927081053565?" alt="这里写图片描述"></p><h3 id="Neural-Network-Representation"><a href="#Neural-Network-Representation" class="headerlink" title="Neural Network Representation"></a>Neural Network Representation</h3><p>下面我们以图示的方式来介绍单隐藏层的神经网络结构。如下图所示，单隐藏层神经网络就是典型的浅层（shallow）神经网络。</p><p><img src="http://img.blog.csdn.net/20170927081305727?" alt="这里写图片描述"></p><p> 结构上，从左到右，可以分成三层：输入层（Input layer），隐藏层（Hidden layer）和输出层（Output layer）。输入层和输出层，顾名思义，对应着训练样本的输入和输出，很好理解。隐藏层是抽象的非线性的中间层，这也是其被命名为隐藏层的原因。</p><p>在写法上，我们通常把输入矩阵X记为$a^{[0]}$，把隐藏层输出记为$a^{[1]}$，上标从0开始。用下标表示第几个神经元，注意下标从1开始。例如$a_1^{[1]}$表示隐藏层第1个神经元，$a_2^{[1]}$表示隐藏层第2个神经元,，等等。这样，隐藏层有4个神经元就可以将其输出$a^{[1]}$写成矩阵的形式：</p><p>$$<br> \boldsymbol{a^{[1]}}=<br> \left[<br> \begin{matrix}<br>   a_1^{[1]} \<br>   a_2^{[1]}  \<br>   a_3^{[1]} \<br>   a_4^{[1]}<br>  \end{matrix}<br>  \right]<br>$$</p><p>最后，相应的输出层记为$a^{[2]}$，即$\hat y$。这种单隐藏层神经网络也被称为两层神经网络（2 layer NN）。之所以叫两层神经网络是因为，通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为0的原因（$a^{[0]}$）。</p><p>关于隐藏层对应的权重$W^{[1]}$和常数项$b^{[1]}$，$W^{[1]}$的维度是（4,3）。这里的4对应着隐藏层神经元个数，3对应着输入层x特征向量包含元素个数。常数项$b^{[1]}$的维度是（4,1），这里的4同样对应着隐藏层神经元个数。关于输出层对应的权重$W^{[2]}$和常数项$b^{[2]}$，$W^{[2]}$的维度是（1,4），这里的1对应着输出层神经元个数，4对应着输出层神经元个数。常数项$b^{[2]}$的维度是（1,1），因为输出只有一个神经元。总结一下，第i层的权重$W^{[i]}$维度的行等于i层神经元的个数，列等于i-1层神经元的个数；第i层常数项$b^{[i]}$维度的行等于i层神经元的个数，列始终为1。</p><h3 id="Computing-a-Neural-Network’s-Output"><a href="#Computing-a-Neural-Network’s-Output" class="headerlink" title="Computing a Neural Network’s Output"></a>Computing a Neural Network’s Output</h3><p>接下来我们开始详细推导神经网络的计算过程。回顾一下，我们前面讲过两层神经网络可以看成是逻辑回归再重复计算一次。如下图所示，逻辑回归的正向计算可以分解成计算z和a的两部分：</p><p>$$z=w^Tx+b$$</p><p>$$a=\sigma(z)$$</p><p><img src="http://img.blog.csdn.net/20170927081331442?" alt="这里写图片描述"></p><p>对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要注意对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如$a_i^{[l]}$表示第l层的第i个神经元。注意，i从1开始，l从0开始。</p><p>下面，我们将从输入层到输出层的计算公式列出来：</p><p>$$z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]},\ a_1^{[1]}=\sigma(z_1^{[1]})$$</p><p>$$z_2^{[1]}=w_2^{[1]T}x+b_2^{[1]},\ a_2^{[1]}=\sigma(z_2^{[1]})$$</p><p>$$z_3^{[1]}=w_3^{[1]T}x+b_3^{[1]},\ a_3^{[1]}=\sigma(z_3^{[1]})$$</p><p>$$z_4^{[1]}=w_4^{[1]T}x+b_4^{[1]},\ a_4^{[1]}=\sigma(z_4^{[1]})$$</p><p>然后，从隐藏层到输出层的计算公式为：</p><p>$$z_1^{[2]}=w_1^{[2]T}a^{[1]}+b_1^{[2]},\ a_1^{[2]}=\sigma(z_1^{[2]})$$</p><p>其中$a^{[1]}$为：</p><p>$$<br> \boldsymbol{a^{[1]}}=<br> \left[<br> \begin{matrix}<br>   a_1^{[1]} \<br>   a_2^{[1]}  \<br>   a_3^{[1]} \<br>   a_4^{[1]}<br>  \end{matrix}<br>  \right]<br>$$</p><p>上述每个节点的计算都对应着一次逻辑运算的过程，分别由计算z和a两部分组成。</p><p>为了提高程序运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式：</p><p>$$z^{[1]}=W^{[1]}x+b^{[1]}$$</p><p>$$a^{[1]}=\sigma(z^{[1]})$$</p><p>$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$</p><p>$$a^{[2]}=\sigma(z^{[2]})$$</p><p><img src="http://img.blog.csdn.net/20170927081521144?" alt="这里写图片描述"></p><p>之前也介绍过，这里顺便提一下，$W^{[1]}$的维度是（4,3），$b^{[1]}$的维度是（4,1），$W^{[2]}$的维度是（1,4），$b^{[2]}$的维度是（1,1）。这点需要特别注意。</p><h3 id="Vectorizing-across-multiple-examples"><a href="#Vectorizing-across-multiple-examples" class="headerlink" title="Vectorizing across multiple examples"></a>Vectorizing across multiple examples</h3><p>上一部分我们只是介绍了单个样本的神经网络正向传播矩阵运算过程。而对于m个训练样本，我们也可以使用矩阵相乘的形式来提高计算效率。而且它的形式与上一部分单个样本的矩阵运算十分相似，比较简单。</p><p>之前我们也介绍过，在书写标记上用上标(i)表示第i个样本，例如$x^{(i)}$，$z^{(i)}$，$a^{[2](i)}$。对于每个样本i，可以使用for循环来求解其正向输出：</p><p>for i = 1 to m:<br>$\ \ \ \ z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]}$<br>$\ \ \ \ a^{[1](i)}=\sigma(z^{[1](i)})$<br>$\ \ \ \ z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]}$<br>$\ \ \ \ a^{[2](i)}=\sigma(z^{[2](i)})$</p><p>不使用for循环，利用矩阵运算的思想，输入矩阵X的维度为（$n_x$,m）。这样，我们可以把上面的for循环写成矩阵运算的形式：</p><p>$$Z^{[1]}=W^{[1]}X+b^{[1]}$$</p><p>$$A^{[1]}=\sigma(Z^{[1]})$$</p><p>$$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$$</p><p>$$A^{[2]}=\sigma(Z^{[2]})$$</p><p>其中，$Z^{[1]}$的维度是（4,m），4是隐藏层神经元的个数；$A^{[1]}$的维度与$Z^{[1]}$相同；$Z^{[2]}$和$A^{[2]}$的维度均为（1,m）。对上面这四个矩阵来说，均可以这样来理解：行表示神经元个数，列表示样本数目m。</p><h3 id="Explanation-for-Vectorized-Implementation"><a href="#Explanation-for-Vectorized-Implementation" class="headerlink" title="Explanation for Vectorized Implementation"></a>Explanation for Vectorized Implementation</h3><p>这部分Andrew用图示的方式解释了m个样本的神经网络矩阵运算过程。其实内容比较简单，只要记住上述四个矩阵的行表示神经元个数，列表示样本数目m就行了。</p><p>值得注意的是输入矩阵X也可以写成$A^{[0]}$。</p><h3 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h3><p>神经网络隐藏层和输出层都需要激活函数（activation function），在之前的课程中我们都默认使用Sigmoid函数$\sigma(x)$作为激活函数。其实，还有其它激活函数可供使用，不同的激活函数有各自的优点。下面我们就来介绍几个不同的激活函数$g(x)$。</p><ul><li>sigmoid函数</li></ul><p><img src="http://img.blog.csdn.net/20170920120759651?" alt="这里写图片描述"></p><ul><li>tanh函数</li></ul><p><img src="http://img.blog.csdn.net/20170919091551404?" alt="这里写图片描述"></p><ul><li>ReLU函数</li></ul><p><img src="http://img.blog.csdn.net/20170919091933651?" alt="这里写图片描述"></p><ul><li>Leaky ReLU函数</li></ul><p><img src="http://img.blog.csdn.net/20170919092253605?" alt="这里写图片描述"></p><p>如上图所示，不同激活函数形状不同，a的取值范围也有差异。</p><p>如何选择合适的激活函数呢？首先我们来比较sigmoid函数和tanh函数。对于隐藏层的激活函数，一般来说，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果。因此，隐藏层的激活函数，tanh比sigmoid更好一些。而对于输出层的激活函数，因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数。</p><p>观察sigmoid函数和tanh函数，我们发现有这样一个问题，就是当|z|很大的时候，激活函数的斜率（梯度）很小。因此，在这个区域内，梯度下降算法会运行得比较慢。在实际应用中，应尽量避免使z落在这个区域，使|z|尽可能限定在零值附近，从而提高梯度下降算法运算速度。</p><p>为了弥补sigmoid函数和tanh函数的这个缺陷，就出现了ReLU激活函数。ReLU激活函数在z大于零时梯度始终为1；在z小于零时梯度始终为0；z等于零时的梯度可以当成1也可以当成0，实际应用中并不影响。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点，实际应用中，这个缺点影响不是很大。为了弥补这个缺点，出现了Leaky ReLU激活函数，能够保证z小于零是梯度不为0。</p><p>最后总结一下，如果是分类问题，输出层的激活函数一般会选择sigmoid函数。但是隐藏层的激活函数通常不会选择sigmoid函数，tanh函数的表现会比sigmoid函数好一些。实际应用中，通常会会选择使用ReLU或者Leaky ReLU函数，保证梯度下降速度不会太小。其实，具体选择哪个函数作为激活函数没有一个固定的准确的答案，应该要根据具体实际问题进行验证（validation）。</p><h3 id="Why-do-you-need-non-linear-activation-functions"><a href="#Why-do-you-need-non-linear-activation-functions" class="headerlink" title="Why do you need non-linear activation functions"></a>Why do you need non-linear activation functions</h3><p>我们知道上一部分讲的四种激活函数都是非线性（non-linear）的。那是否可以使用线性激活函数呢？答案是不行！下面我们就来进行简要的解释和说明。</p><p>假设所有的激活函数都是线性的，为了简化计算，我们直接令激活函数$g(z)=z$，即$a=z$。那么，浅层神经网络的各层输出为：</p><p>$$z^{[1]}=W^{[1]}x+b^{[1]}$$</p><p>$$a^{[1]}=z^{[1]}$$</p><p>$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$</p><p>$$a^{[2]}=z^{[2]}$$</p><p>我们对上式中$a^{[2]}$进行化简计算：</p><p>$$a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W’x+b’$$</p><p>经过推导我们发现$a^{[2]}$仍是输入变量x的线性组合。这表明，使用神经网络与直接使用线性模型的效果并没有什么两样。即便是包含多层隐藏层的神经网络，如果使用线性函数作为激活函数，最终的输出仍然是输入x的线性模型。这样的话神经网络就没有任何作用了。因此，隐藏层的激活函数必须要是非线性的。</p><p>另外，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。</p><p>值得一提的是，如果是预测问题而不是分类问题，输出y是连续的情况下，输出层的激活函数可以使用线性函数。如果输出y恒为正值，则也可以使用ReLU激活函数，具体情况，具体分析。</p><h3 id="Derivatives-of-activation-functions"><a href="#Derivatives-of-activation-functions" class="headerlink" title="Derivatives of activation functions"></a>Derivatives of activation functions</h3><p>在梯度下降反向计算过程中少不了计算激活函数的导数即梯度。</p><p>我们先来看一下sigmoid函数的导数：</p><p>$$g(z)=\frac{1}{1+e^{(-z)}}$$</p><p>$$g’(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))=a(1-a)$$</p><p>对于tanh函数的导数：</p><p>$$g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}}$$</p><p>$$g’(z)=\frac{d}{dz}g(z)=1-(g(z))^2=1-a^2$$</p><p>对于ReLU函数的导数：</p><p>$$g(z)=max(0,z)$$</p><p>$$g’(z)=\begin{cases}<br>        0, &amp; z&lt;0\<br>        1, &amp; z\geq0<br>    \end{cases}$$</p><p>对于Leaky ReLU函数：</p><p>$$g(z)=max(0.01z,z)$$</p><p>$$g’(z)=\begin{cases}<br>        0.01, &amp; z&lt;0\<br>        1, &amp; z\geq0<br>    \end{cases}$$</p><h3 id="Gradient-descent-for-neural-networks"><a href="#Gradient-descent-for-neural-networks" class="headerlink" title="Gradient descent for neural networks"></a>Gradient descent for neural networks</h3><p>接下来看一下在神经网络中如何进行梯度计算。</p><p>仍然是浅层神经网络，包含的参数为$W^{[1]}$，$b^{[1]}$，$W^{[2]}$，$b^{[2]}$。令输入层的特征向量个数$n_x=n^{[0]}$，隐藏层神经元个数为$n^{[1]}$，输出层神经元个数为$n^{[2]}=1$。则$W^{[1]}$的维度为（$n^{[1]}$,$n^{[0]}$），$b^{[1]}$的维度为（$n^{[1]}$,1），$W^{[2]}$的维度为（$n^{[2]}$,$n^{[1]}$），$b^{[2]}$的维度为（$n^{[2]}$,1）。</p><p>该神经网络正向传播过程为：</p><p>$$Z^{[1]}=W^{[1]}X+b^{[1]}$$</p><p>$$A^{[1]}=g(Z^{[1]})$$</p><p>$$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$$</p><p>$$A^{[2]}=g(Z^{[2]})$$</p><p>其中，$g(\cdot)$表示激活函数。</p><p>反向传播是计算导数（梯度）的过程，这里先列出来Cost function对各个参数的梯度：</p><p>$$dZ^{[2]}=A^{[2]}-Y$$</p><p>$$dW^{[2]}=\frac1mdZ^{[2]}A^{[1]T}$$</p><p>$$db^{[2]}=\frac1mnp.sum(dZ^{[2]},axis=1,keepdim=True)$$</p><p>$$dZ^{[1]}=W^{[2]T}dZ^{[2]}\ast g’(Z^{[1]})$$</p><p>$$dW^{[1]}=\frac1mdZ^{[1]}X^T$$</p><p>$$db^{[1]}=\frac1mnp.sum(dZ^{[1]},axis=1,keepdim=True)$$</p><p>反向传播的具体推导过程我们下一部分再进行详细说明。</p><h3 id="Backpropagation-intuition-optional"><a href="#Backpropagation-intuition-optional" class="headerlink" title="Backpropagation intuition(optional)"></a>Backpropagation intuition(optional)</h3><p>我们仍然使用计算图的方式来推导神经网络反向传播过程。记得之前介绍逻辑回归时，我们就引入了计算图来推导正向传播和反向传播，其过程如下图所示：</p><p><img src="http://img.blog.csdn.net/20170927082308231?" alt="这里写图片描述"></p><p>由于多了一个隐藏层，神经网络的计算图要比逻辑回归的复杂一些，如下图所示。对于单个训练样本，正向过程很容易，反向过程可以根据梯度计算方法逐一推导。</p><p>$$dz^{[2]}=a^{[2]}-y$$</p><p>$$dW^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}=dz^{[2]}a^{[1]T}$$</p><p>$$db^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=dz^{[2]}\cdot 1=dz^{[2]}$$</p><p>$$dz^{[1]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}\cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2]T}dz^{[2]}\ast g^{[1]’}(z^{[1]})$$</p><p>$$dW^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=dz^{[1]}x^T$$</p><p>$$db^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=dz^{[1]}\cdot 1=dz^{[1]}$$</p><p><img src="http://img.blog.csdn.net/20170927082510979?" alt="这里写图片描述"></p><p>总结一下，浅层神经网络（包含一个隐藏层），m个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示：</p><p><img src="http://img.blog.csdn.net/20170927082924477?" alt="这里写图片描述"></p><h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h3><p>神经网络模型中的参数权重W是不能全部初始化为零的，接下来我们分析一下原因。</p><p>举个简单的例子，一个浅层神经网络包含两个输入，隐藏层包含两个神经元。如果权重$W^{[1]}$和$W^{[2]}$都初始化为零，即：</p><p>$$W^{[1]}=<br> \left[<br> \begin{matrix}<br>   0 &amp; 0 \<br>   0 &amp; 0<br>  \end{matrix}<br>  \right]<br>$$</p><p>$$W^{[2]}=<br> \left[<br> \begin{matrix}<br>   0 &amp; 0<br>  \end{matrix}<br>  \right]<br>$$</p><p>这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即$a_1^{[1]}=a_2^{[1]}$。经过推导得到$dz_1^{[1]}=dz_2^{[1]}$，以及$dW_1^{[1]}=dW_2^{[1]}$。因此，这样的结果是隐藏层两个神经元对应的权重行向量$W_1^{[1]}$和$W_2^{[1]}$每次迭代更新都会得到完全相同的结果，$W_1^{[1]}$始终等于$W_2^{[1]}$，完全对称。这样隐藏层设置多个神经元就没有任何意义了。值得一提的是，参数b可以全部初始化为零，并不会影响神经网络训练效果。</p><p><img src="http://img.blog.csdn.net/20170927082954620?" alt="这里写图片描述"></p><p>我们把这种权重W全部初始化为零带来的问题称为symmetry breaking problem。解决方法也很简单，就是将W进行随机初始化（b可初始化为零）。python里可以使用如下语句进行W和b的初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_1 = np.random.randn((<span class="number">2</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b_1 = np.zero((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">W_2 = np.random.randn((<span class="number">1</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b_2 = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>这里我们将$W_1^{[1]}$和$W_2^{[1]}$乘以0.01的目的是尽量使得权重W初始化比较小的值。之所以让W比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，W比较小，得到的|z|也比较小（靠近零点），而零点区域的梯度比较大，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解。如果W较大，得到的|z|也比较大，附近曲线平缓，梯度较小，训练过程会慢很多。</p><p>当然，如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题。但是，如果输出层是sigmoid函数，则对应的权重W最好初始化到比较小的值。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了浅层神经网络。首先，我们简单概述了神经网络的结构：包括输入层，隐藏层和输出层。然后，我们以计算图的方式推导了神经网络的正向输出，并以向量化的形式归纳出来。接着，介绍了不同的激活函数并做了比较，实际应用中根据不同需要选择合适的激活函数。激活函数必须是非线性的，不然神经网络模型起不了任何作用。然后，我们重点介绍了神经网络的反向传播过程以及各个参数的导数推导，并以矩阵形式表示出来。最后，介绍了权重随机初始化的重要性，必须对权重W进行随机初始化操作。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170927081305727?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达神经网络与深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《神经网络与深度学习》课程笔记（3）-- 神经网络基础之Python与向量化</title>
    <link href="https://redstonewill.github.io/2018/03/22/36/"/>
    <id>https://redstonewill.github.io/2018/03/22/36/</id>
    <published>2018-03-22T05:36:02.000Z</published>
    <updated>2018-03-22T05:39:25.862Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170926163908656?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了逻辑回归，以输出概率的形式来处理二分类问题。我们介绍了逻辑回归的Cost function表达式，并使用梯度下降算法来计算最小化Cost function时对应的参数w和b。通过计算图的方式来讲述了神经网络的正向传播和反向传播两个过程。本节课我们将来探讨Python和向量化的相关知识。</p><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>深度学习算法中，数据量很大，在程序中应该尽量减少使用loop循环语句，而可以使用向量运算来提高程序运行速度。</p><p>向量化（Vectorization）就是利用矩阵运算的思想，大大提高运算速度。例如下面所示在Python中使用向量化要比使用循环计算速度快得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"Vectorized version:"</span> + str(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"for loop:"</span> + str(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)</span><br></pre></td></tr></table></figure><p>输出结果类似于：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">250286.989866</span></span><br><span class="line">Vectorized version:<span class="number">1.5027523040771484</span>ms</span><br><span class="line"><span class="number">250286.989866</span></span><br><span class="line">For loop:<span class="number">474.29513931274414</span>ms</span><br></pre></td></tr></table></figure><p>从程序运行结果上来看，该例子使用for循环运行时间是使用向量运算运行时间的约300倍。因此，深度学习算法中，使用向量化矩阵运算的效率要高得多。</p><p>为了加快深度学习神经网络运算速度，可以使用比CPU运算能力更强大的GPU。事实上，GPU和CPU都有并行指令（parallelization instructions），称为Single Instruction Multiple Data（SIMD）。SIMD是单指令多数据流，能够复制多个操作数，并把它们打包在大型寄存器的一组指令集。SIMD能够大大提高程序运行速度，例如python的numpy库中的内建函数（built-in function）就是使用了SIMD指令。相比而言，GPU的SIMD要比CPU更强大一些。</p><h3 id="More-Vectorization-Examples"><a href="#More-Vectorization-Examples" class="headerlink" title="More Vectorization Examples"></a>More Vectorization Examples</h3><p>上一部分我们讲了应该尽量避免使用for循环而使用向量化矩阵运算。在python的numpy库中，我们通常使用np.dot()函数来进行矩阵运算。</p><p>我们将向量化的思想使用在逻辑回归算法上，尽可能减少for循环，而只使用矩阵运算。值得注意的是，算法最顶层的迭代训练的for循环是不能替换的。而每次迭代过程对J，dw，b的计算是可以直接使用矩阵运算。</p><h3 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h3><p>在《神经网络与深度学习》课程笔记（2）中我们介绍过，整个训练样本构成的输入矩阵X的维度是（$n_x$，m），权重矩阵w的维度是（$n_x$，1），b是一个常数值，而整个训练样本构成的输出矩阵Y的维度为（1，m）。利用向量化的思想，所有m个样本的线性输出Z可以用矩阵表示：</p><p>$$Z=w^TX+b$$</p><p>在python的numpy库中可以表示为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure><p>其中，w.T表示w的转置。</p><p>这样，我们就能够使用向量化矩阵运算代替for循环，对所有m个样本同时运算，大大提高了运算速度。</p><h3 id="Vectorizing-Logistic-Regression’s-Gradient-Output"><a href="#Vectorizing-Logistic-Regression’s-Gradient-Output" class="headerlink" title="Vectorizing Logistic Regression’s Gradient Output"></a>Vectorizing Logistic Regression’s Gradient Output</h3><p>再来看逻辑回归中的梯度下降算法如何转化为向量化的矩阵形式。对于所有m个样本，dZ的维度是（1，m），可表示为：</p><p>$$dZ=A-Y$$</p><p>db可表示为：</p><p>$$db=\frac1m \sum_{i=1}^mdz^{(i)}$$</p><p>对应的程序为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db = <span class="number">1</span>/m*np.sum(dZ)</span><br></pre></td></tr></table></figure><p>dw可表示为：</p><p>$$dw=\frac1m X\cdot dZ^T$$</p><p>对应的程序为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br></pre></td></tr></table></figure><p>这样，我们把整个逻辑回归中的for循环尽可能用矩阵运算代替，对于单次迭代，梯度下降算法流程如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A-Y</span><br><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br><span class="line">db = <span class="number">1</span>/m*np.sum(dZ)</span><br><span class="line"></span><br><span class="line">w = w - alpha*dw</span><br><span class="line">b = b - alpha*db</span><br></pre></td></tr></table></figure><p>其中，alpha是学习因子，决定w和b的更新速度。上述代码只是对单次训练更新而言的，外层还需要一个for循环，表示迭代次数。</p><h3 id="Broadcasting-in-Python"><a href="#Broadcasting-in-Python" class="headerlink" title="Broadcasting in Python"></a>Broadcasting in Python</h3><p>下面介绍使用python的另一种技巧：广播（Broadcasting）。python中的广播机制可由下面四条表示：</p><ul><li><p><strong>让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐</strong></p></li><li><p><strong>输出数组的shape是输入数组shape的各个轴上的最大值</strong></p></li><li><p><strong>如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错</strong></p></li><li><p><strong>当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值</strong></p></li></ul><p>简而言之，就是python中可以对不同维度的矩阵进行四则混合运算，但至少保证有一个维度是相同的。下面给出几个广播的例子，具体细节可参阅python的相关手册，这里就不赘述了。</p><p><img src="http://img.blog.csdn.net/20170926163908656?" alt="这里写图片描述"></p><p>值得一提的是，在python程序中为了保证矩阵运算正确，可以使用reshape()函数来对矩阵设定所需的维度。这是一个很好且有用的习惯。</p><h3 id="A-note-on-python-numpy-vectors"><a href="#A-note-on-python-numpy-vectors" class="headerlink" title="A note on python/numpy vectors"></a>A note on python/numpy vectors</h3><p>接下来我们将总结一些python的小技巧，避免不必要的code bug。</p><p>python中，如果我们用下列语句来定义一个向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>这条语句生成的a的维度是（5，）。它既不是行向量也不是列向量，我们把a叫做rank 1 array。这种定义会带来一些问题。例如我们对a进行转置，还是会得到a本身。所以，如果我们要定义（5，1）的列向量或者（1，5）的行向量，最好使用下来标准语句，避免使用rank 1 array。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>除此之外，我们还可以使用assert语句对向量或数组的维度进行判断，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape == (<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>assert会对内嵌语句进行判断，即判断a的维度是不是（5，1）的。如果不是，则程序在此处停止。使用assert语句也是一种很好的习惯，能够帮助我们及时检查、发现语句是否正确。</p><p>另外，还可以使用reshape函数对数组设定所需的维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.reshape((<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="Quick-tour-of-Jupyter-iPython-Notebooks"><a href="#Quick-tour-of-Jupyter-iPython-Notebooks" class="headerlink" title="Quick tour of Jupyter/iPython Notebooks"></a>Quick tour of Jupyter/iPython Notebooks</h3><p>Jupyter notebook（又称IPython notebook）是一个交互式的笔记本，支持运行超过40种编程语言。本课程所有的编程练习题都将在Jupyter notebook上进行，使用的语言是python。</p><p>关于Jupyter notebook的简介和使用方法可以看我的另外两篇博客：</p><p><a href="http://blog.csdn.net/red_stone1/article/details/72858962" target="_blank" rel="noopener">Jupyter notebook入门教程（上）</a></p><p><a href="http://blog.csdn.net/red_stone1/article/details/72863749" target="_blank" rel="noopener">Jupyter notebook入门教程（下）</a></p><h3 id="Explanation-of-logistic-regression-cost-function-optional"><a href="#Explanation-of-logistic-regression-cost-function-optional" class="headerlink" title="Explanation of logistic regression cost function(optional)"></a>Explanation of logistic regression cost function(optional)</h3><p>在上一节课的笔记中，我们介绍过逻辑回归的Cost function。接下来我们将简要解释这个Cost function是怎么来的。</p><p>首先，预测输出$\hat y$的表达式可以写成：</p><p>$$\hat y=\sigma(w^Tx+b)$$</p><p>其中，$\sigma(z)=\frac{1}{1+exp(-z)}$。$\hat y$可以看成是预测输出为正类（+1）的概率：</p><p>$$\hat y=P(y=1|x)$$</p><p>那么，当y=1时：</p><p>$$p(y|x)=\hat y$$</p><p>当y=0时：</p><p>$$p(y|x)=1-\hat y$$</p><p>我们把上面两个式子整合到一个式子中，得到：</p><p>$$P(y|x)=\hat y^y(1-\hat y)^{(1-y)}$$</p><p>由于log函数的单调性，可以对上式P(y|x)进行log处理：</p><p>$$log\ P(y|x)=log\ \hat y^y(1-\hat y)^{(1-y)}=y\ log\ \hat y+(1-y)log(1-\hat y)$$</p><p>我们希望上述概率P(y|x)越大越好，对上式加上负号，则转化成了单个样本的Loss function，越小越好，也就得到了我们之前介绍的逻辑回归的Loss function形式。</p><p>$$L=-(y\ log\ \hat y+(1-y)log(1-\hat y))$$</p><p>如果对于所有m个训练样本，假设样本之间是独立同分布的（iid），我们希望总的概率越大越好：</p><p>$$max\ \prod_{i=1}^m\ P(y^{(i)}|x^{(i)})$$</p><p>同样引入log函数，加上负号，将上式转化为Cost function：</p><p>$$J(w,b)=-\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac 1m\sum_{i=1}^my^{(i)}\ log\ \hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})$$</p><p>上式中，$\frac1m$表示对所有m个样本的Cost function求平均，是缩放因子。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课我们主要介绍了神经网络基础——python和向量化。在深度学习程序中，使用向量化和矩阵运算的方法能够大大提高运行速度，节省时间。以逻辑回归为例，我们将其算法流程包括梯度下降转换为向量化的形式。同时，我们也介绍了python的相关编程方法和技巧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170926163908656?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达神经网络与深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础之逻辑回归</title>
    <link href="https://redstonewill.github.io/2018/03/20/35/"/>
    <id>https://redstonewill.github.io/2018/03/20/35/</id>
    <published>2018-03-20T12:06:57.000Z</published>
    <updated>2018-03-20T12:18:40.344Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170926082756271?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要对深度学习（Deep Learning）的概念做了简要的概述。我们先从房价预测的例子出发，建立了标准的神经网络（Neural Network）模型结构。然后从监督式学习入手，介绍了Standard NN，CNN和RNN三种不同的神经网络模型。接着介绍了两种不同类型的数据集：Structured Data和Unstructured Data。最后，我们解释了近些年来深度学习性能优于传统机器学习的原因，归结为三个因素：Data，Computation和Algorithms。本节课，我们将开始介绍神经网络的基础：逻辑回归（Logistic Regression）。通过对逻辑回归模型结构的分析，为我们后面学习神经网络模型打下基础。</p><h3 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h3><p>我们知道逻辑回归模型一般用来解决二分类（Binary Classification）问题。二分类就是输出y只有{0,1}两个离散值（也有{-1,1}的情况）。我们以一个图像识别问题为例，判断图片中是否有猫存在，0代表noncat，1代表cat。主要是通过这个例子简要介绍神经网络模型中一些标准化的、有效率的处理方法和notations。</p><p><img src="http://img.blog.csdn.net/20170926082756271?" alt="这里写图片描述"></p><p>如上图所示，这是一个典型的二分类问题。一般来说，彩色图片包含RGB三个通道。例如该cat图片的尺寸为（64，64，3）。在神经网络模型中，我们首先要将图片输入x（维度是（64，64，3））转化为一维的特征向量（feature vector）。方法是每个通道一行一行取，再连接起来。由于64x64x3=12288，则转化后的输入特征向量维度为（12288，1）。此特征向量x是列向量，维度一般记为$n_x$。</p><p>如果训练样本共有m张图片，那么整个训练样本X组成了矩阵，维度是（$n_x$，m）。注意，这里矩阵X的行$n_x$代表了每个样本$x^{(i)}$特征个数，列m代表了样本个数。这里，Andrew解释了X的维度之所以是（$n_x$，m）而不是（m，$n_x$）的原因是为了之后矩阵运算的方便。算是Andrew给我们的一个小小的经验吧。而所有训练样本的输出Y也组成了一维的行向量，写成矩阵的形式后，它的维度就是（1，m）。</p><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>接下来我们就来介绍如何使用逻辑回归来解决二分类问题。逻辑回归中，预测值$\hat h=P(y=1\ |\ x)$表示为1的概率，取值范围在[0,1]之间。这是其与二分类模型不同的地方。使用线性模型，引入参数w和b。权重w的维度是（$n_x$，1），b是一个常数项。这样，逻辑回归的线性预测输出可以写成：</p><p>$$\hat y = w^Tx+b$$</p><p>值得注意的是，很多其它机器学习资料中，可能把常数b当做$w_0$处理，并引入$x_0=1$。这样从维度上来看，x和w都会增加一维。但在本课程中，为了简化计算和便于理解，Andrew建议还是使用上式这种形式将w和b分开比较好。</p><p>上式的线性输出区间为整个实数范围，而逻辑回归要求输出范围在[0,1]之间，所以还需要对上式的线性函数输出进行处理。方法是引入Sigmoid函数，让输出限定在[0,1]之间。这样，逻辑回归的预测输出就可以完整写成：</p><p>$$\hat y = Sigmoid(w^Tx+b)=\sigma(w^Tx+b)$$</p><p>Sigmoid函数是一种非线性的S型函数，输出被限定在[0,1]之间，通常被用在神经网络中当作激活函数（Activation function）使用。Sigmoid函数的表达式和曲线如下所示：</p><p>$$Sigmoid(z)=\frac{1}{1+e^{-z}}$$</p><p><img src="http://img.blog.csdn.net/20170906231842576?" alt="这里写图片描述"></p><p>从Sigmoid函数曲线可以看出，当z值很大时，函数值趋向于1；当z值很小时，函数值趋向于0。且当z=0时，函数值为0.5。还有一点值得注意的是，Sigmoid函数的一阶导数可以用其自身表示：</p><p>$$\sigma’(z)=\sigma(z)(1-\sigma(z))$$</p><p>这样，通过Sigmoid函数，就能够将逻辑回归的输出限定在[0,1]之间了。</p><h3 id="Logistic-Regression-Cost-Function"><a href="#Logistic-Regression-Cost-Function" class="headerlink" title="Logistic Regression Cost Function"></a>Logistic Regression Cost Function</h3><p>逻辑回归中，w和b都是未知参数，需要反复训练优化得到。因此，我们需要定义一个cost function，包含了参数w和b。通过优化cost function，当cost function取值最小时，得到对应的w和b。</p><p>提一下，对于m个训练样本，我们通常使用上标来表示对应的样本。例如$(x^{(i)},y^{(i)})$表示第i个样本。</p><p>如何定义所有m个样本的cost function呢？先从单个样本出发，我们希望该样本的预测值$\hat y$与真实值越相似越好。我们把单个样本的cost function用Loss function来表示，根据以往经验，如果使用平方错误（squared error）来衡量，如下所示：</p><p>$$L(\hat y,y)=\frac12(\hat y-y)^2$$</p><p>但是，对于逻辑回归，我们一般不使用平方错误来作为Loss function。原因是这种Loss function一般是non-convex的。non-convex函数在使用梯度下降算法时，容易得到局部最小值（local minumum），即局部最优化。而我们最优化的目标是计算得到全局最优化（Global optimization）。因此，我们一般选择的Loss function应该是convex的。</p><p>Loss function的原则和目的就是要衡量预测输出$\hat y$与真实样本输出y的接近程度。平方错误其实也可以，只是它是non-convex的，不利于使用梯度下降算法来进行全局优化。因此，我们可以构建另外一种Loss function，且是convex的，如下所示：</p><p>$$L(\hat y,y)=-(ylog\ \hat y+(1-y)log\ (1-\hat y))$$</p><p>我们来分析一下这个Loss function，它是衡量错误大小的，Loss function越小越好。</p><p>当y=1时，$L(\hat y,y)=-log\ \hat y$。如果$\hat y$越接近1，$L(\hat y,y)\approx 0$，表示预测效果越好；如果$\hat y$越接近0，$L(\hat y,y)\approx +\infty$，表示预测效果越差。这正是我们希望Loss function所实现的功能。</p><p>当y=0时，$L(\hat y,y)=-log\ (1-\hat y)$。如果$\hat y$越接近0，$L(\hat y,y)\approx 0$，表示预测效果越好；如果$\hat y$越接近1，$L(\hat y,y)\approx +\infty$，表示预测效果越差。这也正是我们希望Loss function所实现的功能。</p><p>因此，这个Loss function能够很好地反映预测输出$\hat y$与真实样本输出y的接近程度，越接近的话，其Loss function值越小。而且这个函数是convex的。上面我们只是简要地分析为什么要使用这个Loss function，后面的课程中，我们将详细推导该Loss function是如何得到的。并不是凭空捏造的哦。。。</p><p>还要提一点的是，上面介绍的Loss function是针对单个样本的。那对于m个样本，我们定义Cost function，Cost function是m个样本的Loss function的平均值，反映了m个样本的预测输出$\hat y$与真实样本输出y的平均接近程度。Cost function可表示为：</p><p>$$J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac1m\sum_{i=1}^m[y^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})]$$</p><p>Cost function已经推导出来了，Cost function是关于待求系数w和b的函数。我们的目标就是迭代计算出最佳的w和b值，最小化Cost function，让Cost function尽可能地接近于零。</p><p>其实逻辑回归问题可以看成是一个简单的神经网络，只包含一个神经元。这也是我们这里先介绍逻辑回归的原因。</p><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>我们已经掌握了Cost function的表达式，接下来将使用梯度下降（Gradient Descent）算法来计算出合适的w和b值，从而最小化m个训练样本的Cost function，即J(w,b)。</p><p>由于J(w,b)是convex function，梯度下降算法是先随机选择一组参数w和b值，然后每次迭代的过程中分别沿着w和b的梯度（偏导数）的反方向前进一小步，不断修正w和b。每次迭代更新w和b后，都能让J(w,b)更接近全局最小值。梯度下降的过程如下图所示。</p><p><img src="http://img.blog.csdn.net/20170926083044469?" alt="这里写图片描述"></p><p>梯度下降算法每次迭代更新，w和b的修正表达式为：</p><p>$$w:=w-\alpha\frac{\partial J(w,b)}{\partial w}$$</p><p>$$b:=b-\alpha\frac{\partial J(w,b)}{\partial b}$$</p><p>上式中，$\alpha$是学习因子（learning rate），表示梯度下降的步进长度。$\alpha$越大，w和b每次更新的“步伐”更大一些；$\alpha$越小，w和b每次更新的“步伐”更小一些。在程序代码中，我们通常使用dw来表示$\frac{\partial J(w,b)}{\partial w}$，用db来表示$\frac{\partial J(w,b)}{\partial b}$。微积分里，$\frac{df}{dx}$表示对单一变量求导数，$\frac{\partial f}{\partial x}$表示对多个变量中某个变量求偏导数。</p><p>梯度下降算法能够保证每次迭代w和b都能向着J(w,b)全局最小化的方向进行。其数学原理主要是运用泰勒一阶展开来证明的，可以参考我的另一篇博客中的Gradient Descent有提到如何推导：<a href="http://blog.csdn.net/red_stone1/article/details/72229903" target="_blank" rel="noopener">台湾大学林轩田机器学习基石课程学习笔记10 – Logistic Regression</a></p><h3 id="Derivatives"><a href="#Derivatives" class="headerlink" title="Derivatives"></a>Derivatives</h3><p>这一部分的内容非常简单，Andrew主要是给对微积分、求导数不太清楚的同学介绍的。梯度或者导数一定程度上可以看成是斜率。关于求导数的方法这里就不再赘述了。</p><h3 id="More-Derivative-Examples"><a href="#More-Derivative-Examples" class="headerlink" title="More Derivative Examples"></a>More Derivative Examples</h3><p>Andrew给出了更加复杂的求导数的例子，略。</p><h3 id="Computation-graph"><a href="#Computation-graph" class="headerlink" title="Computation graph"></a>Computation graph</h3><p>整个神经网络的训练过程实际上包含了两个过程：正向传播（Forward Propagation）和反向传播（Back Propagation）。正向传播是从输入到输出，由神经网络计算得到预测输出的过程；反向传播是从输出到输入，对参数w和b计算梯度的过程。下面，我们用计算图（Computation graph）的形式来理解这两个过程。</p><p>举个简单的例子，假如Cost function为J(a,b,c)=3(a+bc)，包含a，b，c三个变量。我们用u表示bc，v表示a+u，则J=3v。它的计算图可以写成如下图所示：</p><p><img src="http://img.blog.csdn.net/20170926083150863?" alt="这里写图片描述"></p><p>令a=5，b=3，c=2，则u=bc=6，v=a+u=11，J=3v=33。计算图中，这种从左到右，从输入到输出的过程就对应着神经网络或者逻辑回归中输入与权重经过运算计算得到Cost function的正向过程。</p><h3 id="Derivatives-with-a-Computation-Graph"><a href="#Derivatives-with-a-Computation-Graph" class="headerlink" title="Derivatives with a Computation Graph"></a>Derivatives with a Computation Graph</h3><p>上一部分介绍的是计算图的正向传播（Forward Propagation），下面我们来介绍其反向传播（Back Propagation），即计算输出对输入的偏导数。</p><p>还是上个计算图的例子，输入参数有3个，分别是a，b，c。</p><p>首先计算J对参数a的偏导数。从计算图上来看，从右到左，J是v的函数，v是a的函数。则利用求导技巧，可以得到：</p><p>$$\frac{\partial J}{\partial a}=\frac{\partial J}{\partial v}\cdot \frac{\partial v}{\partial a}=3\cdot 1=3$$</p><p>根据这种思想，然后计算J对参数b的偏导数。从计算图上来看，从右到左，J是v的函数，v是u的函数，u是b的函数。可以推导：</p><p>$$\frac{\partial J}{\partial b}=\frac{\partial J}{\partial v}\cdot \frac{\partial v}{\partial u}\cdot \frac{\partial u}{\partial b}=3\cdot 1\cdot c=3\cdot 1\cdot 2=6$$</p><p>最后计算J对参数c的偏导数。仍从计算图上来看，从右到左，J是v的函数，v是u的函数，u是c的函数。可以推导：</p><p>$$\frac{\partial J}{\partial c}=\frac{\partial J}{\partial v}\cdot \frac{\partial v}{\partial u}\cdot \frac{\partial u}{\partial c}=3\cdot 1\cdot b=3\cdot 1\cdot 3=9$$</p><p>为了统一格式，在程序代码中，我们使用da，db，dc来表示J对参数a，b，c的偏导数。</p><p><img src="http://img.blog.csdn.net/20170926083227668?" alt="这里写图片描述"></p><h3 id="Logistic-Regression-Gradient-Descent"><a href="#Logistic-Regression-Gradient-Descent" class="headerlink" title="Logistic Regression Gradient Descent"></a>Logistic Regression Gradient Descent</h3><p>现在，我们将对逻辑回归进行梯度计算。对单个样本而言，逻辑回归Loss function表达式如下：</p><p>$$z=w^Tx+b$$</p><p>$$\hat y=a=\sigma(z)$$</p><p>$$L(a,y)=-(ylog(a)+(1-y)log(1-a))$$</p><p>首先，该逻辑回归的正向传播过程非常简单。根据上述公式，例如输入样本x有两个特征$(x_1,x_2)$，相应的权重w维度也是2，即$(w_1,w_2)$。则$z=w_1x_1+w_2x_2+b$，最后的Loss function如下所示：</p><p><img src="http://img.blog.csdn.net/20170926083257736?" alt="这里写图片描述"></p><p>然后，计算该逻辑回归的反向传播过程，即由Loss function计算参数w和b的偏导数。推导过程如下：</p><p>$$da=\frac{\partial L}{\partial a}=-\frac ya+\frac{1-y}{1-a}$$</p><p>$$dz=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z}=(-\frac ya+\frac{1-y}{1-a})\cdot a(1-a)=a-y$$</p><p>知道了dz之后，就可以直接对$w_1$，$w_2$和b进行求导了。</p><p>$$dw_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_1}=x_1\cdot dz=x_1(a-y)$$</p><p>$$dw_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_2}=x_2\cdot dz=x_2(a-y)$$</p><p>$$db=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial b}=1\cdot dz=a-y$$</p><p>则梯度下降算法可表示为：</p><p>$$w_1:=w_1-\alpha\ dw_1$$</p><p>$$w_2:=w_2-\alpha\ dw_2$$</p><p>$$b:=b-\alpha\ db$$</p><p><img src="http://img.blog.csdn.net/20170926083336211?" alt="这里写图片描述"></p><h3 id="Gradient-descent-on-m-examples"><a href="#Gradient-descent-on-m-examples" class="headerlink" title="Gradient descent on m examples"></a>Gradient descent on m examples</h3><p>上一部分讲的是对单个样本求偏导和梯度下降。如果有m个样本，其Cost function表达式如下：</p><p>$$z^{(i)}=w^Tx^{(i)}+b$$</p><p>$$\hat y^{(i)}=a^{(i)}=\sigma(z^{(i)})$$</p><p>$$J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac1m\sum_{i=1}^m[y^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})]$$</p><p>Cost function关于w和b的偏导数可以写成和平均的形式：</p><p>$$dw_1=\frac1m\sum_{i=1}^mx_1^{(i)}(a^{(i)}-y^{(i)})$$</p><p>$$dw_2=\frac1m\sum_{i=1}^mx_2^{(i)}(a^{(i)}-y^{(i)})$$</p><p>$$db=\frac1m\sum_{i=1}^m(a^{(i)}-y^{(i)})$$</p><p>这样，每次迭代中w和b的梯度有m个训练样本计算平均值得到。其算法流程图如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">J=0; dw1=0; dw2=0; db=0;</span><br><span class="line">for i = 1 to m</span><br><span class="line">z(i) = wx(i)+b;</span><br><span class="line">a(i) = sigmoid(z(i));</span><br><span class="line">J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i));</span><br><span class="line">dz(i) = a(i)-y(i);</span><br><span class="line">dw1 += x1(i)dz(i);</span><br><span class="line">dw2 += x2(i)dz(i);</span><br><span class="line">db += dz(i);</span><br><span class="line">J /= m;</span><br><span class="line">dw1 /= m;</span><br><span class="line">dw2 /= m;</span><br><span class="line">db /= m;</span><br></pre></td></tr></table></figure><p>经过每次迭代后，根据梯度下降算法，w和b都进行更新：</p><p>$$w_1:=w_1-\alpha\ dw_1$$</p><p>$$w_2:=w_2-\alpha\ dw_2$$</p><p>$$b:=b-\alpha\ db$$</p><p>这样经过n次迭代后，整个梯度下降算法就完成了。</p><p>值得一提的是，在上述的梯度下降算法中，我们是利用for循环对每个样本进行dw1，dw2和db的累加计算最后再求平均数的。在深度学习中，样本数量m通常很大，使用for循环会让神经网络程序运行得很慢。所以，我们应该尽量避免使用for循环操作，而使用矩阵运算，能够大大提高程序运行速度。关于vectorization的内容我们放在下次笔记中再说。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课的内容比较简单，主要介绍了神经网络的基础——逻辑回归。首先，我们介绍了二分类问题，以图片为例，将多维输入x转化为feature vector，输出y只有{0,1}两个离散值。接着，我们介绍了逻辑回归及其对应的Cost function形式。然后，我们介绍了梯度下降算法，并使用计算图的方式来讲述神经网络的正向传播和反向传播两个过程。最后，我们在逻辑回归中使用梯度下降算法，总结出最优化参数w和b的算法流程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170926082756271?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达神经网络与深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Coursera吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述</title>
    <link href="https://redstonewill.github.io/2018/03/19/34/"/>
    <id>https://redstonewill.github.io/2018/03/19/34/</id>
    <published>2018-03-19T13:32:32.000Z</published>
    <updated>2018-03-19T13:36:37.919Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170925093517265?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>吴恩达（Andrew Ng）相信大家都不陌生了。8 月 8 日，吴恩达在他自己创办的在线教育平台 Coursera 上线了他的人工智能专项课程（Deep Learning Specialization）。此课程广受好评，通过视频讲解、作业与测验等让更多的人对人工智能有了了解与启蒙，国外媒体报道称：吴恩达这次深度学习课程是迄今为止，最全面、系统和容易获取的深度学习课程，堪称普通人的人工智能第一课。</p><p>该专项课程的Coursera地址：<a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">https://www.coursera.org/specializations/deep-learning</a></p><p>另外，网易云课堂前段时间宣布跟吴恩达合作，拿到了独家版权，开设了深度学习微专业课，并且提供中文字幕翻译，降低了学习门槛。但是只有视频和课件材料，没有Coursera 上的作业、考试等环节，也不会提供证书，需要证书的还得去 Coursera 上学习。这里附上网易云课堂该专项课程的地址：<a href="http://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank" rel="noopener">http://mooc.study.163.com/smartSpec/detail/1001319001.htm</a></p><p>好了，在接下来的一段时间里，我将同步开始学习Coursera上深度学习专项课程，并将笔记以博客的形式记录下来。专项课程的第一门课是《神经网络与深度学习》。今天介绍第一讲：深度学习概述。</p><h3 id="What-is-a-neural-network"><a href="#What-is-a-neural-network" class="headerlink" title="What is a neural network?"></a>What is a neural network?</h3><p>简单来说，深度学习（Deep Learning）就是更复杂的神经网络（Neural Network）。那么，什么是神经网络呢？下面我们将通过一个简单的例子来引入神经网络模型的概念。</p><p>假如我们要建立房价的预测模型，一共有六个房子。我们已知输入x即每个房子的面积（多少尺或者多少平方米），还知道其对应的输出y即每个房子的价格。根据这些输入输出，我们要建立一个函数模型，来预测房价：y=f(x)。首先，我们将已知的六间房子的价格和面积的关系绘制在二维平面上，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170925093517265?" alt="这里写图片描述"></p><p>一般地，我们会一条直线来拟合图中这些离散点，即建立房价与面积的线性模型。但是从实际考虑，我们知道价格永远不会是负数。所以，我们对该直线做一点点修正，让它变成折线的形状，当面积小于某个值时，价格始终为零。如下图蓝色折线所示，就是我们建立的房价预测模型。</p><p><img src="http://img.blog.csdn.net/20170925093612288?" alt="这里写图片描述"></p><p>其实这个简单的模型（蓝色折线）就可以看成是一个神经网络，而且几乎是一个最简单的神经网络。我们把该房价预测用一个最简单的神经网络模型来表示，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170925093652899?" alt="这里写图片描述"></p><p>该神经网络的输入x是房屋面积，输出y是房屋价格，中间包含了一个神经元（neuron），即房价预测函数（蓝色折线）。该神经元的功能就是实现函数f(x)的功能。</p><p>值得一提的是，上图神经元的预测函数（蓝色折线）在神经网络应用中比较常见。我们把这个函数称为ReLU函数，即线性整流函数（Rectified Linear Unit），形如下图所示：</p><p><img src="http://img.blog.csdn.net/20170902133054327?" alt="这里写图片描述"></p><p>上面讲的只是由单个神经元（输入x仅仅是房屋面积一个因素）组成的神经网络，而通常一个大型的神经网络往往由许多神经元组成，就像通过乐高积木搭建复杂物体（例如火车）一样。</p><p>现在，我们把上面举的房价预测的例子变得复杂一些，而不是仅仅使用房屋面积一个判断因素。例如，除了考虑房屋面积（size）之外，我们还考虑卧室数目（#bedrooms）。这两点实际上与家庭成员的个数（family size）有关。还有，房屋的邮政编码（zip code/postal code），代表了该房屋位置的交通便利性，是否需要步行还是开车？即决定了可步行性（walkability）。另外，还有可能邮政编码和地区财富水平（wealth）共同影响了房屋所在地区的学校质量（school quality）。如下图所示，该神经网络共有三个神经元，分别代表了family size，walkability和school quality。每一个神经元都包含了一个ReLU函数（或者其它非线性函数）。那么，根据这个模型，我们可以根据房屋的面积和卧室个数来估计family size，根据邮政编码来估计walkability，根据邮政编码和财富水平来估计school quality。最后，由family size，walkability和school quality等这些人们比较关心的因素来预测最终的房屋价格。</p><p><img src="http://img.blog.csdn.net/20170925093733914?" alt="这里写图片描述"></p><p>所以，在这个例子中，x是size，#bedrooms，zip code/postal code和wealth这四个输入；y是房屋的预测价格。这个神经网络模型包含的神经元个数更多一些，相对之前的单个神经元的模型要更加复杂。那么，在建立一个表现良好的神经网络模型之后，在给定输入x时，就能得到比较好的输出y，即房屋的预测价格。</p><p>实际上，上面这个例子真正的神经网络模型结构如下所示。它有四个输入，分别是size，#bedrooms，zip code和wealth。在给定这四个输入后，神经网络所做的就是输出房屋的预测价格y。图中，三个神经元所在的位置称之为中间层或者隐藏层（x所在的称之为输入层，y所在的称之为输出层），每个神经元与所有的输入x都有关联（直线相连）。</p><p><img src="http://img.blog.csdn.net/20170925094655458?" alt="这里写图片描述"></p><p>这就是基本的神经网络模型结构。在训练的过程中，只要有足够的输入x和输出y，就能训练出较好的神经网络模型，该模型在此类房价预测问题中，能够得到比较准确的结果。</p><h3 id="Supervised-Learning-with-Neural-Networks"><a href="#Supervised-Learning-with-Neural-Networks" class="headerlink" title="Supervised Learning with Neural Networks"></a>Supervised Learning with Neural Networks</h3><p>目前为止，由神经网络模型创造的价值基本上都是基于监督式学习（Supervised Learning）的。监督式学习与非监督式学习本质区别就是是否已知训练样本的输出y。在实际应用中，机器学习解决的大部分问题都属于监督式学习，神经网络模型也大都属于监督式学习。下面我们来看几个监督式学习在神经网络中应用的例子。</p><p>首先，第一个例子还是房屋价格预测。根据训练样本的输入x和输出y，训练神经网络模型，预测房价。第二个例子是线上广告，这是深度学习最广泛、最赚钱的应用之一。其中，输入x是广告和用户个人信息，输出y是用户是否对广告进行点击。神经网络模型经过训练，能够根据广告类型和用户信息对用户的点击行为进行预测，从而向用户提供用户自己可能感兴趣的广告。第三个例子是电脑视觉（computer vision）。电脑视觉是近些年来越来越火的课题，而电脑视觉发展迅速的原因很大程度上是得益于深度学习。其中，输入x是图片像素值，输出是图片所属的不同类别。第四个例子是语音识别（speech recognition）。深度学习可以将一段语音信号辨识为相应的文字信息。第五个例子是智能翻译，例如通过神经网络输入英文，然后直接输出中文。除此之外，第六个例子是自动驾驶。通过输入一张图片或者汽车雷达信息，神经网络通过训练来告诉你相应的路况信息并作出相应的决策。至此，神经网络配合监督式学习，其应用是非常广泛的。</p><p><img src="http://img.blog.csdn.net/20170925094723928?" alt="这里写图片描述"></p><p>我们应该知道，根据不同的问题和应用场合，应该使用不同类型的神经网络模型。例如上面介绍的几个例子中，对于一般的监督式学习（房价预测和线上广告问题），我们只要使用标准的神经网络模型就可以了。而对于图像识别处理问题，我们则要使用卷积神经网络（Convolution Neural Network），即CNN。而对于处理类似语音这样的序列信号时，则要使用循环神经网络（Recurrent Neural Network），即RNN。还有其它的例如自动驾驶这样的复杂问题则需要更加复杂的混合神经网络模型。</p><p>CNN和RNN是比较常用的神经网络模型。下图给出了Standard NN，Convolutional NN和Recurrent NN的神经网络结构图。</p><p><img src="http://img.blog.csdn.net/20170925095229513?" alt="这里写图片描述"></p><p>CNN一般处理图像问题，RNN一般处理语音信号。他们的结构是什么意思？如何实现CNN和RNN的结构？这些问题我们将在以后的课程中来深入分析并解决。</p><p>另外，数据类型一般分为两种：Structured Data和Unstructured Data。</p><p><img src="http://img.blog.csdn.net/20170925095423087?" alt="这里写图片描述"></p><p>简单地说，Structured Data通常指的是有实际意义的数据。例如房价预测中的size，#bedrooms，price等；例如在线广告中的User Age，Ad ID等。这些数据都具有实际的物理意义，比较容易理解。而Unstructured Data通常指的是比较抽象的数据，例如Audio，Image或者Text。以前，计算机对于Unstructured Data比较难以处理，而人类对Unstructured Data却能够处理的比较好，例如我们第一眼很容易就识别出一张图片里是否有猫，但对于计算机来说并不那么简单。现在，值得庆幸的是，由于深度学习和神经网络的发展，计算机在处理Unstructured Data方面效果越来越好，甚至在某些方面优于人类。总的来说，神经网络与深度学习无论对Structured Data还是Unstructured Data都能处理得越来越好，并逐渐创造出巨大的实用价值。我们在之后的学习和实际应用中也将会碰到许多Structured Data和Unstructured Data。</p><h3 id="Why-is-Deep-Learning-taking-off？"><a href="#Why-is-Deep-Learning-taking-off？" class="headerlink" title="Why is Deep Learning taking off？"></a>Why is Deep Learning taking off？</h3><p>如果说深度学习和神经网络背后的技术思想已经出现数十年了，那么为什么直到现在才开始发挥作用呢？接下来，我们来看一下深度学习背后的主要动力是什么，方便我们更好地理解并使用深度学习来解决更多问题。</p><p>深度学习为什么这么强大？下面我们用一张图来说明。如下图所示，横坐标x表示数据量（Amount of data），纵坐标y表示机器学习模型的性能表现（Performance）。</p><p><img src="http://img.blog.csdn.net/20170925095445485?" alt="这里写图片描述"></p><p>上图共有4条曲线。其中，最底下的那条红色曲线代表了传统机器学习算法的表现，例如是SVM，logistic regression，decision tree等。当数据量比较小的时候，传统学习模型的表现是比较好的。但是当数据量很大的时候，其表现很一般，性能基本趋于水平。红色曲线上面的那条黄色曲线代表了规模较小的神经网络模型（Small NN）。它在数据量较大时候的性能优于传统的机器学习算法。黄色曲线上面的蓝色曲线代表了规模中等的神经网络模型（Media NN），它在在数据量更大的时候的表现比Small NN更好。最上面的那条绿色曲线代表更大规模的神经网络（Large NN），即深度学习模型。从图中可以看到，在数据量很大的时候，它的表现仍然是最好的，而且基本上保持了较快上升的趋势。值得一提的是，近些年来，由于数字计算机的普及，人类进入了大数据时代，每时每分，互联网上的数据是海量的、庞大的。如何对大数据建立稳健准确的学习模型变得尤为重要。传统机器学习算法在数据量较大的时候，性能一般，很难再有提升。然而，深度学习模型由于网络复杂，对大数据的处理和分析非常有效。所以，近些年来，在处理海量数据和建立复杂准确的学习模型方面，深度学习有着非常不错的表现。然而，在数据量不大的时候，例如上图中左边区域，深度学习模型不一定优于传统机器学习算法，性能差异可能并不大。</p><p>所以说，现在深度学习如此强大的原因归结为三个因素：</p><ul><li><p><strong>Data</strong></p></li><li><p><strong>Computation</strong></p></li><li><p><strong>Algorithms</strong></p></li></ul><p>其中，数据量的几何级数增加，加上GPU出现、计算机运算能力的大大提升，使得深度学习能够应用得更加广泛。另外，算法上的创新和改进让深度学习的性能和速度也大大提升。举个算法改进的例子，之前神经网络神经元的激活函数是Sigmoid函数，后来改成了ReLU函数。之所以这样更改的原因是对于Sigmoid函数，在远离零点的位置，函数曲线非常平缓，其梯度趋于0，所以造成神经网络模型学习速度变得很慢。然而，ReLU函数在x大于零的区域，其梯度始终为1，尽管在x小于零的区域梯度为0，但是在实际应用中采用ReLU函数确实要比Sigmoid函数快很多。</p><p>构建一个深度学习的流程是首先产生Idea，然后将Idea转化为Code，最后进行Experiment。接着根据结果修改Idea，继续这种Idea-&gt;Code-&gt;Experiment的循环，直到最终训练得到表现不错的深度学习网络模型。如果计算速度越快，每一步骤耗时越少，那么上述循环越能高效进行。</p><h3 id="About-this-Course"><a href="#About-this-Course" class="headerlink" title="About this Course"></a>About this Course</h3><p>这里简单列一下本系列深度学习专项课程有哪些：</p><ul><li><p><strong>Neural Networks and Deep Learning</strong></p></li><li><p><strong>Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</strong></p></li><li><p><strong>Structuring your Machine Learning project</strong></p></li><li><p><strong>Convolutional Neural Networks</strong></p></li><li><p><strong>Natural Language Processing: Building sequence models</strong></p></li></ul><p>目前我们正在学习的是第一门课《Neural Networks and Deep Learning》。Coursera上关于这门课的教学日程安排如下：</p><ul><li><p><strong>Week 1: Introduction</strong></p></li><li><p><strong>Week 2: Basics of Neural Network programming</strong></p></li><li><p><strong>Week 3: One hidden layer Neural Networks</strong></p></li><li><p><strong>Week 4: Deep Neural Networks</strong></p></li></ul><p>这门课我打算用5次笔记进行总结。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课的内容比较简单，主要对深度学习进行了简要概述。首先，我们使用房价预测的例子来建立最简单的但个神经元组成的神经网络模型。然后，我们将例子复杂化，建立标准的神经网络模型结构。接着，我们从监督式学习入手，介绍了不同的神经网络类型，包括Standard NN，CNN和RNN。不同的神经网络模型适合处理不同类型的问题。对数据集本身来说，分为Structured Data和Unstructured Data。近些年来，深度学习对Unstructured Data的处理能力大大提高，例如图像处理、语音识别和语言翻译等。最后，我们用一张对比图片解释了深度学习现在飞速发展、功能强大的原因。归纳其原因包含三点：Data，Computation和Algorithms。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170925093517265?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="吴恩达神经网络与深度学习" scheme="https://redstonewill.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="深度学习" scheme="https://redstonewill.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://redstonewill.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Coursera" scheme="https://redstonewill.github.io/tags/Coursera/"/>
    
      <category term="吴恩达" scheme="https://redstonewill.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记16（完结） -- Finale</title>
    <link href="https://redstonewill.github.io/2018/03/18/33/"/>
    <id>https://redstonewill.github.io/2018/03/18/33/</id>
    <published>2018-03-18T06:42:22.000Z</published>
    <updated>2018-03-18T06:44:06.907Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170820202309144?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Matrix Factorization。通过电影推荐系统的例子，介绍Matrix Factorization其实是一个提取用户特征，关于电影的线性模型。反过来也可以看出是关于用户的线性模型。然后，我们使用SGD对模型进行最佳化。本节课我们将对机器学习技法课程介绍过的所有内容做个总结，分成三个部分：Feature Exploitation Techniques，Error Optimization Techniques和Overfitting Elimination Techniques。</p><h3 id="Feature-Exploitation-Techniques"><a href="#Feature-Exploitation-Techniques" class="headerlink" title="Feature Exploitation Techniques"></a>Feature Exploitation Techniques</h3><p>我们在本系列课程中介绍的第一个特征提取的方法就是kernel。Kernel运算将特征转换和计算内积这两个步骤合二为一，提高了计算效率。我们介绍过的kernel有：Polynormial Kernel、Gaussian Kernel、Stump Kernel等。另外，我们可以将不同的kernels相加（transform union）或者相乘（transform combination），得到不同的kernels的结合形式，让模型更加复杂。值得一提的是，要成为kernel，必须满足Mercer Condition。不同的kernel可以搭配不同的kernel模型，比如：SVM、SVR和probabilistic SVM等，还包括一些不太常用的模型：kernel ridge regression、kernel logistic regression。使用这些kernel模型就可以将线性模型扩展到非线性模型，kernel就是实现一种特征转换，从而能够处理非常复杂的非线性模型。顺便提一下，因为PCA、k-Means等算法都包含了内积运算，所以它们都对应有相应的kernel版本。</p><p><img src="http://img.blog.csdn.net/20170820202309144?" alt="这里写图片描述"></p><p>Kernel是我们利用特征转换的第一种方法，那利用特征转换的第二种方法就是Aggregation。我们之前介绍的所有的hypothesis都可以看成是一种特征转换，然后再由这些g组合成G。我们介绍过的分类模型（hypothesis）包括：Decision Stump、Decision Tree和Gaussian RBF等。如果所有的g是已知的，就可以进行blending，例如Uniform、Non-Uniform和Conditional等方式进行aggregation。如果所有的g是未知的，可以使用例如Bagging、AdaBoost和Decision Tree的方法来建立模型。除此之外，还有probabilistic SVM模型。值得一提的是，机器学习中很多模型都是类似的，我们在设计一个机器学习模型时，应该融会贯通。</p><p><img src="http://img.blog.csdn.net/20170820205533778?" alt="这里写图片描述"></p><p>除此之外，我们还介绍了利用提取的方式，找出潜藏的特征（Hidden Features）。一般通过unsupervised learning的方法，从原始数据中提取出隐藏特征，使用权重表征。相应的模型包括：Neural Network、RBF Network、Matrix Factorization等。这些模型使用的unsupervised learning方法包括：AdaBoost、k-Means和Autoencoder、PCA等。</p><p><img src="http://img.blog.csdn.net/20170820211733400?" alt="这里写图片描述"></p><p>另外，还有一种非常有用的特征转换方法是维度压缩，即将高维度的数据降低（投影）到低维度的数据。我们介绍过的维度压缩模型包括：Decision Stump、Random Forest Tree Branching、Autoencoder、PCA和Matrix Factorization等。这些从高纬度到低纬度的特征转换在实际应用中作用很大。</p><p><img src="http://img.blog.csdn.net/20170820212320303?" alt="这里写图片描述"></p><h3 id="Error-Optimization-Techniques"><a href="#Error-Optimization-Techniques" class="headerlink" title="Error Optimization Techniques"></a>Error Optimization Techniques</h3><p>接下来我们将总结一下本系列课程中介绍过哪些优化技巧。首先，第一个数值优化技巧就是梯度下降（Gradient Descent），即让变量沿着其梯度反方向变化，不断接近最优解。例如我们介绍过的SGD、Steepest Descent和Functional GD都是利用了梯度下降的技巧。</p><p><img src="http://img.blog.csdn.net/20170821075109200?" alt="这里写图片描述"></p><p>而对于一些更复杂的最佳化问题，无法直接利用梯度下降方法来做，往往需要一些数学上的推导来得到最优解。最典型的例子是Dual SVM，还包括Kernel LogReg、Kernel RidgeReg和PCA等等。这些模型本身包含了很多数学上的一些知识，例如线性代数等等。除此之外，还有一些boosting和kernel模型，虽然本课程中没有提到，但是都会用到类似的数学推导和转换技巧。</p><p><img src="http://img.blog.csdn.net/20170821080108333?" alt="这里写图片描述"></p><p>如果原始问题比较复杂，求解比较困难，我们可以将原始问题拆分为子问题以简化计算。也就是将问题划分为多个步骤进行求解，即Multi-Stage。例如probabilistic SVM、linear blending、RBF Network等。还可以使用交叉迭代优化的方法，即Alternating Optim。例如k-Means、alternating LeastSqr等。除此之外，还可以采样分而治之的方法，即Divide &amp; Conquer。例如decision tree。</p><p><img src="http://img.blog.csdn.net/20170821081019760?" alt="这里写图片描述"></p><h3 id="Overfitting-Elimination-Techniques"><a href="#Overfitting-Elimination-Techniques" class="headerlink" title="Overfitting Elimination Techniques"></a>Overfitting Elimination Techniques</h3><p>Feature Exploitation Techniques和Error Optimization Techniques都是为了优化复杂模型，减小$E_{in}$。但是$E_{in}$太小有很可能会造成过拟合overfitting。因此，机器学习中，Overfitting Elimination尤为重要。</p><p>首先，可以使用Regularization来避免过拟合现象发生。我们介绍过的方法包括：large-margin、L2、voting/averaging等等。</p><p><img src="http://img.blog.csdn.net/20170821082118629?" alt="这里写图片描述"></p><p>除了Regularization之外，还可以使用Validation来消除Overfitting。我们介绍过的Validation包括：SV、OOB和Internal Validation等。</p><p><img src="http://img.blog.csdn.net/20170821082504296?" alt="这里写图片描述"></p><h3 id="Machine-Learning-in-Action"><a href="#Machine-Learning-in-Action" class="headerlink" title="Machine Learning in Action"></a>Machine Learning in Action</h3><p>本小节介绍了林轩田老师所在的台大团队在近几年的KDDCup国际竞赛上的表现和使用的各种机器算法。融合了我们在本系列课程中所介绍的很多机器学习技法和模型。这里不再一一赘述，将相应的图片贴出来，读者自己看看吧。</p><p><img src="http://img.blog.csdn.net/20170821082919157?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170821083004329?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170821083042817?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170821083128269?" alt="这里写图片描述"></p><p>ICDM在2006年的时候发布了排名前十的数据挖掘算法，如下图所示。其中大部分的算法我们在本系列的课程中都有过介绍。值得一提的是Naive Bayes算法本课程中没有涉及，贝叶斯模型在实际中应用还是挺广泛的，后续可能还需要深入学习一下。</p><p><img src="http://img.blog.csdn.net/20170821083957182?" alt="这里写图片描述"></p><p>最后，我们将所有介绍过的机器学习算法和模型列举出来：</p><p><img src="http://img.blog.csdn.net/20170821084154106?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要从三个方面来对机器学习技法课程做个总结：Feature Exploitation Techniques，Error Optimization Techniques和Overfitting Elimination Techniques。最后介绍了林轩田老师带领的台大团队是如何在历届KDDCup中将很多机器学习算法模型融合起来，并获得了良好的成绩。</p><p><img src="http://img.blog.csdn.net/20170821084916401?" alt="这里写图片描述"></p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p><p>###<strong>写在最后的话</strong></p><p>历时近4个月，终于将台湾大学林轩田老师的《机器学习基石》和《机器学习技法》这两门课程学完了。突然的想法，开始写博客记录下学习历程，通过笔记的形式加深巩固了自己的理解。如果能对读者有些许帮助的话，那便是一大快事。笔者资历尚浅，博客中难免有疏漏和错误，欢迎各位批评指正。另外，鄙人不才，建立了一个QQ群，以便讨论与该课程相关或者其它的机器学习和深度学习问题。有兴趣的朋友可以加一下，QQ群号码是223490966（红色石头机器学习小站）。后续，笔者根据学习情况，可能还会推出一些课程笔记的博客。</p><p>积跬步以致千里，积小流以成江海！</p><p>最后，特别感谢林轩田老师！您的教学风格我很喜欢，深入浅出、寓教于乐。非常有幸能够学到您的课程！再次感谢！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170820202309144?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记15 -- Matrix Factorization</title>
    <link href="https://redstonewill.github.io/2018/03/18/32/"/>
    <id>https://redstonewill.github.io/2018/03/18/32/</id>
    <published>2018-03-18T06:40:21.000Z</published>
    <updated>2018-03-18T06:41:49.172Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170817082634527?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Radial Basis Function Network。它的原理就是基于距离相似性（distance-based similarities）的线性组合（linear aggregation）。我们使用k-Means clustering算法找出具有代表性的k个中心点，然后再计算与这些中心点的distance similarity，最后应用到RBF Network中去。</p><h3 id="LinearNetwork-Hypothesis"><a href="#LinearNetwork-Hypothesis" class="headerlink" title="LinearNetwork Hypothesis"></a>LinearNetwork Hypothesis</h3><p>回顾一下，我们在机器学习基石课程的第一节课就提到过，机器学习的目的就是让机器从数据data中学习到某种能力skill。我们之前举过一个典型的推荐系统的例子。就是说，假如我们手上有许多不同用户对不同电影的排名rank，通过机器学习，训练一个模型，能够对用户没有看过的某部电影进行排名预测。</p><p><img src="http://img.blog.csdn.net/20170817082634527?" alt="这里写图片描述"></p><p>一个典型的电影推荐系统的例子是2006年Netflix举办的一次比赛。数据包含了480189个用户和17770部电影，总共1亿多个排名信息。该推荐系统模型中，我们用$\breve x_n=(n)$表示第n个用户，这是一个抽象的特征，常常使用数字编号来代替具体哪个用户。输出方面，我们使用$y_m=r_{nm}$表示第n个用户对第m部电影的排名数值。</p><p><img src="http://img.blog.csdn.net/20170817083608703?" alt="这里写图片描述"></p><p>下面我们来进一步看看这些抽象的特征，$\breve x_n=(n)$是用户的ID，通常用数字表示。例如1126,5566,6211等。这些编号并没有数值大小上的意义，只是一种ID标识而已。这类特征被称为类别特征（categorical features）。常见的categorical features包括：IDs，blood type，programming languages等等。而许多机器学习模型中使用的大部分都是数值特征（numerical features）。例如linear models，NNet模型等。但决策树（decision tree）是个例外，它可以使用categorical features。所以说，如果要建立一个类似推荐系统的机器学习模型，就要把用户ID这种categorical features转换为numerical features。这种特征转换其实就是训练模型之前一个编码（encoding）的过程。</p><p><img src="http://img.blog.csdn.net/20170817085426956?" alt="这里写图片描述"></p><p>一种最简单的encoding方式就是binary vector encoding。也就是说，如果输入样本有N个，就构造一个维度为N的向量。第n个样本对应向量上第n个元素为1，其它元素都是0。下图就是一个binary vector encoding的例子。</p><p><img src="http://img.blog.csdn.net/20170817090157014?" alt="这里写图片描述"></p><p>经过encoding之后，输入$x_n$是N维的binary vector，表示第n个用户。输出$y_n$是M维的向量，表示该用户对M部电影的排名数值大小。注意，用户不一定对所有M部电影都作过评价，未评价的恰恰是我们要预测的（下图中问号？表示未评价的电影）。</p><p><img src="http://img.blog.csdn.net/20170817091836052?" alt="这里写图片描述"></p><p>总共有N个用户，M部电影。对于这样的数据，我们需要掌握每个用户对不同电影的喜爱程度及排名。这其实就是一个特征提取（feature extraction）的过程，提取出每个用户喜爱的电影风格及每部电影属于哪种风格，从而建立这样的推荐系统模型。可供选择使用的方法和模型很多，这里，我们使用的是NNet模型。NNet模型中的网络结构是$N-\breve d-M$型，其中N是输入层样本个数，$\breve d$是隐藏层神经元个数，M是输出层电影个数。该NNet为了简化计算，忽略了常数项。当然可以选择加上常数项，得到较复杂一些的模型。顺便提一下，这个结构跟我们之前介绍的autoencoder非常类似，都是只有一个隐藏层。</p><p><img src="http://img.blog.csdn.net/20170817104012267?" alt="这里写图片描述"></p><p>说到这里，有一个问题，就是上图NNet中隐藏层的tanh函数是否一定需要呢？答案是不需要。因为输入向量x是经过encoding得到的，其中大部分元素为0，只有一个元素为1。那么，只有一个元素$x_n$与相应权重的乘积进入到隐藏层。由于$x_n=1$，则相当于只有一个权重值进入到tanh函数进行运算。从效果上来说，tanh(x)与x是无差别的，只是单纯经过一个函数的计算，并不影响最终的结果，修改权重值即可得到同样的效果。因此，我们把隐藏层的tanh函数替换成一个线性函数y=x，得到下图所示的结构。</p><p><img src="http://img.blog.csdn.net/20170817105908092?" alt="这里写图片描述"></p><p>由于中间隐藏层的转换函数是线性的，我们把这种结构称为Linear Network（与linear autoencoder比较相似）。看一下上图这个网络结构，输入层到隐藏层的权重$W_{ni}^{(1)}$维度是Nx$\breve d$，用向量$V^T$表示。隐藏层到输出层的权重$W_{im}^{(2)}$维度是$\breve d$xM，用矩阵W表示。把权重由矩阵表示之后，Linear Network的hypothesis 可表示为：</p><p>$$h(x)=W^TVx$$</p><p>如果是单个用户$x_n$，由于X向量中只有元素$x_n$为1，其它均为0，则对应矩阵V只有第n列向量是有效的，其输出hypothesis为：</p><p>$$h(x_n)=W^Tv_n$$</p><p><img src="http://img.blog.csdn.net/20170817111901831?" alt="这里写图片描述"></p><h3 id="Basic-Matrix-Factorization"><a href="#Basic-Matrix-Factorization" class="headerlink" title="Basic Matrix Factorization"></a>Basic Matrix Factorization</h3><p>刚刚我们已经介绍了linear network的模型和hypothesis。其中Vx可以看作是对用户x的一种特征转换$\Phi(x)$。对于单部电影，其预测的排名可表示为：</p><p>$$h_m(x)=w_m^T\Phi(x)$$</p><p><img src="http://img.blog.csdn.net/20170817134258375?" alt="这里写图片描述"></p><p>推导完linear network模型之后，对于每组样本数据（即第n个用户第m部电影），我们希望预测的排名$w_m^Tv_n$与实际样本排名$y_n$尽可能接近。所有样本综合起来，我们使用squared error measure的方式来定义$E_{in}$，$E_{in}$的表达式如下所示：</p><p><img src="http://img.blog.csdn.net/20170817134856500?" alt="这里写图片描述"></p><p>上式中，灰色的部分是常数，并不影响最小化求解，所以可以忽略。接下来，我们就要求出$E_{in}$最小化时对应的V和W解。</p><p>我们的目标是让真实排名与预测排名尽可能一致，即$r_{nm}\approx w_m^Tv_n=v_n^Tw_m$。把这种近似关系写成矩阵的形式：$R\approx V^TW$。矩阵R表示所有不同用户不同电影的排名情况，维度是NxM。这种用矩阵的方式进行处理的方法叫做Matrix Factorization。</p><p><img src="http://img.blog.csdn.net/20170817140653558?" alt="这里写图片描述"></p><p>上面的表格说明了我们希望将实际排名情况R分解成两个矩阵（V和W）的乘积形式。V的维度是$\breve d$xN的，N是用户个数，$\breve d$可以是影片类型，例如（喜剧片，爱情片，悬疑片，动作片，…）。根据用户喜欢的类型不同，赋予不同的权重。W的维度是$\breve d$xM，M是电影数目，$\breve d$同样是影片类型，该部电影属于哪一类型就在那个类型上占比较大的权重。当然，$\breve d$维特征不一定就是影片类型，还可以是其它特征，例如明显阵容、年代等等。</p><p><img src="http://img.blog.csdn.net/20170817145203692?" alt="这里写图片描述"></p><p>那么，Matrix Factorization的目标就是最小化$E_{in}$函数。$E_{in}$表达式如下所示：</p><p><img src="http://img.blog.csdn.net/20170817145558938?" alt="这里写图片描述"></p><p>$E_{in}$中包含了两组待优化的参数，分别是$v_n$和$w_m$。我们可以借鉴上节课中k-Means的做法，将其中第一个参数固定，优化第二个参数，然后再固定第二个参数，优化第一个参数，一步一步进行优化。</p><p>当$v_n$固定的时候，只需要对每部电影做linear regression即可，优化得到每部电影的$\breve d$维特征值$w_m$。</p><p>当$w_m$固定的时候，因为V和W结构上是对称的，同样只需要对每个用户做linear regression即可，优化得到每个用户对$\breve d$维电影特征的喜爱程度$v_n$。</p><p><img src="http://img.blog.csdn.net/20170817151128423?" alt="这里写图片描述"></p><p>这种算法叫做alternating least squares algorithm。它的处理思想与k-Means算法相同，其算法流程图如下所示：</p><p><img src="http://img.blog.csdn.net/20170817151443027?" alt="这里写图片描述"></p><p>alternating least squares algorithm有两点需要注意。第一是initialize问题，通常会随机选取$v_n$和$w_m$。第二是converge问题，由于每次迭代更新都能减小$E_{in}$，$E_{in}$会趋向于0，则保证了算法的收敛性。</p><p><img src="http://img.blog.csdn.net/20170817151830032?" alt="这里写图片描述"></p><p>在上面的分析中，我们提过Matrix Factorization与Linear Autoencoder的相似性，下图列出了二者之间的比较。</p><p><img src="http://img.blog.csdn.net/20170817152251324?" alt="这里写图片描述"></p><p>Matrix Factorization与Linear Autoencoder有很强的相似性，都可以从原始资料汇总提取有用的特征。其实，linear autoencoder可以看成是matrix factorization的一种特殊形式。</p><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>我们刚刚介绍了alternating least squares algorithm来解决Matrix Factorization的问题。这部分我们将讨论使用Stochastic Gradient Descent方法来进行求解。之前的alternating least squares algorithm中，我们考虑了所有用户、所有电影。现在使用SGD，随机选取一笔资料，然后只在与这笔资料有关的error function上使用梯度下降算法。使用SGD的好处是每次迭代只要处理一笔资料，效率很高；而且程序简单，容易实现；最后，很容易扩展到其它的error function来实现。</p><p><img src="http://img.blog.csdn.net/20170817163304014?" alt="这里写图片描述"></p><p>对于每笔资料，它的error function可表示为：</p><p><img src="http://img.blog.csdn.net/20170817163646644?" alt="这里写图片描述"></p><p>上式中的err是squared error function，仅与第n个用户$v_n$，第m部电影$w_m$有关。其对$v_n$和$w_m$的偏微分结果为：</p><p>$$\nabla v_n=-2(r_{nm}-w_m^Tv_n)w_m$$</p><p>$$\nabla w_m=-2(r_{nm}-w_m^Tv_n)v_n$$</p><p><img src="http://img.blog.csdn.net/20170817164230994?" alt="这里写图片描述"></p><p>很明显，$\nabla v_n$和$\nabla w_m$都由两项乘积构成。（忽略常数因子2）。第一项都是$r_{nm}-w_m^Tv_n$，即余数residual。我们在之前介绍的GBDT算法中也介绍过余数这个概念。$\nabla v_n$的第二项是$w_m$，而$\nabla w_m$的第二项是$v_n$。二者在结构上是对称的。</p><p>计算完任意一个样本点的SGD后，就可以构建Matrix Factorization的算法流程。SGD for Matrix Factorization的算法流程如下所示：</p><p><img src="http://img.blog.csdn.net/20170817170815240?" alt="这里写图片描述"></p><p>在实际应用中，由于SGD算法简单高效，Matrix Factorization大多采用这种算法。</p><p>介绍完SGD for Matrix Factorization之后，我们来看一个实际的应用例子。问题大致是这样的：根据现在有的样本资料，预测未来的趋势和结果。显然，这是一个与时间先后有关的预测模型。比如说一个用户三年前喜欢的电影可能现在就不喜欢了。所以在使用SGD选取样本点的时候有一个技巧，就是最后T次迭代，尽量选择时间上靠后的样本放入到SGD算法中。这样最后的模型受这些时间上靠后的样本点影响比较大，也相对来说比较准确，对未来的预测会比较准。</p><p><img src="http://img.blog.csdn.net/20170817173549256?" alt="这里写图片描述"></p><p>所以，在实际应用中，我们除了使用常规的机器学习算法外，还需要根据样本数据和问题的实际情况来修改我们的算法，让模型更加切合实际，更加准确。我们要学会灵活运用各种机器学习算法，而不能只是照搬。</p><h3 id="Summary-of-Extraction-Models"><a href="#Summary-of-Extraction-Models" class="headerlink" title="Summary of Extraction Models"></a>Summary of Extraction Models</h3><p>从第12节课开始到现在，我们总共用了四节课的时间来介绍Extraction Models。虽然我们没有给出Extraction Models明确的定义，但是它主要的功能就是特征提取和特征转换，将原始数据更好地用隐藏层的一些节点表征出来，最后使用线性模型将所有节点aggregation。这种方法使我们能够更清晰地抓住数据的本质，从而建立最佳的机器学习模型。</p><p>下图所示的就是我们介绍过的所有Extraction Models，除了这四节课讲的内容之外，还包括之前介绍的Adaptive/Gradient Boosting模型。因为之前笔记中都详细介绍过，这里就不再一一总结了。</p><p><img src="http://img.blog.csdn.net/20170817212312885?" alt="这里写图片描述"></p><p>除了各种Extraction Models之外，我们这四节课还介绍了不同的Extraction Techniques。下图所示的是对应于不同的Extraction Models的Extraction Techniques。</p><p><img src="http://img.blog.csdn.net/20170817212840069?" alt="这里写图片描述"></p><p>最后，总结一下这些Extraction Models有什么样的优点和缺点。从优点上来说：</p><ul><li><p><strong>easy：机器自己完成特征提取，减少人类工作量</strong></p></li><li><p><strong>powerful：能够处理非常复杂的问题和特征提取</strong></p></li></ul><p>另一方面，从缺点上来说：</p><ul><li><p><strong>hard：通常遇到non-convex的优化问题，求解较困难，容易得到局部最优解而非全局最优解</strong></p></li><li><p><strong>overfitting：模型复杂，容易造成过拟合，需要进行正则化处理</strong></p></li></ul><p>所以说，Extraction Models是一个非常强大的机器学习工具，但是使用的时候也要小心处理各种可能存在的问题。</p><p><img src="http://img.blog.csdn.net/20170817213747309?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Matrix Factorization。从电影推荐系统模型出发，首先，我们介绍了Linear Network。它从用户ID编码后的向量中提取出有用的特征，这是典型的feature extraction。然后，我们介绍了基本的Matrix Factorization算法，即alternating least squares，不断地在用户和电影之间交互地做linear regression进行优化。为了简化计算，提高运算速度，也可以使用SGD来实现。事实证明，SGD更加高效和简单。同时，我们可以根据具体的问题和需求，对固有算法进行一些简单的调整，来获得更好的效果。最后，我们对已经介绍的所有Extraction Models做个简单的总结。Extraction Models在实际应用中是个非常强大的工具，但是也要避免出现过拟合等问题。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170817082634527?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记14 -- Radial Basis Function Network</title>
    <link href="https://redstonewill.github.io/2018/03/18/31/"/>
    <id>https://redstonewill.github.io/2018/03/18/31/</id>
    <published>2018-03-18T06:38:01.000Z</published>
    <updated>2018-03-18T06:39:30.623Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170814084714413?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Deep Learning的概念。Deep Learing其实是Neural Networ的延伸，神经元更多，网络结构更加复杂。深度学习网络在训练的过程中最核心的问题就是pre-training和regularization。pre-training中，我们使用denoising autoencoder来对初始化权重进行选择。denoising autoencoder与统计学中经常用来进行数据处理的PCA算法具有很大的关联性。这节课我们将介绍Radial Basis Function Network，把之前介绍的adial Basis Function和Neural Network联系起来。</p><h3 id="RBF-Network-Hypothesis"><a href="#RBF-Network-Hypothesis" class="headerlink" title="RBF Network Hypothesis"></a>RBF Network Hypothesis</h3><p>之前我们介绍过，在SVM中引入Gaussian Kernel就能在无限多维的特征转换中得到一条“粗壮”的分界线（或者高维分界平面、分界超平面）。从结果来看，Gaussian SVM其实就是将一些Gaussian函数进行线性组合，而Gaussian函数的中心就位于Support Vectors上，最终得到预测模型$g_{svm}(x)$。</p><p><img src="http://img.blog.csdn.net/20170814084714413?" alt="这里写图片描述"></p><p>Gaussian kernel的另一种叫法是Radial Basis Function(RBF) kernel，即径向基函数。这个名字从何而来？首先，radial表示Gaussian函数计算结果只跟新的点x与中心点$x_n$的距离有关，与其它无关。basis function就是指Gaussian函数，最终的矩$g_{svm}(x)$就是由这些basis function线性组合而成。</p><p>从另外一个角度来看Gaussian SVM。首先，构造一个函数$g_n(x)$：</p><p>$$g_n(x)=y_ne^{-\gamma||x-x_n||^2}$$</p><p>上式中，指数项表示新的点x与$x_n$之间的距离大小。距离越近，即权重越大，相当于对$y_n$投的票数更多；而距离越远，权重越小，相当于对$y_n$投的票数更少。其物理意义是新的点与$x_n$的距离远近决定了$g_n(x)$与$y_n$的接近程度。如果距离越近，则$y_n$对$g_n(x)$的权重影响越大；如果距离越远，则$y_n$对$g_n(x)$的权重影响越小。那么整体来说，$g_{svm}(x)$就由所有的SV组成的$g_n(x)$线性组合而成，不同$g_n(x)$对应的系数是$\alpha_n$，最后由sign函数做最后的选择。这个过程很类型我们之前介绍的aggregation中将所有较好的hypothesis线性组合，不同的$g_n(x)$有不同的权重$\alpha_n$。我们把$g_n(x)$叫做radial hypotheses，Gaussian SVM就是将所有SV对应的radial hypotheses进行线性组合（linear aggregation）。</p><p><img src="http://img.blog.csdn.net/20170814092622529?" alt="这里写图片描述"></p><p>那么，Radial Basis Function(RBF) Network其实就是上面Gaussian SVM概念的延伸，目的就是找到所有radial hypotheses的linear aggregation，得到更好的网络模型。</p><p>之所以叫作RBF Network是因为它的模型结构类似于我们之前介绍的Neural Network。</p><p><img src="http://img.blog.csdn.net/20170814133904920?" alt="这里写图片描述"></p><p>Neural Network与RBF Network在输出层基本是类似的，都是上一层hypotheses的线性组合（linear aggregation）。但是对于隐藏层的各个神经元来说，Neural Network是使用内积（inner-product）加上tanh()函数的方法，而RBF Network是使用距离（distance）加上Gaussian函数的方法。总的来说，RBF Network是Neural Network的一个分支。</p><p><img src="http://img.blog.csdn.net/20170814135229130?" alt="这里写图片描述"></p><p>至此，RBF Network Hypothesis以及网络结构可以写成如下形式：</p><p><img src="http://img.blog.csdn.net/20170814135931865?" alt="这里写图片描述"></p><p>上式中，$\mu_m$表示每个中心点的位置，隐藏层每个神经元对应一个中心点；$\beta_m$表示每个RBF的权重，即投票所占比重。 </p><p>对应到Gaussian SVM上，上式中的RBF就是Gaussian函数。由于是分类问题，上式中的Output就是sign函数。其中，RBF的个数M就等于支持向量的个数SV，$\mu_m$就代表每个SV的坐标$x_m$，而$\beta_m$就是在Dual SVM中推导得到的$\alpha_ny_m$值。那我们学习的目标就是根据已知的RBF和Output，来决定最好的中心点位置$\mu_m$和权重系数$\beta_m$。</p><p><img src="http://img.blog.csdn.net/20170814141742989?" alt="这里写图片描述"></p><p>在之前介绍SVM的时候，我们就讲过Mercer定理：一个矩阵是Kernel的充分必要条件是它是对称的且是半正定的，条件比较苛刻。除了Gaussian kernel还有Polynomial kernel等等。Kernel实际上描述了两个向量之间的相似性，通过转换到z空间计算内积的方式，来表征二者之间的相似性。而RBF实际上是直接使用x空间的距离来描述了一种相似性，距离越近，相似性越高。因此，kernel和RBF可以看成是两种衡量相似性（similarity）的方式。本文介绍的Gaussian RBF即为二者的交集。</p><p><img src="http://img.blog.csdn.net/20170814145418972?" alt="这里写图片描述"></p><p>除了kernel和RBF之外，还有其它衡量相似性的函数。例如神经网络中的神经元就是衡量输入和权重之间的相似性。</p><p>经过以上分析，我们知道了RBF Network中distance similarity是一个很好的定义特征转换的方法。除此之外，我们还可以使用其它相似性函数来表征特征转换，从而得到更好的机器学习模型。</p><h3 id="RBF-Network-Learning"><a href="#RBF-Network-Learning" class="headerlink" title="RBF Network Learning"></a>RBF Network Learning</h3><p>我们已经介绍了RBF Network的Hypothesis可表示为：</p><p><img src="http://img.blog.csdn.net/20170814161854002?" alt="这里写图片描述"></p><p>其中$\mu_m$表示中心点的位置。$\mu_m$的个数M是人为决定的，如果将每个样本点$x_m$都作为一个中心点，即M=N，则我们把这种结构称为full RBF Network。也就是说，对于full RBF Network，每个样本点都对最终的预测都有影响（uniform influence），影响的程度由距离函数和权重$\beta_m$决定。如果每个样本点的影响力都是相同的，设为1，$\beta_m=1\cdot y_m$，那么相当于只根据距离的远近进行投票。最终将x与所有样本点的RBF距离线性组合，经过sign函数后，得到最终的预测分类结果。这实际上就是aggregation的过程，考虑并计入所有样本点的影响力，最后将x与所有样本点的distance similarity进行线性组合。</p><p><img src="http://img.blog.csdn.net/20170814164639278?" alt="这里写图片描述"></p><p>full RBF Network的矩可以表示为：</p><p><img src="http://img.blog.csdn.net/20170814224716379?" alt="这里写图片描述"></p><p>我们来看上式中的Gaussian函数项，当x与样本点$x_m$越接近的时候，其高斯函数值越大。由于Gaussian函数曲线性质，越靠近中心点，值越大；偏离中心点，其值会下降得很快。也就是说，在所有N个中心样本点中，往往只有距离x最近的那个样本点起到关键作用，而其它距离x较远的样本点其值很小，基本可以忽略。因此，为了简化运算，我们可以找到距离x最近的中心样本点，只用这一个点来代替所有N个点，最后得到的矩$g_{nbor}(x)$也只由该最近的中心点决定。这种模型叫做nearest neighbor model，只考虑距离x最近的那一个“邻居”。</p><p>当然可以对nearest neighbor model进行扩展，如果不是只选择一个“邻居”，而是选择距离x最近的k个“邻居”，进行uniformly aggregation，得到最终的矩$g_{nbor}(x)$。这种方法通常叫做k近邻算法（k nearest neighbor）。</p><p><img src="http://img.blog.csdn.net/20170814231531021?" alt="这里写图片描述"></p><p>k nearest neighbor通常比nearest neighbor model效果更好，计算量上也比full RBF Network要简单一些。值得一提的是，k nearest neighbor与full RBF Network都是比较“偷懒”的方法。因为它们在训练模型的时候比较简单，没有太多的运算，但是在测试的时候却要花费更多的力气，找出最相近的中心点，计算相对复杂一些。</p><p>接下来，我们来看一下Full RBF Network有什么样的优点和好处。考虑一个squared error regression问题，且每个RBF的权重为$\beta_m$而不是前面简化的$y_m$。目的是计算最优化模型对应的$\beta_m$值。该hypothesis可表示为：</p><p><img src="http://img.blog.csdn.net/20170815075312519?" alt="这里写图片描述"></p><p>很明显，这是一个简单的线性回归问题，每个RBF都可以看成是特征转换。特征转换后的向量$z_n$可表示为：</p><p>$$z_n=[RBF(x_n,x_1),\  RBF(x_n,x_2),\  \cdots,\  RBF(x_n,x_N)]$$</p><p>那么，根据之前线性回归介绍过的最优化解公式，就能快速地得到$\beta$的最优解为：</p><p>$$\beta=(Z^TZ)^{-1}Z^Ty$$</p><p>上述解的条件是矩阵$Z^TZ$是可逆的。</p><p>矩阵Z的大小是NxN，是一个方阵。而且，由于Z中每个向量$z_n$表示该点与其它所有点的RBF distance，所以从形式上来说，Z也是对称矩阵。如果所有的样本点$x_n$都不一样，则Z一定是可逆的。</p><p><img src="http://img.blog.csdn.net/20170815082805680?" alt="这里写图片描述"></p><p>根据Z矩阵的这些性质，我们可以对$\beta$的解进行化简，得到：</p><p>$$\beta=Z^{-1}y$$</p><p>将$\beta$的解代入矩的计算中，以$x_1$为例，得到：</p><p>$$g_{RBF}(x_1)=\beta^Tz_1=y^TZ^{-1}z_1=y^T\ [1\  0\  \cdots\  0]^T=y_1$$</p><p>结果非常有趣，模型的输出与原样本$y_1$完全相同。同样，对任意的$x_n$，都能得到$g_{RBF}(x_n)=y_n$。因此，$E_{in}(g_{RBF})=0$。看起来，这个模型非常完美了，没有error。但是，我们之前就说过，机器学习中，$E_{in}=0$并非好事，很可能造成模型复杂度增加及过拟合。</p><p><img src="http://img.blog.csdn.net/20170815084315626?" alt="这里写图片描述"></p><p>当然，这种方法在某些领域还是很有用的。比如在函数拟合（function approximation）中，目标就是让$E_{in}=0$，使得原所有样本都尽可能地落在拟合的函数曲线上。</p><p>为了避免发生过拟合，我们可以引入正则项$\lambda$，得到$\beta$的最优解为：</p><p>$$\beta=(Z^TZ+\lambda I)^{-1}Z^Ty$$</p><p><img src="http://img.blog.csdn.net/20170815091954727?" alt="这里写图片描述"></p><p>我们再来看一下Z矩阵，Z矩阵是由一系列Gaussian函数组成，每个Gaussian函数计算的是两个样本之间的distance similarity。这里的Z与之前我们介绍的Gaussian SVM中的kernel K是一致的。当时我们得到kernel ridgeregression中线性系数$\beta$的解为：</p><p>$$\beta=(K+\lambda I)^{-1}y$$</p><p>比较一下kernel ridgeregression与regularized full RBF Network的$\beta$解，形式上相似但不完全相同。这是因为regularization不一样，在kernel ridgeregression中，是对无限多维的特征转换做regularization，而在regularized full RBF Network中，是对有限维（N维度）的特征转换做regularization。因此，两者的公式解有细微差别。</p><p><img src="http://img.blog.csdn.net/20170815092618603?" alt="这里写图片描述"></p><p>除此之外，还有另外一种regularization的方法，就是不把所有N个样本点都拿来作中心点，而是只选择其中的M个样本点作为中心点。类似于SVM中的SV一样，只选择具有代表性的M个中心点。这样减少中心点数量的同时也就减少了权重的数量，能够起到regularization的效果，避免发生过拟合。</p><p><img src="http://img.blog.csdn.net/20170815093443497?" alt="这里写图片描述"></p><p>下一部分，我们将讨论如何选取M个中心点作为好的代表。</p><h3 id="k-Means-Algorithm"><a href="#k-Means-Algorithm" class="headerlink" title="k-Means Algorithm"></a>k-Means Algorithm</h3><p>之所以要选择代表，是因为如果某些样本点很接近，那么就可以用一个中心点来代表它们。这就是聚类（cluster）的思想，从所有N个样本点中选择少数几个代表作为中心点。</p><p><img src="http://img.blog.csdn.net/20170815163423702?" alt="这里写图片描述"></p><p>聚类（clustering）问题是一种典型的非监督式学习（unsupervised learning）。它的优化问题有两个变量需要确定：一个是分类的分群值$S_m$，每一类可表示为$S_1,S_2,\cdots,S_M$；另外一个是每一类对应的中心点$\mu_1,\mu_2,\cdots,\mu_M$。那么对于该聚类问题的优化，其error function可使用squared error measure来衡量。</p><p><img src="http://img.blog.csdn.net/20170815165700953?" alt="这里写图片描述"></p><p>那么，我们的目标就是通过选择最合适的$S_1,S_2,\cdots,S_M$和$\mu_1,\mu_2,\cdots,\mu_M$，使得$E_{in}$最小化。对应的公式可表示为：</p><p><img src="http://img.blog.csdn.net/20170815170616266?" alt="这里写图片描述"></p><p>从这个最小化公式，我们能够发现这是一个组合最佳化的问题，既要优化分群值$S_m$，又要求解每一类的中心点$u_m$。所以，这个最小化问题是比较复杂、难优化的。通常的办法是对S和$\mu$分别进行最优化求解。</p><p>首先，如果$\mu_1,\mu_2,\cdots,\mu_M$是固定的，目标就是只要对所有的$x_n$进行分群归类。这个求解过程很简单，因为每个样本点只能属于一个群S，不能同时属于两个或多个群。所以，只要根据距离公式，计算选择离$x_n$最近的中心点$\mu$即可。</p><p><img src="http://img.blog.csdn.net/20170823170743970?" alt="这里写图片描述"></p><p>然后，如果$S_1,S_2,\cdots,S_M$是固定的，目标就是只要找出每个类的中心点$\mu$。显然，根据上式中的error function，所有的$x_n$分群是已知的，那么该最小化问题就是一个典型的数值最优化问题。对于每个类群$S_m$，利用梯度下降算法，即可得到$\mu_m$的解。</p><p><img src="http://img.blog.csdn.net/20170815192317319?" alt="这里写图片描述"></p><p>如上图所示，中心点$\mu_m$就等于所有属于类群$S_m$的平均位置处。</p><p>经过以上的推导，我们得到了一个非常有名的一种unsupervised learning算法，叫做k-Means Algorithm。这里的k就是代表上面的M，表示类群的个数。</p><p>k-Means Algorithm的流程是这样的：首先，随机选择k个中心点$\mu_1,\mu_2,\cdots,\mu_k$；然后，再由确定的中心点得到不同的类群$S_1,S_2,\cdots,S_k$；接着，再由确定的类群计算出新的不同的k个中心点；继续循环迭代计算，交互地对$\mu$和S值进行最优化计算，不断更新$\mu$和S值，直到程序收敛，实现$E_{in}$最小化。具体算法流程图如下所示：</p><p><img src="http://img.blog.csdn.net/20170815193601984?" alt="这里写图片描述"></p><p>有一个问题是，k-Means Algorithm的循环迭代一定会停止吗？或者说一定能得到最优解吗？答案是肯定的。因为每次迭代更新，$\mu$和S值都会比上一次的值更接近最优解，也就是说$E_{in}$是不断减小的。而$E_{in}$的下界是0，所以，$E_{in}$最终会等于0，$\mu$和S最终能得到最优解。</p><p>k-Means Algorithm已经介绍完毕。接下来，我们把k-Means Algorithm应用到RBF Network中去。首先，使用k-Means，得到原始样本的k个中心点。原始样本到k个中心点组成了RBF特征转换$\Phi(x)$。然后，根据上面介绍过的线性模型，由最优化公式解计算得到权重$\beta$值。最后，将所有的$\Phi(x)$用$\beta$线性组合，即得到矩$g_{RBFNET}(x)$的表达式。具体的算法流程如下所示：</p><p><img src="http://img.blog.csdn.net/20170815204928962?" alt="这里写图片描述"></p><p>值得一提的是，这里我们使用了unsupervised learning（k-Means）与我们上节课介绍的autoencoder类似，同样都是特征转换（feature transform）的方法。</p><p>在最优化求解过程中，参数有k-Means类群个数M、Gaussian函数参数$\lambda$等。我们可以采用validation的方法来选取最佳的参数值。</p><p><img src="http://img.blog.csdn.net/20170815205719760?" alt="这里写图片描述"></p><h3 id="k-means-and-RBF-Network-in-Action"><a href="#k-means-and-RBF-Network-in-Action" class="headerlink" title="k-means and RBF Network in Action"></a>k-means and RBF Network in Action</h3><p>下面这部分，我们将举几个例子，看一下k-Means Algorithm是如何处理分类问题的。</p><p>第一个例子，平面上有4个类群，k=4。首先，我们随机选择4个中心点，如下图中四种颜色的方块所示：</p><p><img src="http://img.blog.csdn.net/20170815210706556?" alt="这里写图片描述"></p><p>第一次迭代，由初始中心点，得到4个类群点的分布：</p><p><img src="http://img.blog.csdn.net/20170815210718226?" alt="这里写图片描述"></p><p>4个类群点确定后，再更新4个中心点的位置：</p><p><img src="http://img.blog.csdn.net/20170815210947603?" alt="这里写图片描述"></p><p>第二次迭代，由上面得到的4个中心点，再计算4个类群点的分布：</p><p><img src="http://img.blog.csdn.net/20170815211219673?" alt="这里写图片描述"></p><p>4个类群点确定后，再更新4个中心点的位置：</p><p><img src="http://img.blog.csdn.net/20170815211325330?" alt="这里写图片描述"></p><p>第三次迭代，由上面得到的4个中心点，再计算4个类群点的分布：</p><p><img src="http://img.blog.csdn.net/20170815211541395?" alt="这里写图片描述"></p><p>4个类群点确定后，再更新4个中心点的位置：</p><p><img src="http://img.blog.csdn.net/20170815211558354?" alt="这里写图片描述"></p><p>第四次迭代，由上面得到的4个中心点，再计算4个类群点的分布：</p><p><img src="http://img.blog.csdn.net/20170815211706419?" alt="这里写图片描述"></p><p>4个类群点确定后，再更新4个中心点的位置：</p><p><img src="http://img.blog.csdn.net/20170815211724216?" alt="这里写图片描述"></p><p>第五次迭代，由上面得到的4个中心点，再计算4个类群点的分布：</p><p><img src="http://img.blog.csdn.net/20170815211826045?" alt="这里写图片描述"></p><p>4个类群点确定后，再更新4个中心点的位置：</p><p><img src="http://img.blog.csdn.net/20170815211841648?" alt="这里写图片描述"></p><p>第六次迭代，由上面得到的4个中心点，再计算4个类群点的分布：</p><p><img src="http://img.blog.csdn.net/20170815211943356?" alt="这里写图片描述"></p><p>4个类群点确定后，再更新4个中心点的位置：</p><p><img src="http://img.blog.csdn.net/20170815212000373?" alt="这里写图片描述"></p><p>从上图我们可以看到，经过六次迭代计算后，聚类的效果已经相当不错了。从另外一个角度来说，k值的选择很重要，下面我们来看看不同的k值对应什么样的分类效果。</p><p><img src="http://img.blog.csdn.net/20170815212602972?" alt="这里写图片描述"></p><p>如上图所示，初始时，我们分别设定k为2，4，7，随机选择中心点位置。在经过多次迭代后，得到的聚类结果如下：</p><p><img src="http://img.blog.csdn.net/20170815212818702?" alt="这里写图片描述"></p><p>通过上面这个例子可以得出，不同的k值会得到不同的聚类效果。还有一点值得注意的是，初始中心点位置也可能会影响最终的聚类。例如上图中k=7的例子，初始值选取的右边三个中心点比较靠近，最后得到的右边三个聚类中心点位置也跟初始位置比较相近。所以，k值大小和初始中心点位置都会影响聚类效果。</p><p>接下来，我们把k-Means应用到RBF Network中，同样分别设定k为2，4，7，不同模型得到的分类效果如下：</p><p><img src="http://img.blog.csdn.net/20170815214012047?" alt="这里写图片描述"></p><p>很明显，k=2时，分类效果不是太好；k=4时，分类效果好一些；而k=7时，分类效果更好，能够更细致地将样本准确分类。这说明了k-Means中k值设置得是否合理，对RBF Network的分类效果起到重要的作用。</p><p>再来看一个例子，如果使用full RBF Network进行分类，即k=N，如下图左边所示，设置正则化因子$\lambda=0.001$。下图右边表示只考虑full RBF Network中的nearest neighbor。下图中间表示的是k=4的RBF Network的分类效果。</p><p><img src="http://img.blog.csdn.net/20170816075335699?" alt="这里写图片描述"></p><p>从上图的比较中，我们可以发现full RBF Network得到的分类线比较弯曲复杂。由于full RBF Network的计算量比较大，所以一般情况下，实际应用得不太多。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Radial Basis Function Network。RBF Network Hypothesis就是计算样本之间distance similarity的Gaussian函数，这类原型替代了神经网络中的神经元。RBF Network的训练学习过程，其实就是对所有的原型Hypotheses进行linear aggregation。然后，我们介绍了一个确定k个中心点的unsupervised learning算法，叫做k-Means Algorithm。这是一种典型的聚类算法，实现对原始样本数据的聚类分群。接着，将k-Means Algorithm应用到RBF Network中，选择合适数量的中心点，得到更好的分类模型。最后，我们列举了几个在实际中使用k-Means和RBF Network的例子，结果显示不同的类群k值对分类的效果影响很大。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170814084714413?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记13 -- Deep Learning</title>
    <link href="https://redstonewill.github.io/2018/03/18/30/"/>
    <id>https://redstonewill.github.io/2018/03/18/30/</id>
    <published>2018-03-18T06:32:42.000Z</published>
    <updated>2018-03-18T06:37:11.100Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170807082811236?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了神经网络Neural Network。神经网络是由一层一层的神经元构成，其作用就是帮助提取原始数据中的模式即特征，简称为pattern feature extraction。神经网络模型的关键是计算出每个神经元的权重，方法就是使用Backpropagation算法，利用GD/SGD，得到每个权重的最优解。本节课我们将继续对神经网络进行深入研究，并介绍层数更多、神经元个数更多、模型更复杂的神经网络模型，即深度学习模型。</p><h3 id="Deep-Neural-Network"><a href="#Deep-Neural-Network" class="headerlink" title="Deep Neural Network"></a>Deep Neural Network</h3><p>总的来说，根据神经网络模型的层数、神经元个数、模型复杂度不同，大致可分为两类：Shallow Neural Networks和Deep Neural Networks。上节课介绍的神经网络模型层数较少，属于Shallow Neural Networks，而本节课将着重介绍Deep Neural Networks。首先，比较一下二者之间的优缺点有哪些：</p><p><img src="http://img.blog.csdn.net/20170807082811236?" alt="这里写图片描述"></p><p>值得一提的是，近些年来，deep learning越来越火，尤其在电脑视觉和语音识别等领域都有非常广泛的应用。原因在于一层一层的神经网络有助于提取图像或者语音的一些物理特征，即pattern feature extraction，从而帮助人们掌握这些问题的本质，建立准确的模型。</p><p>下面举个例子，来看一下深度学习是如何提取出问题潜在的特征从而建立准确的模型的。如下图所示，这是一个手写识别的问题，简单地识别数字1和数字5。</p><p><img src="http://img.blog.csdn.net/20170807084328199?" alt="这里写图片描述"></p><p>如何进行准确的手写识别呢？我们可以将写上数字的图片分解提取出一块一块不同部位的特征。例如左边三幅图每张图代表了数字1的某个部位的特征，三幅图片组合起来就是完整的数字1。右边四幅图也是一样，每张图代表了数字5的某个部位的特征，五幅图组合起来就是完整的数字5。对计算机来说，图片由许多像素点组成。要达到识别的目的，每层神经网络从原始像素中提取出更复杂的特征，再由这些特征对图片内容进行匹配和识别。层数越多，提取特征的个数和深度就越大，同时解决复杂问题的能量就越强，其中每一层都具有相应的物理意义。以上就是深度学习的作用和意义。</p><p>深度学习很强大，同时它也面临很多挑战和困难：</p><ul><li><p><strong>difficult structural decisions</strong></p></li><li><p><strong>high model complexity</strong></p></li><li><p><strong>hard optimization problem</strong></p></li><li><p><strong>huge computational complexity</strong></p></li></ul><p>面对以上深度学习的4个困难，有相应的技术和解决的办法：</p><p><img src="http://img.blog.csdn.net/20170807092913952?" alt="这里写图片描述"></p><p>其中，最关键的技术就是regularization和initialization。</p><p>深度学习中，权重的初始化选择很重要，好的初始值能够帮助避免出现局部最优解的出现。常用的方法就是pre-train，即先权重进行初始值的选择，选择之后再使用backprop算法训练模型，得到最佳的权重值。在接下来的部分，我们将重点研究pre-training的方法。</p><p><img src="http://img.blog.csdn.net/20170807101911830?" alt="这里写图片描述"></p><h3 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h3><p>我们已经介绍了深度学习的架构，那么从算法模型上来说，如何进行pre-training，得到较好的权重初始值呢？首先，我们来看看，权重是什么？神经网络模型中，权重代表了特征转换（feature transform）。从另一个方面也可以说，权重表示一种编码（encoding），就是把数据编码成另外一些数据来表示。因为神经网络是一层一层进行的，有先后顺序，所以就单一层来看，好的权重初始值应该是尽可能地包含了该层输入数据的所有特征，即类似于information-preserving encoding。也就是说，能够把第i层的输入数据的特征传输到第i+1层，再把第i+1层的输入数据的特征传输到第i+2层，一层一层进行下去。这样，每层的权重初始值起到了对该层输入数据的编码作用，能够最大限度地保持其特征。</p><p>举个例子，上一小节我们讲了简单的手写识别的例子。从原始的一张像素图片转换到分解的不同笔画特征，那么反过来，这几个笔画特征也可以组合成原来的数字。这种可逆的转换被称为information-preserving，即转换后的特征保留了原输入的特征，而且转换是可逆的。这正是pre-train希望做到的，通过encoding将输入转换为一些特征，而这些特征又可以复原原输入x，实现information-preserving。所以，pre-training得到的权重初始值就应该满足这样的information-preserving特性。</p><p><img src="http://img.blog.csdn.net/20170807175831186?" alt="这里写图片描述"></p><p>如何在pre-training中得到这样的权重初始值（即转换特征）呢？方法是建立一个简单的三层神经网络（一个输入层、一个隐藏层、一个输出层），如下图所示。</p><p><img src="http://img.blog.csdn.net/20170807224639626?" alt="这里写图片描述"></p><p>该神经网络中，输入层是原始数据（即待pre-training的数据），经过权重$W_{ij}^{(1)}$得到隐藏层的输出为原始数据新的表达方式（即转换特征）。这些转换特征再经过权重$W_{ji}^{(2)}$得到输出层，输出层的结果要求跟原始数据类似，即输入层和输出层是近似相等的。整个网络是$d-\breve{d}-d$ NNet结构。其核心在于“重构性”，从输入层到隐藏层实现特征转换，从隐藏层到输出层实现重构，满足上文所说的information-preserving的特性。这种结构的神经网络我们称之为autoencoder，输入层到隐藏层对应编码，而隐藏层到输出层对应解码。其中，$W_{ij}^{(1)}$表示编码权重，而$W_{ji}^{(2)}$表示解码权重。整个过程类似于在学习如何近似逼近identity function。</p><p><img src="http://img.blog.csdn.net/20170808074316857?" alt="这里写图片描述"></p><p>那么为什么要使用这样的结构来逼近identity function，有什么好处呢？首先对于监督式学习（supervised learning），这种$d-\breve{d}-d$的NNet结构中含有隐藏层。隐藏层的输出实际上就是对原始数据合理的特征转换$\phi(x)$，例如手写识别中隐藏层分解的各个笔画，包含了有用的信息。这样就可以从数据中学习得到一些有用的具有代表性的信息。然后，对于非监督式学习（unsupervised learning），autoencoder也可以用来做density estimation。如果网络最终的输出$g(x)\approx x$，则表示密度较大；如果g(x)与x相差甚远，则表示密度较小。也就是说可以根据g(x)与x的接近程度来估计测试数据是落在密度较大的地方还是密度较小的地方。这种方法同样适用于outlier detection，异常检测。这样就可以从数据中学习得到一些典型的具有代表性的信息，找出哪些是典型资料，哪些不是典型资料。所以说，通过autoencoder不断逼近identity function，对监督式学习和非监督式学习都具有深刻的物理意义和非常广泛的应用。</p><p><img src="http://img.blog.csdn.net/20170808081826212?" alt="这里写图片描述"></p><p>其实，对于autoencoder来说，我们更关心的是网络中间隐藏层，即原始数据的特征转换以及特征转换的编码权重$W_{ij}^{(1)}$。</p><p>Basic Autoencoder一般采用$d-\breve{d}-d$的NNet结构，对应的error function是squared error，即$\sum_{i=1}^d(g_i(x)-x_i)^2$。</p><p><img src="http://img.blog.csdn.net/20170808082741616?" alt="这里写图片描述"></p><p>basic autoencoder在结构上比较简单，只有三层网络，容易训练和优化。各层之间的神经元数量上，通常限定$\breve d&lt;d$，便于数据编码。数据集可表示为：${(x_1,y_1=x_1),(x_2,y_2=x_2),\cdots,(x_N,y_N=x_N)}$，即输入输出都是x，可以看成是非监督式学习。一个重要的限制条件是$W_{ij}^{(1)}=W_{ji}^{(2)}$，即编码权重与解码权重相同。这起到了regularization的作用，但是会让计算复杂一些。</p><p><img src="http://img.blog.csdn.net/20170808083737393?" alt="这里写图片描述"></p><p>以上就是basic autoencoder的结构和一些限定条件。深度学习中，basic autoencoder的过程也就对应着pre-training的过程，使用这种方法，对无label的原始数据进行编码和解码，得到的编码权重$W_{ij}^{(1)}$就可以作为pre-trained的比较不错的初始化权重，也就是作为深度学习中层与层之间的初始化权重。</p><p><img src="http://img.blog.csdn.net/20170808084602404?" alt="这里写图片描述"></p><p>我们在本节课第一部分就说了深度学习中非常重要的一步就是pre-training，即权重初始化，而autoencoder可以作为pre-training的一个合理方法。Pre-training的整个过程是：首先，autoencoder会对深度学习网络第一层（即原始输入）进行编码和解码，得到编码权重$W_{ij}^{(1)}$，作为网络第一层到第二层的的初始化权重；然后再对网络第二层进行编码和解码，得到编码权重$W_{ij}^{(1)}$，作为网络第二层到第三层的初始化权重，以此类推，直到深度学习网络中所有层与层之间都得到初始化权重。值得注意的是，对于l-1层的网络${x_n^{(l-1)}}$，autoencoder中的$\breve d$应与下一层（即l层）的神经元个数相同。</p><p><img src="http://img.blog.csdn.net/20170808090507228?" alt="这里写图片描述"></p><p>当然，除了basic autoencoder之外还有许多其它表现不错的pre-training方法。这些方法大都采用不同的结构和正则化技巧来得到不同的’fancier’ autoencoders，这里不再赘述。</p><h3 id="Denoising-Autoencoder"><a href="#Denoising-Autoencoder" class="headerlink" title="Denoising Autoencoder"></a>Denoising Autoencoder</h3><p>上一部分，我们使用autoencoder解决了deep learning中pre-training的问题。接下来，我们将讨论deep learning中有什么样的regularization方式来控制模型的复杂度。</p><p><img src="http://img.blog.csdn.net/20170809075826822?" alt="这里写图片描述"></p><p>由于深度学习网络中神经元和权重的个数非常多，相应的模型复杂度就会很大，因此，regularization非常必要。之前我门也介绍过一些regularization的方法，包括：</p><ul><li><p><strong>structural decisions/constraints</strong></p></li><li><p><strong>weight decay or weight elimination regularizers</strong></p></li><li><p><strong>early stopping</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170809080411186?" alt="这里写图片描述"></p><p>下面我们将介绍另外一种regularization的方式，它在deep learning和autoencoder中都有很好的效果。</p><p>首先我们来复习一下之前介绍的overfitting产生的原因有哪些。如下图所示，我们知道overfitting与样本数量、噪声大小都有关系，数据量减少或者noise增大都会造成overfitting。如果数据量是固定的，那么noise的影响就非常大，此时，实现regularization的一个方法就是消除noise的影响。</p><p><img src="http://img.blog.csdn.net/20170809081534072?" alt="这里写图片描述"></p><p>去除noise的一个简单方法就是对数据进行cleaning/pruning的操作。但是，这种方法通常比较麻烦，费时费力。此处，有一种比较“疯狂”的方法，就是往数据中添加一些noise。注意是添加noise！下面我们来解释这样做到底有什么作用。</p><p><img src="http://img.blog.csdn.net/20170809082321042?" alt="这里写图片描述"></p><p>这种做法的idea来自于如何建立一个健壮（robust）的autoencoder。在autoencoder中，编码解码后的输出g(x)会非常接近真实样本值x。此时，如果对原始输入加入一些noise，对于健壮的autoencoder，编码解码后的输出g(x)同样会与真实样本值x很接近。举个例子，手写识别中，通常情况下，写的很规范的数字1经过autoencoder后能够复原为数字1。如果原始图片数字1歪斜或加入噪声，经过autoencoder后应该仍然能够解码为数字1。这表明该autoencoder是robust的，一定程度上起到了抗噪声和regularization的作用，这正是我们希望看到的。</p><p>所以，这就引出了denoising autoencoder的概念。denoising autoencoder不仅能实现编码和解码的功能，还能起到去噪声、抗干扰的效果，即输入一些混入noise的数据，经过autoencoder之后能够得到较纯净的数据。这样，autoencoder的样本集为：</p><p>$${(\breve{x}_1,y_1=x_1),(\breve{x}_2,y_2=x_2),\cdots,(\breve{x}_N,y_N=x_N)}$$</p><p>其中$\breve{x}_n=x_n+noise$，为混入噪声的样本，而$x_n$为纯净样本。</p><p>autoencoder训练的目的就是让$\breve{x}_n$经过编码解码后能够复原为纯净的样本$x_n$。那么，在deep learning的pre-training中，如果使用这种denoising autoencoder，不仅能从纯净的样本中编解码得到纯净的样本，还能从混入noise的样本中编解码得到纯净的样本。这样得到的权重初始值更好，因为它具有更好的抗噪声能力，即健壮性好。实际应用中，denoising autoencoder非常有用，在训练过程中，输入混入人工noise，输出纯净信号，让模型本身具有抗噪声的效果，让模型健壮性更强，最关键的是起到了regularization的作用。</p><p><img src="http://img.blog.csdn.net/20170809085825116?" alt="这里写图片描述"></p><h3 id="Principal-Component-Analysis"><a href="#Principal-Component-Analysis" class="headerlink" title="Principal Component Analysis"></a>Principal Component Analysis</h3><p>刚刚我们介绍的autoencoder是非线性的，因为其神经网络模型中包含了tanh()函数。这部分我们将介绍linear autoencoder。nonlinear autoencoder通常比较复杂，多应用于深度学习中；而linear autoencoder通常比较简单，我们熟知的主成分分析（Principal Component Analysis，PCA），其实跟linear autoencoder有很大的关系。</p><p>对于一个linear autoencoder，它的第k层输出不包含tanh()函数，可表示为：</p><p>$$h_k(x)=\sum_{j=0}^{\breve{d}}w_{jk}^{(2)}(\sum_{i=0}^dw_{ij}^{(1)}x_i)$$</p><p>其中，$w_{ij}^{(1)}$和$w_{jk}^{(2)}$分别是编码权重和解码权重。而且，有三个限制条件，分别是：</p><ul><li><p><strong>移除常数项$x_0$，让输入输出维度一致</strong></p></li><li><p><strong>编码权重与解码权重一致：$w_{ij}^{(1)}=w_{jk}^{(2)}=w_{ij}</strong></p></li><li><p><strong>$\breve{d}&lt;d$</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170809143637045?" alt="这里写图片描述"></p><p>这样，编码权重用W表示，维度是d x $\breve{d}$，解码权重用$W^T$表示。x的维度为d x 1。则linear autoencoder hypothesis可经过下式计算得到：</p><p>$$h(x)=WW^Tx$$</p><p>其实，linear autoencoder hypothesis就应该近似于原始输入x的值，即h(x)=x。根据这个，我们可以写出它的error function：</p><p><img src="http://img.blog.csdn.net/20170809145218227?" alt="这里写图片描述"></p><p>我们的目的是计算出$E_{in}(h)$最小化时对应的W。根据线性代数知识，首先进行特征值分解：</p><p>$$WW^T=V\Gamma V^T$$</p><p>其中$WW^T$是半正定矩阵。V矩阵满足$VV^T=V^TV=I_d$。$\Gamma$是对角矩阵，对角线上有不超过$\breve{d}$个非零值（即为1），即对角线零值个数大于等于$d-\breve(d)$。根据特征值分解的思想，我们可以把$x_n$进行类似分解：</p><p>$$x_n=VIV^Tx_n$$</p><p>其中，I是单位矩阵，维度为dxd。这样，通过特征值分解我们就把对W的优化问题转换成对$\Gamma$和V的优化问题。</p><p><img src="http://img.blog.csdn.net/20170809155615900?" alt="这里写图片描述"></p><p>首先，我们来优化$\Gamma$值，表达式如下：</p><p><img src="http://img.blog.csdn.net/20170809155740158?" alt="这里写图片描述"></p><p>要求上式的最小化，可以转化为$(I-\Gamma)$越小越好，其结果对角线上零值越多越好，即I与$\Gamma$越接近越好。因为$\Gamma$的秩是小于等于$\breve{d}$的，$\Gamma$最多有$\breve{d}$个1。所以，$\Gamma$的最优解是其对角线上有$\breve{d}$个1。</p><p><img src="http://img.blog.csdn.net/20170809160725018?" alt="这里写图片描述"></p><p>那么，$\Gamma$的最优解已经得出，表达式变成：</p><p><img src="http://img.blog.csdn.net/20170809160853847?" alt="这里写图片描述"></p><p>这里的最小化问题似乎有点复杂，我们可以做一些转换，把它变成最大化问题求解，转换后的表达式为：</p><p><img src="http://img.blog.csdn.net/20170809161356367?" alt="这里写图片描述"></p><p>当$\breve{d}=1$时，$V^T$中只有第一行$v^T$有用，最大化问题转化为：</p><p>$$max_v\sum_{n=1}^Nv^Tx_nx_n^Tv\ \ \ \ \ subject\ to\ v^Tv=1$$</p><p>引入拉格朗日因子$\lambda$，表达式的微分与条件微分应该是平行的，且由$\lambda$联系起来，即：</p><p>$$\sum_{n=1}^Nx_nx_n^Tv=\lambda v$$</p><p>根据线性代数的知识，很明显能够看出，v就是矩阵$X^TX$的特征向量，而$\lambda$就是相对应的特征值。我们要求的是最大值，所以最优解v就是矩阵$X^TX$最大特征值对应的特征向量。</p><p>当$\breve{d}&gt;1$时，求解方法是类似的，最优解${v_j}_{j=1}^{\breve d}$就是矩阵$X^TX$前$\breve{d}$大的特征值对应的$\breve{d}$个特征向量。</p><p>经过以上分析，我们得到了$\Gamma$和V的最优解。这就是linear autoencoder的编解码推导过程。</p><p><img src="http://img.blog.csdn.net/20170809163958200?" alt="这里写图片描述"></p><p>值得一提的是，linear autoencoder与PCA推导过程十分相似。但有一点不同的是，一般情况下，PCA会对原始数据x进行处理，即减去其平均值。这是为了在推导过程中的便利。这两种算法的计算流程大致如下：</p><p><img src="http://img.blog.csdn.net/20170809164756333?" alt="这里写图片描述"></p><p>linear autoencoder与PCA也有差别，PCA是基于统计学分析得到的。一般我们认为，将高维数据投影（降维）到低维空间中，应该保证数据本身的方差越大越好，而噪声方差越小越好，而PCA正是基于此原理推导的。linear autoencoder与PCA都可以用来进行数据压缩，但是PCA应用更加广泛一些。</p><p><img src="http://img.blog.csdn.net/20170809165309964?" alt="这里写图片描述"></p><p>以上关于PCA的推导基本上是从几何的角度，而没有从代数角度进行详细的数学推导。网上关于PCA的资料很多，这里附上一篇个人觉得讲解得通俗易懂的PCA原理介绍：<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">PCA的数学原理</a>。有兴趣的朋友可以看一看。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了深度学习（deep learning）的数学模型，也是上节课讲的神经网络的延伸。由于深度学习网络的复杂性，其建模优化是比较困难的。通常，我们可以从pre-training和regularization的角度来解决这些困难。首先，autoencoder可以得到比较不错的初始化权重，起到pre-training的效果。然后，denoising autoencoder通过引入人工噪声，训练得到初始化权重，从而使模型本身抗噪声能力更强，更具有健壮性，起到了regularization的效果。最后，我们介绍了linear autoencoder并从几何角度详述了其推导过程。linear autoencoder与PCA十分类似，都可以用来进行数据压缩和数据降维处理。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170807082811236?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记12 -- Neural Network</title>
    <link href="https://redstonewill.github.io/2018/03/18/29/"/>
    <id>https://redstonewill.github.io/2018/03/18/29/</id>
    <published>2018-03-18T06:20:46.000Z</published>
    <updated>2018-03-18T06:22:18.565Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170731201555148?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Gradient Boosted Decision Tree。GBDT通过使用functional gradient的方法得到一棵一棵不同的树，然后再使用steepest descent的方式给予每棵树不同的权重，最后可以用来处理任何而定error measure。上节课介绍的GBDT是以regression为例进行介绍的，使用的是squared error measure。本节课讲介绍一种出现时间较早，但当下又非常火的一种机器算法模型，就是神经网络（Neural Network）。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>在之前的机器学习基石课程中，我们就接触过Perceptron模型了，例如PLA算法。Perceptron就是在矩$g_t(x)$外面加上一个sign函数，取值为{-1,+1}。现在，如果把许多perceptrons线性组合起来，得到的模型G就如下图所示：</p><p><img src="http://img.blog.csdn.net/20170731201555148?" alt="这里写图片描述"></p><p>将左边的输入$(x_0,x_1,x_2,\cdots,x_d)$与T个不同的权重$(w_1,w_2,\cdots,w_T)$相乘（每个$w_i$是d+1维的），得到T个不同的perceptrons为$(g_1,g_2,\cdots,g_T)$。最后，每个$g_t$给予不同的权重$(\alpha_1,\alpha_2,\cdots,\alpha_T)$，线性组合得到G。G也是一个perceptron模型。</p><p>从结构上来说，上面这个模型包含了两层的权重，分别是$w_t$和$\alpha$。同时也包含了两层的sign函数，分别是$g_t$和G。那么这样一个由许多感知机linear aggregation的模型能实现什么样的boundary呢？</p><p>举个简单的例子，如下图所示，$g_1$和$g_2$分别是平面上两个perceptrons。其中，红色表示-1，蓝色表示+1。这两个perceptrons线性组合可能得到下图右侧的模型，这表示的是$g_1$和$g_2$进行与（AND）的操作，蓝色区域表示+1。</p><p><img src="http://img.blog.csdn.net/20170731205216172?" alt="这里写图片描述"></p><p>如何通过感知机模型来实现上述的$AND(g_1,g_2)$逻辑操作呢？一种方法是令第二层中的$\alpha_0=-1,\alpha_1=+1,\alpha_2=+1$。这样，G(x)就可表示为：</p><p>$$G(x)=sign(-1+g_1(x)+g_2(x))$$</p><p>$g_1$和$g_2$的取值是{-1,+1}，当$g_1=-1，g_2=-1$时，G(x)=0；当$g_1=-1，g_2=+1$时，G(x)=0；当$g_1=+1，g_2=-1$时，G(x)=0；当$g_1=+1，g_2=+1$时，G(x)=1。感知机模型如下所示：</p><p><img src="http://img.blog.csdn.net/20170731210353253?" alt="这里写图片描述"></p><p>这个例子说明了一些简单的线性边界，如上面的$g_1$和$g_2$，在经过一层感知机模型，经线性组合后，可以得到一些非线性的复杂边界（AND运算）G(x)。</p><p>除此之外，或（OR）运算和非（NOT）运算都可以由感知机建立相应的模型，非常简单。</p><p>所以说，linear aggregation of perceptrons实际上是非常powerful的模型同时也是非常complicated模型。再看下面一个例子，如果二维平面上有个圆形区域，圆内表示+1，圆外表示-1。这样复杂的圆形边界是没有办法使用单一perceptron来解决的。如果使用8个perceptrons，用刚才的方法线性组合起来，能够得到一个很接近圆形的边界（八边形）。如果使用16个perceptrons，那么得到的边界更接近圆形（十六边形）。因此，使用的perceptrons越多，就能得到各种任意的convex set，即凸多边形边界。之前我们在机器学习基石中介绍过，convex set的VC Dimension趋向于无穷大（$2^N$）。这表示只要perceptrons够多，我们能得到任意可能的情况，可能的模型。但是，这样的坏处是模型复杂度可能会变得很大，从而造成过拟合（overfitting）。</p><p><img src="http://img.blog.csdn.net/20170731212555912?" alt="这里写图片描述"></p><p>总的来说，足够数目的perceptrons线性组合能够得到比较平滑的边界和稳定的模型，这也是aggregation的特点之一。</p><p>但是，也有单层perceptrons线性组合做不到的事情。例如刚才我们将的AND、OR、NOT三种逻辑运算都可以由单层perceptrons做到，而如果是异或（XOR）操作，就没有办法只用单层perceptrons实现。这是因为XOR得到的是非线性可分的区域，如下图所示，没有办法由$g_1$和$g_2$线性组合实现。所以说linear aggregation of perceptrons模型的复杂度还是有限制的。</p><p><img src="http://img.blog.csdn.net/20170731213544161?" alt="这里写图片描述"></p><p>那么，为了实现XOR操作，可以使用多层perceptrons，也就是说一次transform不行，我们就用多层的transform，这其实就是Basic Neural Network的基本原型。下面我们就尝试使用两层perceptrons来实现XOR的操作。</p><p>首先，根据布尔运算，异或XOR操作可以拆分成：</p><p>$$XOR(g_1,g_2)=OR(AND(-g_1,g_2),AND(g_1,-g_2))$$</p><p>这种拆分实际上就包含了两层transform。第一层仅有AND操作，第二层是OR操作。这种两层的感知机模型如下所示：</p><p><img src="http://img.blog.csdn.net/20170731230146579?" alt="这里写图片描述"></p><p>这样，从AND操作到XOR操作，从简单的aggregation of perceptrons到multi-layer perceptrons，感知机层数在增加，模型的复杂度也在增加，使最后得到的G能更容易解决一些非线性的复杂问题。这就是基本神经网络的基本模型。</p><p><img src="http://img.blog.csdn.net/20170731230552144?" alt="这里写图片描述"></p><p>顺便提一下，这里所说的感知机模型实际上就是在模仿人类的神经元模型（这就是Neural Network名称的由来）。感知机模型每个节点的输入就对应神经元的树突dendrite，感知机每个节点的输出就对应神经元的轴突axon。</p><h3 id="Neural-Network-Hypothesis"><a href="#Neural-Network-Hypothesis" class="headerlink" title="Neural Network Hypothesis"></a>Neural Network Hypothesis</h3><p>上一部分我们介绍的这种感知机模型其实就是Neural Network。输入部分经过一层一层的运算，相当于一层一层的transform，最后通过最后一层的权重，得到一个分数score。即在OUTPUT层，输出的就是一个线性模型。得到s后，下一步再进行处理。</p><p><img src="http://img.blog.csdn.net/20170801074857278?" alt="这里写图片描述"></p><p>我们之前已经介绍过三种线性模型：linear classification，linear regression，logistic regression。那么，对于OUTPUT层的分数s，根据具体问题，可以选择最合适的线性模型。如果是binary classification问题，可以选择linear classification模型；如果是linear regression问题，可以选择linear regression模型；如果是soft classification问题，则可以选择logistic regression模型。本节课接下来将以linear regression为例，选择squared error来进行衡量。</p><p><img src="http://img.blog.csdn.net/20170801081220770?" alt="这里写图片描述"></p><p>上面讲的是OUTPUT层，对于中间层，每个节点对应一个perceptron，都有一个transform运算。上文我们已经介绍过的transformation function是阶梯函数sign()。那除了sign()函数外，有没有其他的transformation function呢？</p><p>如果每个节点的transformation function都是线性运算（跟OUTPUT端一样），那么由每个节点的线性模型组合成的神经网络模型也必然是线性的。这跟直接使用一个线性模型在效果上并没有什么差异，模型能力不强，反而花费了更多不必要的力气。所以一般来说，中间节点不会选择线性模型。</p><p>如果每个节点的transformation function都是阶梯函数（即sign()函数）。这是一个非线性模型，但是由于阶梯函数是离散的，并不是处处可导，所以在优化计算时比较难处理。所以，一般也不选择阶梯函数作为transformation function。</p><p>既然线性函数和阶梯函数都不太适合作为transformation function，那么最常用的一种transformation function就是tanh(s)，其表达式如下：</p><p>$$tanh(s)=\frac{exp(s)-exp(-s)}{exp(s)+exp(-s)}$$</p><p>tanh(s)函数是一个平滑函数，类似“s”型。当|s|比较大的时候，tanh(s)与阶梯函数相近；当|s|比较小的时候，tanh(s)与线性函数比较接近。从数学上来说，由于处处连续可导，便于最优化计算。而且形状上类似阶梯函数，具有非线性的性质，可以得到比较复杂强大的模型。</p><p>顺便提一下，tanh(x)函数与sigmoid函数存在下列关系：</p><p>$$tanh(s)=2\theta(2s)-1$$</p><p>其中，</p><p>$$\theta(s)=\frac{1}{1+exp(-s)}$$</p><p><img src="http://img.blog.csdn.net/20170801084655875?" alt="这里写图片描述"></p><p>那么，接下来我们就使用tanh函数作为神经网络中间层的transformation function，所有的数学推导也基于此。实际应用中，可以选择其它的transformation function，不同的transformation function，则有不同的推导过程。</p><p>下面我们将仔细来看看Neural Network Hypothesis的结构。如下图所示，该神经网络左边是输入层，中间两层是隐藏层，右边是输出层。整体上来说，我们设定输入层为第0层，然后往右分别是第一层、第二层，输出层即为第3层。</p><p><img src="http://img.blog.csdn.net/20170801085603255?" alt="这里写图片描述"></p><p>Neural Network Hypothesis中，$d^{(0)},d^{(1)},\cdots,d^{(L)}$分别表示神经网络的第几层，其中L为总层数。例如上图所示的是3层神经网络，L=3。我们先来看看每一层的权重$w_{ij}^{(l)}$，上标l满足$1\leq l\leq L$，表示是位于哪一层。下标i满足$0\leq i\leq d^{(l-1)}$，表示前一层输出的个数加上bias项（常数项）。下标j满足$1\leq j\leq d^{(l)}$，表示该层节点的个数（不包括bias项）。</p><p>对于每层的分数score，它的表达式为：</p><p>$$s_j^{(l)}=\sum_{i=0}^{d^{(l-1)}}w_{ij}^{(l)}x_i^{(l-1)}$$</p><p>对于每层的transformation function，它的表达式为：</p><p>$$x_j^{(l)}=\begin{cases}<br>        tanh(s_j^{(l)}), &amp; if\ l&lt;L\<br>        s_j^{(l)}, &amp; if\ l=L<br>    \end{cases}$$</p><p>因为是regression模型，所以在输出层（l=L）直接得到$x_j^{(l)}=s_j^{(l)}$。</p><p><img src="http://img.blog.csdn.net/20170801102651009?" alt="这里写图片描述"></p><p>介绍完Neural Network Hypothesis的结构之后，我们来研究下这种算法结构到底有什么实际的物理意义。还是看上面的神经网络结构图，每一层输入到输出的运算过程，实际上都是一种transformation，而转换的关键在于每个权重值$w_{ij}^{(l)}$。每层网络利用输入x和权重w的乘积，在经过tanh函数，得到该层的输出，从左到右，一层一层地进行。其中，很明显，x和w的乘积$\sum_{i=0}^{d^{(l-1)}}w_{ij}^{(l)}x_i^{(l-1)}$越大，那么tanh(wx)就会越接近1，表明这种transformation效果越好。再想一下，w和x是两个向量，乘积越大，表明两个向量内积越大，越接近平行，则表明w和x有模式上的相似性。从而，更进一步说明了如果每一层的输入向量x和权重向量w具有模式上的相似性，比较接近平行，那么transformation的效果就比较好，就能得到表现良好的神经网络模型。也就是说，神经网络训练的核心就是pattern extraction，即从数据中找到数据本身蕴含的模式和规律。通过一层一层找到这些模式，找到与输入向量x最契合的权重向量w，最后再由G输出结果。</p><p><img src="http://img.blog.csdn.net/20170801105658700?" alt="这里写图片描述"></p><h3 id="Neural-Network-Learning"><a href="#Neural-Network-Learning" class="headerlink" title="Neural Network Learning"></a>Neural Network Learning</h3><p>我们已经介绍了Neural Network Hypothesis的结构和算法流程。确定网络结构其实就是确定各层的权重值$w_{ij}^{(l)}$。那如何根据已有的样本数据，找到最佳的权重$w_{ij}^{(l)}$使error最小化呢？下面我们将详细推导。</p><p><img src="http://img.blog.csdn.net/20170801134927174?" alt="这里写图片描述"></p><p>首先，我们的目标是找到最佳的$w_{ij}^{(l)}$让$E_{in}({w_{ij}^{(l)}})$最小化。如果只有一层隐藏层，就相当于是aggregation of perceptrons。可以使用我们上节课介绍的gradient boosting算法来一个一个确定隐藏层每个神经元的权重，输入层到隐藏层的权重可以通过C&amp;RT算法计算的到。这不是神经网络常用的算法。如果隐藏层个数有两个或者更多，那么aggregation of perceptrons的方法就行不通了。就要考虑使用其它方法。</p><p>根据error function的思想，从输出层来看，我们可以得到每个样本神经网络预测值与实际值之间的squared error：$e_n=(y_n-NNet(x_n))^2$，这是单个样本点的error。那么，我们只要能建立$e_n$与每个权重$w_{ij}^{(l)}$的函数关系，就可以利用GD或SGD算法对$w_{ij}^{(l)}$求偏微分，不断迭代优化$w_{ij}^{(l)}$值，最终得到使$e_n$最小时对应的$w_{ij}^{(l)}$。</p><p><img src="http://img.blog.csdn.net/20170801140710642?" alt="这里写图片描述"></p><p>为了建立$e_n$与各层权重$w_{ij}^{(l)}$的函数关系，求出$e_n$对$w_{ij}^{(l)}$的偏导数$\frac{\partial e_n}{w_{ij}^{(l)}}$，我们先来看输出层如何计算$\frac{\partial e_n}{w_{i1}^{(L)}}$。$e_n$与$w_{i1}^{(L)}$的函数关系为：</p><p><img src="http://img.blog.csdn.net/20170801142747629?" alt="这里写图片描述"></p><p>计算$e_n$对$w_{i1}^{(L)}$的偏导数，得到：</p><p><img src="http://img.blog.csdn.net/20170801143202613?" alt="这里写图片描述"></p><p>以上是输出层求偏导的结果。如果是其它层，即$l\neq L$，偏导计算可以写成如下形式：</p><p><img src="http://img.blog.csdn.net/20170801143443495?" alt="这里写图片描述"></p><p>上述推导中，令$e_n$与第l层第j个神经元的分数$s_j^{(l)}$的偏导数记为$\delta_j^{(l)}$。即：</p><p>$$\frac{\partial e_n}{\partial s_j^{(l)}}=\delta_j^{(l)}$$</p><p>当$l=L$时，$\delta_1^{(L)}=-2(y_n-s_1^{(L)})$；当$l\neq L$时，$\delta_j^{(l)}$是未知的，下面我们将进行运算推导，看看不同层之间的$\delta_j^{(l)}$是否有递推关系。</p><p><img src="http://img.blog.csdn.net/20170801144815735?" alt="这里写图片描述"></p><p>如上图所示，第l层第j个神经元的分数$s_j^{(l)}$经过tanh函数，得到该层输出$x_j^{(l)}$，再与下一层权重$w_{jk}^{(l+1)}$相乘，得到第l+1层的分数$s_j^{(l+1)}$，直到最后的输出层$e_n$。</p><p>那么，利用上面$s_j^{(l)}$到$s_j^{(l+1)}$这样的递推关系，我们可以对偏导数$\delta_j^{(l)}$做一些中间变量替换处理，得到如下表达式：</p><p><img src="http://img.blog.csdn.net/20170801150154804?" alt="这里写图片描述"></p><p>值得一提的是，上式中有个求和项，其中k表示下一层即l+1层神经元的个数。表明l层的$s_j^{(l)}$与l+1层的所有$s_k^{(l+1)}$都有关系。因为$s_j^{(l)}$参与到每个$s_k^{(l+1)}$的运算中了。</p><p>这样，我们得到了$\delta_j^{(l)}$与$\delta_k^{(l)}$的递推关系。也就是说如果知道了$\delta_k^{(l)}$的值，就能推导出$\delta_j^{(l)}$的值。而最后一层，即输出层的$\delta_1^{(L)}=-2(y_n-s_1^{(L)})$，那么就能一层一层往前推导，得到每一层的$\delta_j^{(l)}$，从而可以计算出$e_n$对各个$w_{ij}^{(l)}$的偏导数$\frac{\partial e_n}{w_{ij}^{(l)}}$。计算完偏微分之后，就可以使用GD或SGD算法进行权重的迭代优化，最终得到最优解。</p><p>神经网络中，这种从后往前的推导方法称为Backpropagation Algorithm，即我们常常听到的BP神经网络算法。它的算法流程如下所示：</p><p><img src="http://img.blog.csdn.net/20170801151811019?" alt="这里写图片描述"></p><p>上面采用的是SGD的方法，即每次迭代更新时只取一个点，这种做法一般不够稳定。所以通常会采用mini-batch的方法，即每次选取一些数据，例如$\frac{N}{10}$，来进行训练，最后求平均值更新权重w。这种做法的实际效果会比较好一些。</p><h3 id="Optimization-and-Regularization"><a href="#Optimization-and-Regularization" class="headerlink" title="Optimization and Regularization"></a>Optimization and Regularization</h3><p>经过以上的分析和推导，我们知道神经网络优化的目标就是让$E_{in}(w)$最小化。本节课我们采用error measure是squared error，当然也可以采用其它的错误衡量方式，只要在推导上做稍稍修改就可以了，此处不再赘述。</p><p><img src="http://img.blog.csdn.net/20170801193835369?" alt="这里写图片描述"></p><p>下面我们将主要分析神经网络的优化问题。由于神经网络由输入层、多个隐藏层、输出层构成，结构是比较复杂的非线性模型，因此$E_{in}(w)$可能有许多局部最小值，是non-convex的，找到全局最小值（globalminimum）就会困难许多。而我们使用GD或SGD算法得到的很可能就是局部最小值（local minimum）。</p><p>基于这个问题，不同的初始值权重$w_{ij}^{(l)}$通常会得到不同的local minimum。也就是说最终的输出G与初始权重$w_{ij}^{(l)}$有很大的关系。在选取$w_{ij}^{(l)}$上有个技巧，就是通常选择比较小的值，而且最好是随机random选择。这是因为，如果权重$w_{ij}^{(l)}$很大，那么根据tanh函数，得到的值会分布在两侧比较平缓的位置（类似于饱和saturation），这时候梯度很小，每次迭代权重可能只有微弱的变化，很难在全局上快速得到最优解。而随机选择的原因是通常对权重$w_{ij}^{(l)}$如何选择没有先验经验，只能通过random，从普遍概率上选择初始值，随机性避免了人为因素的干预，可以说更有可能经过迭代优化得到全局最优解。</p><p><img src="http://img.blog.csdn.net/20170801203424117?" alt="这里写图片描述"></p><p>下面从理论上看一下神经网络模型的VC Dimension。对于tanh这样的transfer function，其对应的整个模型的复杂度$d_{vc}=O(VD)$。其中V是神经网络中神经元的个数（不包括bias点）,D表示所有权值的数量。所以，如果V足够大的时候，VC Dimension也会非常大，这样神经网络可以训练出非常复杂的模型。但同时也可能会造成过拟合overfitting。所以，神经网络中神经元的数量V不能太大。</p><p>为了防止神经网络过拟合，一个常用的方法就是使用regularization。之前我们就介绍过可以在error function中加入一个regularizer，例如熟悉的L2 regularizer $\Omega(w)$：</p><p>$$\Omega(w)=\sum(w_{ij}^{(l)})^2$$</p><p>但是，使用L2 regularizer 有一个缺点，就是它使每个权重进行等比例缩小（shrink）。也就是说大的权重缩小程度较大，小的权重缩小程度较小。这会带来一个问题，就是等比例缩小很难得到值为零的权重。而我们恰恰希望某些权重$w_{ij}^{(l)}=0$，即权重的解是松散（sparse）的。因为这样能有效减少VC Dimension，从而减小模型复杂度，防止过拟合发生。</p><p>那么为了得到sparse解，有什么方法呢？我们之前就介绍过可以使用L1 regularizer：$\sum|w{ij}^{(l)}|$，但是这种做法存在一个缺点，就是包含绝对值不容易微分。除此之外，另外一种比较常用的方法就是使用weight-elimination regularizer。weight-elimination regularizer类似于L2 regularizer，只不过是在L2 regularizer上做了尺度的缩小，这样能使large weight和small weight都能得到同等程度的缩小，从而让更多权重最终为零。weight-elimination regularizer的表达式如下：</p><p>$$\sum\frac{(w_{ij}^{(l)})^2}{1+(w_{ij}^{(l)})^2}$$</p><p><img src="http://img.blog.csdn.net/20170801215651811?" alt="这里写图片描述"></p><p>除了weight-elimination regularizer之外，还有另外一个很有效的regularization的方法，就是Early Stopping。简而言之，就是神经网络训练的次数t不能太多。因为，t太大的时候，相当于给模型寻找最优值更多的可能性，模型更复杂，VC Dimension增大，可能会overfitting。而t不太大时，能有效减少VC Dimension，降低模型复杂度，从而起到regularization的效果。$E_{in}$和$E_{test}$随训练次数t的关系如下图右下角所示：</p><p><img src="http://img.blog.csdn.net/20170801220409481?" alt="这里写图片描述"></p><p>那么，如何选择最佳的训练次数t呢？可以使用validation进行验证选择。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Neural Network模型。首先，我们通过使用一层甚至多层的perceptrons来获得更复杂的非线性模型。神经网络的每个神经元都相当于一个Neural Network Hypothesis，训练的本质就是在每一层网络上进行pattern extraction，找到最合适的权重$w_{ij}^{(l)}$，最终得到最佳的G。本课程以regression模型为例，最终的G是线性模型，而中间各层均采用tanh函数作为transform function。计算权重$w_{ij}^{(l)}$的方法就是采用GD或者SGD，通过Backpropagation算法，不断更新优化权重值，最终使得$E_{in}(w)$最小化，即完成了整个神经网络的训练过程。最后，我们提到了神经网络的可以使用一些regularization来防止模型过拟合。这些方法包括随机选择较小的权重初始值，使用weight-elimination regularizer或者early stopping等。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170731201555148?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记11 -- Gradient Boosted Decision Tree</title>
    <link href="https://redstonewill.github.io/2018/03/18/28/"/>
    <id>https://redstonewill.github.io/2018/03/18/28/</id>
    <published>2018-03-18T06:13:28.000Z</published>
    <updated>2018-03-18T06:20:07.897Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170727224137764?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Random Forest算法模型。Random Forest就是通过bagging的方式将许多不同的decision tree组合起来。除此之外，在decision tree中加入了各种随机性和多样性，比如不同特征的线性组合等。RF还可以使用OOB样本进行self-validation，而且可以通过permutation test进行feature selection。本节课将使用Adaptive Boosting的方法来研究decision tree的一些算法和模型。</p><h3 id="Adaptive-Boosted-Decision-Tree"><a href="#Adaptive-Boosted-Decision-Tree" class="headerlink" title="Adaptive Boosted Decision Tree"></a>Adaptive Boosted Decision Tree</h3><p>Random Forest的算法流程我们上节课也详细介绍过，就是先通过bootstrapping“复制”原样本集D，得到新的样本集D’；然后对每个D’进行训练得到不同的decision tree和对应的$g_t$；最后再将所有的$g_t$通过uniform的形式组合起来，即以投票的方式得到G。这里采用的Bagging的方式，也就是把每个$g_t$的预测值直接相加。现在，如果将Bagging替换成AdaBoost，处理方式有些不同。首先每轮bootstrap得到的D’中每个样本会赋予不同的权重$u^{(t)}$；然后在每个decision tree中，利用这些权重训练得到最好的$g_t$；最后得出每个$g_t$所占的权重，线性组合得到G。这种模型称为AdaBoost-D Tree。</p><p><img src="http://img.blog.csdn.net/20170727224137764?" alt="这里写图片描述"></p><p>但是在AdaBoost-DTree中需要注意的一点是每个样本的权重$u^{(t)}$。我们知道，在Adaptive Boosting中进行了bootstrap操作，$u^{(t)}$表示D中每个样本在D’中出现的次数。但是在决策树模型中，例如C&amp;RT算法中并没有引入$u^{(t)}$。那么，如何在决策树中引入这些权重$u^{(t)}$来得到不同的$g_t$而又不改变原来的决策树算法呢？</p><p>在Adaptive Boosting中，我们使用了weighted algorithm，形如：</p><p>$$E_{in}^u(h)=\frac1N\sum_{n=1}^Nu_n\cdot err(y_n,h(x_n))$$</p><p>每个犯错误的样本点乘以相应的权重，求和再平均，最终得到了$E_{in}^u(h)$。如果在决策树中使用这种方法，将当前分支下犯错误的点赋予权重，每层分支都这样做，会比较复杂，不易求解。为了简化运算，保持决策树算法本身的稳定性和封闭性，我们可以把决策树算法当成一个黑盒子，即不改变其结构，不对算法本身进行修改，而从数据来源D’上做一些处理。按照这种思想，我们来看权重u实际上表示该样本在bootstrap中出现的次数，反映了它出现的概率。那么可以根据u值，对原样本集D进行一次重新的随机sampling，也就是带权重的随机抽样。sampling之后，会得到一个新的D’，D’中每个样本出现的几率与它权重u所占的比例应该是差不多接近的。因此，使用带权重的sampling操作，得到了新的样本数据集D’，可以直接代入决策树进行训练，从而无需改变决策树算法结构。sampling可看成是bootstrap的反操作，这种对数据本身进行修改而不更改算法结构的方法非常重要！</p><p><img src="http://img.blog.csdn.net/20170728082507735?" alt="这里写图片描述"></p><p>所以，AdaBoost-DTree结合了AdaBoost和DTree，但是做了一点小小的改变，就是使用sampling替代权重$u^{(t)}$，效果是相同的。</p><p><img src="http://img.blog.csdn.net/20170728083007219?" alt="这里写图片描述"></p><p>上面我们通过使用sampling，将不同的样本集代入决策树中，得到不同的$g_t$。除此之外，我们还要确定每个$g_t$所占的权重$\alpha_t$。之前我们在AdaBoost中已经介绍过，首先算出每个$g_t$的错误率$\epsilon_t$，然后计算权重：</p><p>$$\alpha_t=ln\ \diamond_t=ln \sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$$</p><p>如果现在有一棵完全长成的树（fully grown tree），由所有的样本$x_n$训练得到。若每个样本都不相同的话，一刀刀切割分支，直到所有的$x_n$都被完全分开。这时候，$E_{in}(g_t)=0$，加权的$E_{in}^u(g_t)=0$而且$\epsilon_t$也为0，从而得到权重$\alpha_t=\infty$。$\alpha_t=\infty$表示该$g_t$所占的权重无限大，相当于它一个就决定了G结构，是一种autocracy，而其它的$g_t$对G没有影响。</p><p><img src="http://img.blog.csdn.net/20170728091659613?" alt="这里写图片描述"></p><p>显然$\alpha_t=\infty$不是我们想看到的，因为autocracy总是不好的，我们希望使用aggregation将不同的$g_t$结合起来，发挥集体智慧来得到最好的模型G。首先，我们来看一下什么原因造成了$\alpha_t=\infty$。有两个原因：一个是使用了所有的样本$x_n$进行训练；一个是树的分支过多，fully grown。针对这两个原因，我们可以对树做一些修剪（pruned），比如只使用一部分样本，这在sampling的操作中已经起到这类作用，因为必然有些样本没有被采样到。除此之外，我们还可以限制树的高度，让分支不要那么多，从而避免树fully grown。</p><p><img src="http://img.blog.csdn.net/20170728092206113?" alt="这里写图片描述"></p><p>因此，AdaBoost-DTree使用的是pruned DTree，也就是说将这些预测效果较弱的树结合起来，得到最好的G，避免出现autocracy。</p><p><img src="http://img.blog.csdn.net/20170728100826215?" alt="这里写图片描述"></p><p>刚才我们说了可以限制树的高度，那索性将树的高度限制到最低，即只有1层高的时候，有什么特性呢？当树高为1的时候，整棵树只有两个分支，切割一次即可。如果impurity是binary classification error的话，那么此时的AdaBoost-DTree就跟AdaBoost-Stump没什么两样。也就是说AdaBoost-Stump是AdaBoost-DTree的一种特殊情况。</p><p><img src="http://img.blog.csdn.net/20170728101843620?" alt="这里写图片描述"></p><p>值得一提是，如果树高为1时，通常较难遇到$\epsilon_t=0$的情况，且一般不采用sampling的操作，而是直接将权重u代入到算法中。这是因为此时的AdaBoost-DTree就相当于是AdaBoost-Stump，而AdaBoost-Stump就是直接使用u来优化模型的。</p><h3 id="Optimization-View-of-AdaBoost"><a href="#Optimization-View-of-AdaBoost" class="headerlink" title="Optimization View of AdaBoost"></a>Optimization View of AdaBoost</h3><p>接下来，我们继续将继续探讨AdaBoost算法的一些奥妙之处。我们知道AdaBoost中的权重的迭代计算如下所示：</p><p><img src="http://img.blog.csdn.net/20170728112800984?" alt="这里写图片描述"></p><p>之前对于incorrect样本和correct样本，$u_n^{(t+1)}$的表达式不同。现在，把两种情况结合起来，将$u_n^{(t+1)}$写成一种简化的形式：</p><p>$$u_n^{(t+1)}=u_n^{(t)}\cdot \diamond_t^{-y_ng_t(x_n)}=u_n^{(t)}\cdot exp(-y_n\alpha_tg_t(x_n))$$</p><p>其中，对于incorrect样本，$y_ng_t(x_n)&lt;0$，对于correct样本，$y_ng_t(x_n)&gt;0$。从上式可以看出，$u_n^{(t+1)}$由$u_n^{(t)}$与某个常数相乘得到。所以，最后一轮更新的$u_n^{(T+1)}$可以写成$u_n^{(1)}$的级联形式，我们之前令$u_n^{(1)}=\frac1N$，则有如下推导：</p><p>$$u_n^{(T+1)}=u_n^{(1)}\cdot \prod_{t=1}^Texp(-y_n\alpha_tg_t(x_n))=\frac1N\cdot exp(-y_n\sum_{t=1}^T\alpha_tg_t(x_n))$$</p><p>上式中$\sum_{t=1}^T\alpha_tg_t(x_n)$被称为voting score，最终的模型$G=sign(\sum_{t=1}^T\alpha_tg_t(x_n))$。可以看出，在AdaBoost中，$u_n^{(T+1)}$与$exp(-y_n(voting\ score\ on\ x_n))$成正比。</p><p><img src="http://img.blog.csdn.net/20170728152407683?" alt="这里写图片描述"></p><p>接下来我们继续看一下voting score中蕴含了哪些内容。如下图所示，voting score由许多$g_t(x_n)$乘以各自的系数$\alpha_t$线性组合而成。从另外一个角度来看，我们可以把$g_t(x_n)$看成是对$x_n$的特征转换$\phi_i(x_n)$，$\alpha_t$就是线性模型中的权重$w_i$。看到这里，我们回忆起之前SVM中，w与$\phi (x_n)$的乘积再除以w的长度就是margin，即点到边界的距离。另外，乘积项再与$y_n$相乘，表示点的位置是在正确的那一侧还是错误的那一侧。所以，回过头来，这里的voting score实际上可以看成是没有正规化（没有除以w的长度）的距离，即可以看成是该点到分类边界距离的一种衡量。从效果上说，距离越大越好，也就是说voting score要尽可能大一些。</p><p><img src="http://img.blog.csdn.net/20170728152804003?" alt="这里写图片描述"></p><p>我们再来看，若voting score与$y_n$相乘，则表示一个有对错之分的距离。也就是说，如果二者相乘是负数，则表示该点在错误的一边，分类错误；如果二者相乘是正数，则表示该点在正确的一边，分类正确。所以，我们算法的目的就是让$y_n$与voting score的乘积是正的，而且越大越好。那么在刚刚推导的$u_n^{(T+1)}$中，得到$exp(-y_n(voting\ score))$越小越好，从而得到$u_n^{(T+1)}$越小越好。也就是说，如果voting score表现不错，与$y_n$的乘积越大的话，那么相应的$u_n^{(T+1)}$应该是最小的。</p><p><img src="http://img.blog.csdn.net/20170728160416115?" alt="这里写图片描述"></p><p>那么在AdaBoost中，随着每轮学习的进行，每个样本的$u_n^{(t)}$是逐渐减小的，直到$u_n^{(T+1)}$最小。以上是从单个样本点来看的。总体来看，所有样本的$u_n^{(T+1)}$之和应该也是最小的。我们的目标就是在最后一轮（T+1）学习后，让所有样本的$u_n^{(T+1)}$之和尽可能地小。$u_n^{(T+1)}$之和表示为如下形式：</p><p><img src="http://img.blog.csdn.net/20170728164526912?" alt="这里写图片描述"></p><p>上式中，$\sum_{t=1}^T\alpha_tg_t(x_n)$被称为linear score，用s表示。对于0/1 error：若ys&lt;0，则$err_{0/1}=1$；若ys&gt;=0，则$err_{0/1}=0$。如下图右边黑色折线所示。对于上式中提到的指数error，即$\hat{err}_{ADA}(s,y)=exp(-ys)$，随着ys的增加，error单调下降，且始终落在0/1 error折线的上面。如下图右边蓝色曲线所示。很明显，$\hat{err}_{ADA}(s,y)$可以看成是0/1 error的上界。所以，我们可以使用$\hat{err}_{ADA}(s,y)$来替代0/1 error，能达到同样的效果。从这点来说，$\sum_{n=1}^Nu_n^{(T+1)}$可以看成是一种error measure，而我们的目标就是让其最小化，求出最小值时对应的各个$\alpha_t$和$g_t(x_n)$。</p><p><img src="http://img.blog.csdn.net/20170728164931978?" alt="这里写图片描述"></p><p>下面我们来研究如何让$\sum_{n=1}^Nu_n^{(T+1)}$取得最小值，思考是否能用梯度下降（gradient descent）的方法来进行求解。我们之前介绍过gradient descent的核心是在某点处做一阶泰勒展开：</p><p><img src="http://img.blog.csdn.net/20170728204308067?" alt="这里写图片描述"></p><p>其中，$w_t$是泰勒展开的位置，v是所要求的下降的最好方向，它是梯度$\nabla E_{in}(w_t)$的反方向，而$\eta$是每次前进的步长。则每次沿着当前梯度的反方向走一小步，就会不断逼近谷底（最小值）。这就是梯度下降算法所做的事情。</p><p>现在，我们对$\check{E}_{ADA}$做梯度下降算法处理，区别是这里的方向是一个函数$g_t$，而不是一个向量$w_t$。其实，函数和向量的唯一区别就是一个下标是连续的，另一个下标是离散的，二者在梯度下降算法应用上并没有大的区别。因此，按照梯度下降算法的展开式，做出如下推导：</p><p><img src="http://img.blog.csdn.net/20170728211939429?" alt="这里写图片描述"></p><p>上式中，$h(x_n)$表示当前的方向，它是一个矩，$\eta$是沿着当前方向前进的步长。我们要求出这样的$h(x_n)$和$\eta$，使得$\check{E}_{ADA}$是在不断减小的。当$\check{E}_{ADA}$取得最小值的时候，那么所有的方向即最佳的$h(x_n)$和$\eta$就都解出来了。上述推导使用了在$-y_n\eta h(x_n)=0$处的一阶泰勒展开近似。这样经过推导之后，$\check{E}_{ADA}$被分解为两个部分，一个是前N个u之和$\sum_{n=1}^Nu_n^{(t)}$，也就是当前所有的$E_{in}$之和；另外一个是包含下一步前进的方向$h(x_n)$和步进长度$\eta$的项$-\eta\sum_{n=1}^Nu_n^{(t)}y_nh(x_n)$。$\check{E}_{ADA}$的这种形式与gradient descent的形式基本是一致的。</p><p>那么接下来，如果要最小化$\check{E}_{ADA}$的话，就要让第二项$-\eta\sum_{n=1}^Nu_n^{(t)}y_nh(x_n)$越小越好。则我们的目标就是找到一个好的$h(x_n)$（即好的方向）来最小化$\sum_{n=1}^Nu_n^{(t)}(-y_nh(x_n))$，此时先忽略步进长度$\eta$。</p><p><img src="http://img.blog.csdn.net/20170729142004595?" alt="这里写图片描述"></p><p>对于binary classification，$y_n$和$h(x_n)$均限定取值-1或+1两种。我们对$\sum_{n=1}^Nu_n^{(t)}(-y_nh(x_n))$做一些推导和平移运算：</p><p><img src="http://img.blog.csdn.net/20170729143137084?" alt="这里写图片描述"></p><p>最终$\sum_{n=1}^Nu_n^{(t)}(-y_nh(x_n))$化简为两项组成，一项是$-\sum_{n=1}^Nu_n^{(t)}$；另一项是$2E_{in}^{u(t)}(h)\cdot N$。则最小化$\sum_{n=1}^Nu_n^{(t)}(-y_nh(x_n))$就转化为最小化$E_{in}^{u(t)}(h)$。要让$E_{in}^{u(t)}(h)$最小化，正是由AdaBoost中的base algorithm所做的事情。所以说，AdaBoost中的base algorithm正好帮我们找到了梯度下降中下一步最好的函数方向。</p><p><img src="http://img.blog.csdn.net/20170729144354065?" alt="这里写图片描述"></p><p>以上就是从数学上，从gradient descent角度验证了AdaBoost中使用base algorithm得到的$g_t$就是让$\check{E}_{ADA}$减小的方向，只不过这个方向是一个函数而不是向量。</p><p>在解决了方向问题后，我们需要考虑步进长度$\eta$如何选取。方法是在确定方向$g_t$后，选取合适的$\eta$，使$\check{E}_{ADA}$取得最小值。也就是说，把$\check{E}_{ADA}$看成是步进长度$\eta$的函数，目标是找到$\check{E}_{ADA}$最小化时对应的$\eta$值。</p><p><img src="http://img.blog.csdn.net/20170729150613470?" alt="这里写图片描述"></p><p>目的是找到在最佳方向上的最大步进长度，也就是steepest decent。我们先把$\check{E}_{ADA}$表达式写下来：</p><p>$$\check{E}_{ADA}=\sum_{n=1}^Nu_n^{(t)}exp(-y_n\eta g_t(x_n))$$</p><p>上式中，有两种情况需要考虑：</p><ul><li><p><strong>$y_n=g_t(x_n)$：$u_n^{(t)}exp(-\eta)$  correct</strong></p></li><li><p><strong>$y_n\neq g_t(x_n)$：$u_n^{(t)}exp(+\eta)$ incorrect</strong></p></li></ul><p>经过推导，可得：</p><p>$$\check{E}_{ADA}=(\sum_{n=1}^Nu_n^{(t)})\cdot ((1-\epsilon_t)exp(-\eta)+\epsilon_t\ exp(+\eta))$$</p><p><img src="http://img.blog.csdn.net/20170729153724406?" alt="这里写图片描述"></p><p>然后对$\eta$求导，令$\frac{\partial \check{E}_{ADA}}{\partial \eta}=0$，得：</p><p>$$\eta_t=ln\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}=\alpha_t$$</p><p>由此看出，最大的步进长度就是$\alpha_t$，即AdaBoost中计算$g_t$所占的权重。所以，AdaBoost算法所做的其实是在gradient descent上找到下降最快的方向和最大的步进长度。这里的方向就是$g_t$，它是一个函数，而步进长度就是$\alpha_t$。也就是说，在AdaBoost中确定$g_t$和$\alpha_t$的过程就相当于在gradient descent上寻找最快的下降方向和最大的步进长度。</p><h3 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h3><p>前面我们从gradient descent的角度来重新介绍了AdaBoost的最优化求解方法。整个过程可以概括为：</p><p><img src="http://img.blog.csdn.net/20170729163241034?" alt="这里写图片描述"></p><p>以上是针对binary classification问题。如果往更一般的情况进行推广，对于不同的error function，比如logistic error function或者regression中的squared error function，那么这种做法是否仍然有效呢？这种情况下的GradientBoost可以写成如下形式：</p><p><img src="http://img.blog.csdn.net/20170729163848352?" alt="这里写图片描述"></p><p>仍然按照gradient descent的思想，上式中，$h(x_n)$是下一步前进的方向，$\eta$是步进长度。此时的error function不是前面所讲的exp了，而是任意的一种error function。因此，对应的hypothesis也不再是binary classification，最常用的是实数输出的hypothesis，例如regression。最终的目标也是求解最佳的前进方向$h(x_n)$和最快的步进长度$\eta$。</p><p><img src="http://img.blog.csdn.net/20170729164531417?" alt="这里写图片描述"></p><p>接下来，我们就来看看如何求解regression的GradientBoost问题。它的表达式如下所示：</p><p><img src="http://img.blog.csdn.net/20170729171758072?" alt="这里写图片描述"></p><p>利用梯度下降的思想，我们把上式进行一阶泰勒展开，写成梯度的形式：</p><p><img src="http://img.blog.csdn.net/20170729193033365?" alt="这里写图片描述"></p><p>上式中，由于regression的error function是squared的，所以，对s的导数就是$2(s_n-y_n)$。其中标注灰色的部分表示常数，对最小化求解并没有影响，所以可以忽略。很明显，要使上式最小化，只要令$h(x_n)$是梯度$2(s_n-y_n)$的反方向就行了，即$h(x_n)=-2(s_n-y_n)$。但是直接这样赋值，并没有对$h(x_n)$的大小进行限制，一般不直接利用这个关系求出$h(x_n)$。</p><p><img src="http://img.blog.csdn.net/20170729194745405?" alt="这里写图片描述"></p><p>实际上$h(x_n)$的大小并不重要，因为有步进长度$\eta$。那么，我们上面的最小化问题中需要对$h(x_n)$的大小做些限制。限制$h(x_n)$的一种简单做法是把$h(x_n)$的大小当成一个惩罚项（$h^2(x_n)$）添加到上面的最小化问题中，这种做法与regularization类似。如下图所示，经过推导和整理，忽略常数项，我们得到最关心的式子是：</p><p>$$min\ \sum_{n=1}^N((h(x_n)-(y_n-s_n))^2)$$</p><p>上式是一个完全平方项之和，$y_n-s_n$表示当前第n个样本真实值和预测值的差，称之为余数。余数表示当前预测能够做到的效果与真实值的差值是多少。那么，如果我们想要让上式最小化，求出对应的$h(x_n)$的话，只要让$h(x_n)$尽可能地接近余数$y_n-s_n$即可。在平方误差上尽可能接近其实很简单，就是使用regression的方法，对所有N个点$(x_n,y_n-s_n)$做squared-error的regression，得到的回归方程就是我们要求的$g_t(x_n)$。</p><p><img src="http://img.blog.csdn.net/20170729212101227?" alt="这里写图片描述"></p><p>以上就是使用GradientBoost的思想来解决regression问题的方法，其中应用了一个非常重要的概念，就是余数$y_n-s_n$。根据这些余数做regression，得到好的矩$g_t(x_n)$，方向函数$g_t(x_n)$也就是由余数决定的。</p><p><img src="http://img.blog.csdn.net/20170729212214760?" alt="这里写图片描述"></p><p>在求出最好的方向函数$g_t(x_n)$之后，就要来求相应的步进长度$\eta$。表达式如下：</p><p><img src="http://img.blog.csdn.net/20170729212637843?" alt="这里写图片描述"></p><p>同样，对上式进行推导和化简，得到如下表达式：</p><p><img src="http://img.blog.csdn.net/20170729213112322?" alt="这里写图片描述"></p><p>上式中也包含了余数$y_n-s_n$，其中$g_t(x_n)$可以看成是$x_n$的特征转换，是已知量。那么，如果我们想要让上式最小化，求出对应的$\eta$的话，只要让$\eta g_t(x_n)$尽可能地接近余数$y_n-s_n$即可。显然，这也是一个regression问题，而且是一个很简单的形如y=ax的线性回归，只有一个未知数$\eta$。只要对所有N个点$(\eta g_t(x_n),y_n-s_n)$做squared-error的linear regression，利用梯度下降算法就能得到最佳的$\eta$。</p><p>将上述这些概念合并到一起，我们就得到了一个最终的演算法Gradient Boosted Decision Tree(GBDT)。可能有人会问，我们刚才一直没有说到Decison Tree，只是讲到了GradientBoost啊？下面我们来看看Decison Tree究竟是在哪出现并使用的。其实刚刚我们在计算方向函数$g_t$的时候，是对所有N个点$(x_n,y_n-s_n)$做squared-error的regression。那么这个回归算法就可以是决策树C&amp;RT模型（决策树也可以用来做regression）。这样，就引入了Decision Tree，并将GradientBoost和Decision Tree结合起来，构成了真正的GBDT算法。GBDT算法的基本流程图如下所示：</p><p><img src="http://img.blog.csdn.net/20170729215009725?" alt="这里写图片描述"></p><p>值得注意的是，$s_n$的初始值一般均设为0，即$s_1=s_2=\cdots =s_N=0$。每轮迭代中，方向函数$g_t$通过C&amp;RT算法做regression，进行求解；步进长度$\eta$通过简单的单参数线性回归进行求解；然后每轮更新$s_n$的值，即$s_n\leftarrow s_n+\alpha_tg_t(x_n)$。T轮迭代结束后，最终得到$G(x)=\sum_{t=1}^T\alpha_tg_t(x)$。</p><p>值得一提的是，本节课第一部分介绍的AdaBoost-DTree是解决binary classification问题，而此处介绍的GBDT是解决regression问题。二者具有一定的相似性，可以说GBDT就是AdaBoost-DTree的regression版本。</p><p><img src="http://img.blog.csdn.net/20170729220455297?" alt="这里写图片描述"></p><h3 id="Summary-ofAggregation-Models"><a href="#Summary-ofAggregation-Models" class="headerlink" title="Summary ofAggregation Models"></a>Summary ofAggregation Models</h3><p>从机器学习技法课程的第7节课笔记到现在的第11节课笔记，我们已经介绍完所有的aggregation模型了。接下来，我们将对这些内容进行一个简单的总结和概括。</p><p>首先，我们介绍了blending。blending就是将所有已知的$g_t$ aggregate结合起来，发挥集体的智慧得到G。值得注意的一点是这里的$g_t$都是已知的。blending通常有三种形式：</p><ul><li><p><strong>uniform：简单地计算所有$g_t$的平均值</strong></p></li><li><p><strong>non-uniform：所有$g_t$的线性组合</strong></p></li><li><p><strong>conditional：所有$g_t$的非线性组合</strong></p></li></ul><p>其中，uniform采用投票、求平均的形式更注重稳定性；而non-uniform和conditional追求的更复杂准确的模型，但存在过拟合的危险。</p><p><img src="http://img.blog.csdn.net/20170730095242485?" alt="这里写图片描述"></p><p>刚才讲的blending是建立在所有$g_t$已知的情况。那如果所有$g_t$未知的情况，对应的就是learning模型，做法就是一边学$g_t$，一边将它们结合起来。learning通常也有三种形式（与blending的三种形式一一对应）：</p><ul><li><p><strong>Bagging：通过bootstrap方法，得到不同$g_t$，计算所有$g_t$的平均值</strong></p></li><li><p><strong>AdaBoost：通过bootstrap方法，得到不同$g_t$，所有$g_t$的线性组合</strong></p></li><li><p><strong>Decision Tree：通过数据分割的形式得到不同的$g_t$，所有$g_t$的非线性组合</strong></p></li></ul><p>然后，本节课我们将AdaBoost延伸到另一个模型GradientBoost。对于regression问题，GradientBoost通过residual fitting的方式得到最佳的方向函数$g_t$和步进长度$\eta$。</p><p><img src="http://img.blog.csdn.net/20170730103158989?" alt="这里写图片描述"></p><p>除了这些基本的aggregation模型之外，我们还可以把某些模型结合起来得到新的aggregation模型。例如，Bagging与Decision Tree结合起来组成了Random Forest。Random Forest中的Decision Tree是比较“茂盛”的树，即每个树的$g_t$都比较强一些。AdaBoost与Decision Tree结合组成了AdaBoost-DTree。AdaBoost-DTree的Decision Tree是比较“矮弱”的树，即每个树的$g_t$都比较弱一些，由AdaBoost将所有弱弱的树结合起来，让综合能力更强。同样，GradientBoost与Decision Tree结合就构成了经典的算法GBDT。</p><p><img src="http://img.blog.csdn.net/20170730105341073?" alt="这里写图片描述"></p><p>Aggregation的核心是将所有的$g_t$结合起来，融合到一起，即集体智慧的思想。这种做法之所以能得到很好的模型G，是因为aggregation具有两个方面的优点：cure underfitting和cure overfitting。</p><p>第一，aggregation models有助于防止欠拟合（underfitting）。它把所有比较弱的$g_t$结合起来，利用集体智慧来获得比较好的模型G。aggregation就相当于是feature transform，来获得复杂的学习模型。</p><p>第二，aggregation models有助于防止过拟合（overfitting）。它把所有$g_t$进行组合，容易得到一个比较中庸的模型，类似于SVM的large margin一样的效果，从而避免一些极端情况包括过拟合的发生。从这个角度来说，aggregation起到了regularization的效果。</p><p>由于aggregation具有这两个方面的优点，所以在实际应用中aggregation models都有很好的表现。</p><p><img src="http://img.blog.csdn.net/20170730125759015?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Gradient Boosted Decision Tree。首先讲如何将AdaBoost与Decision Tree结合起来，即通过sampling和pruning的方法得到AdaBoost-D Tree模型。然后，我们从optimization的角度来看AdaBoost，找到好的hypothesis也就是找到一个好的方向，找到权重$\alpha$也就是找到合适的步进长度。接着，我们从binary classification的0/1 error推广到其它的error function，从Gradient Boosting角度推导了regression的squared error形式。Gradient Boosting其实就是不断迭代，做residual fitting。并将其与Decision Tree算法结合，得到了经典的GBDT算法。最后，我们将所有的aggregation models做了总结和概括，这些模型有的能防止欠拟合有的能防止过拟合，应用十分广泛。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170727224137764?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记10 -- Random Forest</title>
    <link href="https://redstonewill.github.io/2018/03/18/27/"/>
    <id>https://redstonewill.github.io/2018/03/18/27/</id>
    <published>2018-03-18T06:08:43.000Z</published>
    <updated>2018-03-18T06:10:44.505Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170725105700489?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Decision Tree模型。Decision Tree算法的核心是通过递归的方式，将数据集不断进行切割，得到子分支，最终形成数的结构。C&amp;RT算法是决策树比较简单和常用的一种算法，其切割的标准是根据纯度来进行，每次切割都是为了让分支内部纯度最大。最终，决策树不同的分支得到不同的$g_t(x)$（即树的叶子，C&amp;RT算法中，$g_t(x)$是常数）。本节课将介绍随机森林（Random Forest）算法，它是我们之前介绍的Bagging和上节课介绍的Decision Tree的结合。</p><h3 id="Random-Forest-Algorithm"><a href="#Random-Forest-Algorithm" class="headerlink" title="Random Forest Algorithm"></a>Random Forest Algorithm</h3><p>首先我们来复习一下之前介绍过的两个机器学习模型：Bagging和Decision Tree。Bagging是通过bootstrap的方式，从原始的数据集D中得到新的$\hat{D}$；然后再使用一些base algorithm对每个$\hat{D}$都得到相应的$g_t$；最后将所有的$g_t$通过投票uniform的形式组合成一个G，G即为我们最终得到的模型。Decision Tree是通过递归形式，利用分支条件，将原始数据集D切割成一个个子树结构，长成一棵完整的树形结构。Decision Tree最终得到的G(x)是由相应的分支条件b(x)和分支树$G_c(x)$递归组成。</p><p><img src="http://img.blog.csdn.net/20170725105700489?" alt="这里写图片描述"></p><p>Bagging和Decison Tree算法各自有一个很重要的特点。Bagging具有减少不同$g_t$的方差variance的特点。这是因为Bagging采用投票的形式，将所有$g_t$uniform结合起来，起到了求平均的作用，从而降低variance。而Decision Tree具有增大不同$g_t$的方差variance的特点。这是因为Decision Tree每次切割的方式不同，而且分支包含的样本数在逐渐减少，所以它对不同的资料D会比较敏感一些，从而不同的D会得到比较大的variance。</p><p>所以说，Bagging能减小variance，而Decision Tree能增大variance。如果把两者结合起来，能否发挥各自的优势，起到优势互补的作用呢？这就是我们接下来将要讨论的aggregation of aggregation，即使用Bagging的方式把众多的Decision Tree进行uniform结合起来。这种算法就叫做随机森林（Random Forest），它将完全长成的C&amp;RT决策树通过bagging的形式结合起来，最终得到一个庞大的决策模型。</p><p><img src="http://img.blog.csdn.net/20170725133154097?" alt="这里写图片描述"></p><p>Random Forest算法流程图如下所示：</p><p><img src="http://img.blog.csdn.net/20170725134328625?" alt="这里写图片描述"></p><p>Random Forest算法的优点主要有三个。第一，不同决策树可以由不同主机并行训练生成，效率很高；第二，随机森林算法继承了C&amp;RT的优点；第三，将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题。</p><p><img src="http://img.blog.csdn.net/20170725135521243?" alt="这里写图片描述"></p><p>以上是基本的Random Forest算法，我们再来看一下如何让Random Forest中决策树的结构更有多样性。Bagging中，通过bootstrap的方法得到不同于D的D’，使用这些随机抽取的资料得到不同的$g_t$。除了随机抽取资料获得不同$g_t$的方式之外，还有另外一种方法，就是随机抽取一部分特征。例如，原来有100个特征，现在只从中随机选取30个来构成决策树，那么每一轮得到的树都由不同的30个特征构成，每棵树都不一样。假设原来样本维度是d，则只选择其中的d’（d’小于d）个维度来建立决策树结构。这类似是一种从d维到d’维的特征转换，相当于是从高维到低维的投影，也就是说d’维z空间其实就是d维x空间的一个随机子空间（subspace）。通常情况下，d’远小于d，从而保证算法更有效率。Random Forest算法的作者建议在构建C&amp;RT每个分支b(x)的时候，都可以重新选择子特征来训练，从而得到更具有多样性的决策树。</p><p><img src="http://img.blog.csdn.net/20170725143051066?" alt="这里写图片描述"></p><p>所以说，这种增强的Random Forest算法增加了random-subspace。</p><p><img src="http://img.blog.csdn.net/20170725143437035?" alt="这里写图片描述"></p><p>上面我们讲的是随机抽取特征，除此之外，还可以将现有的特征x，通过数组p进行线性组合，来保持多样性：</p><p>$$\phi_i(x)=p_i^Tx$$</p><p>这种方法使每次分支得到的不再是单一的子特征集合，而是子特征的线性组合（权重不为1）。好比在二维平面上不止得到水平线和垂直线，也能得到各种斜线。这种做法使子特征选择更加多样性。值得注意的是，不同分支i下的$p_i$是不同的，而且向量$p_i$中大部分元素为零，因为我们选择的只是一部分特征，这是一种低维映射。</p><p><img src="http://img.blog.csdn.net/20170725152429905?" alt="这里写图片描述"></p><p>所以，这里的Random Forest算法又有增强，由原来的random-subspace变成了random-combination。顺便提一下，这里的random-combination类似于perceptron模型。</p><p><img src="http://img.blog.csdn.net/20170725152726569?" alt="这里写图片描述"></p><h3 id="Out-Of-Bag-Estimate"><a href="#Out-Of-Bag-Estimate" class="headerlink" title="Out-Of-Bag Estimate"></a>Out-Of-Bag Estimate</h3><p>上一部分我们已经介绍了Random Forest算法，而Random Forest算法重要的一点就是Bagging。接下来将继续探讨bagging中的bootstrap机制到底蕴含了哪些可以为我们所用的东西。</p><p>通过bootstrap得到新的样本集D’，再由D’训练不同的$g_t$。我们知道D’中包含了原样本集D中的一些样本，但也有些样本没有涵盖进去。如下表所示，不同的$g_t$下，红色的<em>表示在$\hat D_t$中没有这些样本。例如对$g_1$来说，$(x_2,y_2)$和$(x_3,y_4)$没有包含进去，对$g_2$来说，$(x_1,y_1)$和$(x_2,y_2)$没有包含进去，等等。每个$g_t$中，红色</em>表示的样本被称为out-of-bag(OOB) example。</p><p><img src="http://img.blog.csdn.net/20170725211414590?" alt="这里写图片描述"></p><p>首先，我们来计算OOB样本到底有多少。假设bootstrap的数量N’=N，那么某个样本$(x_n,y_n)$是OOB的概率是：</p><p>$$(1-\frac1N)^N=\frac{1}{(\frac{N}{N-1})^N}=\frac{1}{(1+\frac{1}{N-1})^N}\approx \frac1e$$</p><p>其中，e是自然对数，N是原样本集的数量。由上述推导可得，每个$g_t$中，OOB数目大约是$\frac1eN$，即大约有三分之一的样本没有在bootstrap中被抽到。</p><p>然后，我们将OOB与之前介绍的Validation进行对比：</p><p><img src="http://img.blog.csdn.net/20170725215140036?" alt="这里写图片描述"></p><p>在Validation表格中，蓝色的$D_{train}$用来得到不同的$g_m^-$，而红色的$D_{val}$用来验证各自的$g_m^-$。$D_{train}$与$D_{val}$没有交集，一般$D_{train}$是$D_{val}$的数倍关系。再看左边的OOB表格，之前我们也介绍过，蓝色的部分用来得到不同的$g_t$，而红色的部分是OOB样本。而我们刚刚也推导过，红色部分大约占N的$\frac1e$。通过两个表格的比较，我们发现OOB样本类似于$D_{val}$，那么是否能使用OOB样本来验证$g_t$的好坏呢？答案是肯定的。但是，通常我们并不需要对单个$g_t$进行验证。因为我们更关心的是由许多$g_t$组合成的G，即使$g_t$表现不太好，只要G表现足够好就行了。那么问题就转化成了如何使用OOB来验证G的好坏。方法是先看每一个样本$(x_n,y_n)$是哪些$g_t$的OOB资料，然后计算其在这些$g_t$上的表现，最后将所有样本的表现求平均即可。例如，样本$(x_N,y_N)$是$g_2$，$g_3$，$g_T$的OOB，则可以计算$(x_N,y_N)$在$G_N^-(x)$上的表现为：</p><p>$$G_N^-(x)=average(g_2,g_3,g_T)$$</p><p>这种做法我们并不陌生，就像是我们之前介绍过的Leave-One-Out Cross Validation，每次只对一个样本进行$g^-$的验证一样，只不过这里选择的是每个样本是哪些$g_t$的OOB，然后再分别进行$G_n^-(x)$的验证。每个样本都当成验证资料一次（与留一法相同），最后计算所有样本的平均表现：</p><p>$$E_{oob}(G)=\frac1N\sum_{n=1}^Nerr(y_n,G_n^-(x_n))$$</p><p>$E_{oob}(G)$估算的就是G的表现好坏。我们把$E_{oob}$称为bagging或者Random Forest的self-validation。</p><p>这种self-validation相比于validation来说还有一个优点就是它不需要重复训练。如下图左边所示，在通过$D_{val}$选择到表现最好的$g_{m^<em>}^-$之后，还需要在$D_{train}$和$D_{val}$组成的所有样本集D上重新对该模型$g_{m^</em>}^-$训练一次，以得到最终的模型系数。但是self-validation在调整随机森林算法相关系数并得到最小的$E_{oob}$之后，就完成了整个模型的建立，无需重新训练模型。随机森林算法中，self-validation在衡量G的表现上通常相当准确。</p><p><img src="http://img.blog.csdn.net/20170726082341574?" alt="这里写图片描述"></p><h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><p>如果样本资料特征过多，假如有10000个特征，而我们只想从中选取300个特征，这时候就需要舍弃部分特征。通常来说，需要移除的特征分为两类：一类是冗余特征，即特征出现重复，例如“年龄”和“生日”；另一类是不相关特征，例如疾病预测的时候引入的“保险状况”。这种从d维特征到d’维特征的subset-transform $\Phi(x)$称为Feature Selection，最终使用这些d’维的特征进行模型训练。</p><p><img src="http://img.blog.csdn.net/20170726084338825?" alt="这里写图片描述"></p><p>特征选择的优点是：</p><ul><li><p><strong>提高效率，特征越少，模型越简单</strong></p></li><li><p><strong>正则化，防止特征过多出现过拟合</strong></p></li><li><p><strong>去除无关特征，保留相关性大的特征，解释性强</strong></p></li></ul><p>同时，特征选择的缺点是：</p><ul><li><p><strong>筛选特征的计算量较大</strong></p></li><li><p><strong>不同特征组合，也容易发生过拟合</strong></p></li><li><p><strong>容易选到无关特征，解释性差</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170726085416881?" alt="这里写图片描述"></p><p>值得一提的是，在decision tree中，我们使用的decision stump切割方式也是一种feature selection。</p><p>那么，如何对许多维特征进行筛选呢？我们可以通过计算出每个特征的重要性（即权重），然后再根据重要性的排序进行选择即可。</p><p><img src="http://img.blog.csdn.net/20170726092243523?" alt="这里写图片描述"></p><p>这种方法在线性模型中比较容易计算。因为线性模型的score是由每个特征经过加权求和而得到的，而加权系数的绝对值$|w_i|$正好代表了对应特征$x_i$的重要性为多少。$|w_i|$越大，表示对应特征$x_i$越重要，则该特征应该被选择。w的值可以通过对已有的数据集$(x_i,y_i)$建立线性模型而得到。</p><p><img src="http://img.blog.csdn.net/20170726093713158?" alt="这里写图片描述"></p><p>然而，对于非线性模型来说，因为不同特征可能是非线性交叉在一起的，所以计算每个特征的重要性就变得比较复杂和困难。例如，Random Forest就是一个非线性模型，接下来，我们将讨论如何在RF下进行特征选择。</p><p>RF中，特征选择的核心思想是random test。random test的做法是对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无。所以，通过比较某特征被随机值替代前后的表现，就能推断出该特征的权重和重要性。</p><p>那么random test中的随机值如何选择呢？通常有两种方法：一是使用uniform或者gaussian抽取随机值替换原特征；一是通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）。比较而言，第二种方法更加科学，保证了特征替代值与原特征的分布是近似的（只是重新洗牌而已）。这种方法叫做permutation test（随机排序测试），即在计算第i个特征的重要性的时候，将N个样本的第i个特征重新洗牌，然后比较D和$D^{(p)}$表现的差异性。如果差异很大，则表明第i个特征是重要的。</p><p><img src="http://img.blog.csdn.net/20170726143830318?" alt="这里写图片描述"></p><p>知道了permutation test的原理后，接下来要考虑的问题是如何衡量上图中的performance，即替换前后的表现。显然，我们前面介绍过performance可以用$E_{oob}(G)$来衡量。但是，对于N个样本的第i个特征值重新洗牌重置的$D^{(p)}$，要对它进行重新训练，而且每个特征都要重复训练，然后再与原D的表现进行比较，过程非常繁琐。为了简化运算，RF的作者提出了一种方法，就是把permutation的操作从原来的training上移到了OOB validation上去，记为$E_{oob}(G^{(p)})\rightarrow E_{oob}^{(p)}(G)$。也就是说，在训练的时候仍然使用D，但是在OOB验证的时候，将所有的OOB样本的第i个特征重新洗牌，验证G的表现。这种做法大大简化了计算复杂度，在RF的feature selection中应用广泛。</p><p><img src="http://img.blog.csdn.net/20170726151729634?" alt="这里写图片描述"></p><h3 id="Random-Forest-in-Action"><a href="#Random-Forest-in-Action" class="headerlink" title="Random Forest in Action"></a>Random Forest in Action</h3><p>最后，我们通过实际的例子来看一下RF的特点。首先，仍然是一个二元分类的例子。如下图所示，左边是一个C&amp;RT树没有使用bootstrap得到的模型分类效果，其中不同特征之间进行了随机组合，所以有斜线作为分类线；中间是由bootstrap（N’=N/2）后生成的一棵决策树组成的随机森林，图中加粗的点表示被bootstrap选中的点；右边是将一棵决策树进行bagging后的分类模型，效果与中间图是一样的，都是一棵树。</p><p><img src="http://img.blog.csdn.net/20170726163347833?" alt="这里写图片描述"></p><p>当t=100，即选择了100棵树时，中间的模型是第100棵决策树构成的，还是只有一棵树；右边的模型是由100棵决策树bagging起来的，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170726164544531?" alt="这里写图片描述"></p><p>当t=200时：</p><p><img src="http://img.blog.csdn.net/20170726164429459?" alt="这里写图片描述"></p><p>当t=300时：</p><p><img src="http://img.blog.csdn.net/20170726164627674?" alt="这里写图片描述"></p><p>当t=400时：</p><p><img src="http://img.blog.csdn.net/20170726164705931?" alt="这里写图片描述"></p><p>当t=500时：</p><p><img src="http://img.blog.csdn.net/20170726164745342?" alt="这里写图片描述"></p><p>当t=600时：</p><p><img src="http://img.blog.csdn.net/20170726164831176?" alt="这里写图片描述"></p><p>当t=700时：</p><p><img src="http://img.blog.csdn.net/20170726164913572?" alt="这里写图片描述"></p><p>当t=800时：</p><p><img src="http://img.blog.csdn.net/20170726164952799?" alt="这里写图片描述"></p><p>当t=900时：</p><p><img src="http://img.blog.csdn.net/20170726165038648?" alt="这里写图片描述"></p><p>当t=1000时：</p><p><img src="http://img.blog.csdn.net/20170726165122742?" alt="这里写图片描述"></p><p>随着树木个数的增加，我们发现，分界线越来越光滑而且得到了large-margin-like boundary，类似于SVM一样的效果。也就是说，树木越多，分类器的置信区间越大。</p><p>然后，我们再来看一个比较复杂的例子，二维平面上分布着许多离散点，分界线形如sin函数。当只有一棵树的时候（t=1），下图左边表示单一树组成的RF，右边表示所有树bagging组合起来构成的RF。因为只有一棵树，所以左右两边效果一致。</p><p><img src="http://img.blog.csdn.net/20170726170024403?" alt="这里写图片描述"></p><p>当t=6时：</p><p><img src="http://img.blog.csdn.net/20170726170110624?" alt="这里写图片描述"></p><p>当t=11时：</p><p><img src="http://img.blog.csdn.net/20170726170152368?" alt="这里写图片描述"></p><p>当t=16时：</p><p><img src="http://img.blog.csdn.net/20170726170228226?" alt="这里写图片描述"></p><p>当t=21时：</p><p><img src="http://img.blog.csdn.net/20170726170319362?" alt="这里写图片描述"></p><p>可以看到，当RF由21棵树构成的时候，分界线就比较平滑了，而且它的边界比单一树构成的RF要robust得多，更加平滑和稳定。</p><p>最后，基于上面的例子，再让问题复杂一点：在平面上添加一些随机噪声。当t=1时，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170726170845858?" alt="这里写图片描述"></p><p>当t=6时：</p><p><img src="http://img.blog.csdn.net/20170726170944858?" alt="这里写图片描述"></p><p>当t=11时：</p><p><img src="http://img.blog.csdn.net/20170726171026145?" alt="这里写图片描述"></p><p>当t=16时：</p><p><img src="http://img.blog.csdn.net/20170726171107707?" alt="这里写图片描述"></p><p>当t=21时：</p><p><img src="http://img.blog.csdn.net/20170726171144679?" alt="这里写图片描述"></p><p>从上图中，我们发现21棵树的时候，随机noise的影响基本上能够修正和消除。这种bagging投票的机制能够保证较好的降噪性，从而得到比较稳定的结果。</p><p>经过以上三个例子，我们发现RF中，树的个数越多，模型越稳定越能表现得好。在实际应用中，应该尽可能选择更多的树。值得一提的是，RF的表现同时也与random seed有关，即随机的初始值也会影响RF的表现。</p><p><img src="http://img.blog.csdn.net/20170726172115021?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Random Forest算法模型。RF将bagging与decision tree结合起来，通过把众多的决策树组进行组合，构成森林的形式，利用投票机制让G表现最佳，分类模型更稳定。其中为了让decision tree的随机性更强一些，可以采用randomly projected subspaces操作，即将不同的features线性组合起来，从而进行各式各样的切割。同时，我们也介绍了可以使用OOB样本来进行self-validation，然后可以使用self-validation来对每个特征进行permutaion test，得到不同特征的重要性，从而进行feature selection。总的来说，RF算法能够得到比较平滑的边界，稳定性强，前提是有足够多的树。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170725105700489?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记9 -- Decision Tree</title>
    <link href="https://redstonewill.github.io/2018/03/18/26/"/>
    <id>https://redstonewill.github.io/2018/03/18/26/</id>
    <published>2018-03-18T06:06:39.000Z</published>
    <updated>2018-03-18T06:08:24.631Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170719082956412?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Adaptive Boosting。AdaBoost演算法通过调整每笔资料的权重，得到不同的hypotheses，然后将不同的hypothesis乘以不同的系数$\alpha$进行线性组合。这种演算法的优点是，即使底层的演算法g不是特别好（只要比乱选好点），经过多次迭代后算法模型会越来越好，起到了boost提升的效果。本节课将在此基础上介绍一种新的aggregation算法：决策树（Decision Tree）。</p><h3 id="Decision-Tree-Hypothesis"><a href="#Decision-Tree-Hypothesis" class="headerlink" title="Decision Tree Hypothesis"></a>Decision Tree Hypothesis</h3><p>从第7节课开始，我们就一直在介绍aggregation model。aggregation的核心就是将许多可供选择使用的比较好的hypothesis融合起来，利用集体的智慧组合成G，使其得到更好的机器学习预测模型。下面，我们先来看看已经介绍过的aggregation type有哪些。</p><p><img src="http://img.blog.csdn.net/20170719082956412?" alt="这里写图片描述"></p><p>aggregation type有三种：uniform，non-uniform，conditional。它有两种情况，一种是所有的g是已知的，即blending。对应的三种类型分别是voting/averaging，linear和stacking。另外一种情况是所有g未知，只能通过手上的资料重构g，即learning。其中uniform和non-uniform分别对应的是Bagging和AdaBoost算法，而conditional对应的就是我们本节课将要介绍的Decision Tree算法。</p><p>决策树（Decision Tree）模型是一种传统的算法，它的处理方式与人类思维十分相似。例如下面这个例子，对下班时间、约会情况、提交截止时间这些条件进行判断，从而决定是否要进行在线课程测试。如下图所示，整个流程类似一个树状结构。</p><p><img src="http://img.blog.csdn.net/20170719084744044?" alt="这里写图片描述"></p><p>图中每个条件和选择都决定了最终的结果，Y or N。蓝色的圆圈表示树的叶子，即最终的决定。</p><p>把这种树状结构对应到一个hypothesis G(x)中，G(x)的表达式为：</p><p>$$G(x)=\sum_{t=1}^Tq_t(x)\cdot g_t(x)$$</p><p>G(x)由许多$g_t(x)$组成，即aggregation的做法。每个$g_t(x)$就代表上图中的蓝色圆圈（树的叶子）。这里的$g_t(x)$是常数，因为是处理简单的classification问题。我们把这些$g_t(x)$称为base hypothesis。$q_t(x)$表示每个$g_t(x)$成立的条件，代表上图中橘色箭头的部分。不同的$g_t(x)$对应于不同的$q_t(x)$，即从树的根部到顶端叶子的路径不同。图中中的菱形代表每个简单的节点。所以，这些base hypothesis和conditions就构成了整个G(x)的形式，就像一棵树一样，从根部到顶端所有的叶子都安全映射到上述公式上去了。</p><p><img src="http://img.blog.csdn.net/20170719103654190?" alt="这里写图片描述"></p><p>决策树实际上就是在模仿人类做决策的过程。一直以来，决策树的应用十分广泛而且分类预测效果都很不错，而它在数学上的理论完备性不充分，倒也不必在意。</p><p>如果从另外一个方面来看决策树的形式，不同于上述G(x)的公式，我们可以利用条件分支的思想，将整体G(x)分成若干个$G_c(x)$，也就是把整个大树分成若干个小树，如下所示：</p><p>$$G(x)=\sum_{c=1}^C[b(x)=c]\cdot G_c(x)$$</p><p>上式中，G(x)表示完整的大树，即full-tree hypothesis，b(x)表示每个分支条件，即branching criteria，$G_c(x)$表示第c个分支下的子树，即sub-tree。这种结构被称为递归型的数据结构，即将大树分割成不同的小树，再将小树继续分割成更小的子树。所以，决策树可以分为两部分：root和sub-trees。</p><p><img src="http://img.blog.csdn.net/20170719103835724?" alt="这里写图片描述"></p><p>在详细推导决策树算法之前，我们先来看一看它的优点和缺点。首先，decision tree的优点有：</p><ul><li><p><strong>模型直观，便于理解，应用广泛</strong></p></li><li><p><strong>算法简单，容易实现</strong></p></li><li><p><strong>训练和预测时，效率较高</strong></p></li></ul><p>然而，decision tree也有相应的缺点：</p><ul><li><p><strong>缺少足够的理论支持</strong></p></li><li><p><strong>如何选择合适的树结构对初学者来说比较困惑</strong></p></li><li><p><strong>决策树代表性的演算法比较少</strong></p></li></ul><p><img src="http://img.blog.csdn.net/20170719155104812?" alt="这里写图片描述"></p><h3 id="Decision-Tree-Algorithm"><a href="#Decision-Tree-Algorithm" class="headerlink" title="Decision Tree Algorithm"></a>Decision Tree Algorithm</h3><p>我们可以用递归形式将decision tree表示出来，它的基本的算法可以写成：</p><p><img src="http://img.blog.csdn.net/20170720073120706?" alt="这里写图片描述"></p><p>这个Basic Decision Tree Algorithm的流程可以分成四个部分，首先学习设定划分不同分支的标准和条件是什么；接着将整体数据集D根据分支个数C和条件，划为不同分支下的子集Dc；然后对每个分支下的Dc进行训练，得到相应的机器学习模型Gc；最后将所有分支下的Gc合并到一起，组成大矩G(x)。但值得注意的是，这种递归的形式需要终止条件，否则程序将一直进行下去。当满足递归的终止条件之后，将会返回基本的hypothesis $g_t(x)$。</p><p><img src="http://img.blog.csdn.net/20170720074057776?" alt="这里写图片描述"></p><p>所以，决策树的基本演算法包含了四个选择：</p><ul><li><p><strong>分支个数（number of branches）</strong></p></li><li><p><strong>分支条件（branching criteria）</strong></p></li><li><p><strong>终止条件（termination criteria）</strong></p></li><li><p><strong>基本算法（base hypothesis）</strong></p></li></ul><p>下面我们来介绍一种常用的决策树模型算法，叫做Classification and Regression Tree(C&amp;RT)。C&amp;RT算法有两个简单的设定，首先，分支的个数C=2，即二叉树（binary tree）的数据结构；然后，每个分支最后的$g_t(x)$（数的叶子）是一个常数。按照最小化$E_{in}$的目标，对于binary/multiclass classification(0/1 error)问题，看正类和负类哪个更多，$g_t(x)$取所占比例最多的那一类$y_n$；对于regression(squared error)问题，$g_t(x)$则取所有$y_n$的平均值。</p><p><img src="http://img.blog.csdn.net/20170720081326313?" alt="这里写图片描述"></p><p>对于决策树的基本演算法流程，C&amp;RT还有一些简单的设定。首先，C&amp;RT分支个数C=2，一般采用上节课介绍过的decision stump的方法进行数据切割。也就是每次在一个维度上，只对一个特征feature将数据一分为二，左子树和右子树，分别代表不同的类别。然而，怎么切割才能让数据划分得最好呢（error最小）？C&amp;RT中使用纯净度purifying这个概念来选择最好的decision stump。purifying的核心思想就是每次切割都尽可能让左子树和右子树中同类样本占得比例最大或者$y_n$都很接近（regression），即错误率最小。比如说classifiacation问题中，如果左子树全是正样本，右子树全是负样本，那么它的纯净度就很大，说明该分支效果很好。</p><p><img src="http://img.blog.csdn.net/20170720083357302?" alt="这里写图片描述"></p><p>根据C&amp;RT中purifying的思想，我们得到选择合适的分支条件b(x)的表达式如上所示。最好的decision stump重点包含两个方面：一个是刚刚介绍的分支纯净度purifying，purifying越大越好，而这里使用purifying相反的概念impurity，则impurity越小越好；另外一个是左右分支纯净度所占的权重，权重大小由该分支的数据量决定，分支包含的样本个数越多，则所占权重越大，分支包含的样本个数越少，则所占权重越小。上式中的$|D_c\ with\ h|$代表了分支c所占的权重。这里b(x)类似于error function（这也是为什么使用impurity代替purifying的原因），选择最好的decision stump，让所有分支的不纯度最小化，使b(x)越小越好。</p><p>不纯度Impurity如何用函数的形式量化？一种简单的方法就是类比于$E_{in}$，看预测值与真实值的误差是多少。对于regression问题，它的impurity可表示为：</p><p>$$impurity(D)=\frac1N\sum_{n=1}^N(y_n-\overline{y})^2$$</p><p>其中，$\overline{y}$表示对应分支下所有$y_n$的均值。</p><p>对应classification问题，它的impurity可表示为：</p><p>$$impurity(D)=\frac1N\sum_{n=1}^N[y_n\neq y^*]$$</p><p>其中，$y^*$表示对应分支下所占比例最大的那一类。</p><p><img src="http://img.blog.csdn.net/20170720090259338?" alt="这里写图片描述"></p><p>以上这些impurity是基于原来的regression error和classification error直接推导的。进一步来看classification的impurity functions，如果某分支条件下，让其中一个分支纯度最大，那么就选择对应的decision stump，即得到的classification error为：</p><p>$$1-max_{1\leq k\leq K}\frac{\sum_{n=1}^N[y_n=k]}{N}$$</p><p>其中，K为分支个数。</p><p>上面这个式子只考虑纯度最大的那个分支，更好的做法是将所有分支的纯度都考虑并计算在内，用基尼指数（Gini index）表示：</p><p>$$1-\sum_{k=1}^K(\frac{\sum_{n=1}^N[y_n=k]}{N})^2$$</p><p>Gini index的优点是将所有的class在数据集中的分布状况和所占比例全都考虑了，这样让decision stump的选择更加准确。</p><p><img src="http://img.blog.csdn.net/20170720112908397?" alt="这里写图片描述"></p><p>对于决策树C&amp;RT算法，通常来说，上面介绍的各种impurity functions中，Gini index更适合求解classification问题，而regression error更适合求解regression问题。</p><p>C&amp;RT算法迭代终止条件有两种情况，第一种情况是当前各个分支下包含的所有样本$y_n$都是同类的，即不纯度impurity为0，表示该分支已经达到了最佳分类程度。第二种情况是该特征下所有的$x_n$相同，无法对其进行区分，表示没有decision stumps。遇到这两种情况，C&amp;RT算法就会停止迭代。</p><p><img src="http://img.blog.csdn.net/20170720135537933?" alt="这里写图片描述"></p><p>所以，C&amp;RT算法遇到迭代终止条件后就成为完全长成树（fully-grown tree）。它每次分支为二，是二叉树结构，采用purify来选择最佳的decision stump来划分，最终得到的叶子（$g_t(x)$）是常数。</p><h3 id="Decision-Tree-Heuristics-in-C-amp-RT"><a href="#Decision-Tree-Heuristics-in-C-amp-RT" class="headerlink" title="Decision Tree Heuristics in C&amp;RT"></a>Decision Tree Heuristics in C&amp;RT</h3><p>现在我们已经知道了C&amp;RT算法的基本流程：</p><p><img src="http://img.blog.csdn.net/20170720152355327?" alt="这里写图片描述"></p><p>可以看到C&amp;RT算法在处理binary classification和regression问题时非常简单实用，而且，处理muti-class classification问题也十分容易。</p><p>考虑这样一个问题，有N个样本，如果我们每次只取一个样本点作为分支，那么在经过N-1次分支之后，所有的样本点都能完全分类正确。最终每片叶子上只有一个样本，有N片叶子，即必然能保证$E_{in}=0$。这样看似是完美的分割，但是不可避免地造成VC Dimension无限大，造成模型复杂度增加，从而出现过拟合现象。为了避免overfit，我们需要在C&amp;RT算法中引入正则化，来控制整个模型的复杂度。</p><p>考虑到避免模型过于复杂的方法是减少叶子（$g_t(x)$）的数量，那么可以令regularizer就为决策树中叶子的总数，记为$\Omega(G)$。正则化的目的是尽可能减少$\Omega(G)$的值。这样，regularized decision tree的形式就可以表示成：</p><p>$$argmin_{(all\ possible\ G)}\ E_{in}(G)+\lambda\Omega(G)$$</p><p>我们把这种regularized decision tree称为pruned decision tree。pruned是修剪的意思，通过regularization来修剪决策树，去掉多余的叶子，更简洁化，从而达到避免过拟合的效果。</p><p>那么如何确定修剪多少叶子，修剪哪些叶子呢？假设由C&amp;RT算法得到一棵完全长成树（fully-grown tree），总共10片叶子。首先分别减去其中一片叶子，剩下9片，将这10种情况比较，取$E_{in}$最小的那个模型；然后再从9片叶子的模型中分别减去一片，剩下8片，将这9种情况比较，取$E_{in}$最小的那个模型。以此类推，继续修建叶子。这样，最终得到包含不同叶子的几种模型，将这几个使用regularized decision tree的error function来进行选择，确定包含几片叶子的模型误差最小，就选择该模型。另外，参数$\lambda$可以通过validation来确定最佳值。</p><p><img src="http://img.blog.csdn.net/20170721083658001?" alt="这里写图片描述"></p><p>我们一直讨论决策树上的叶子（features）都是numerical features，而实际应用中，决策树的特征值可能不是数字量，而是类别（categorical features）。对于numerical features，我们直接使用decision stump进行数值切割；而对于categorical features，我们仍然可以使用decision subset，对不同类别进行“左”和“右”，即是与不是（0和1）的划分。numerical features和categorical features的具体区别如下图所示：</p><p><img src="http://img.blog.csdn.net/20170721084601173?" alt="这里写图片描述"></p><p>在决策树中预测中，还会遇到一种问题，就是当某些特征缺失的时候，没有办法进行切割和分支选择。一种常用的方法就是surrogate branch，即寻找与该特征相似的替代feature。如何确定是相似的feature呢？做法是在决策树训练的时候，找出与该特征相似的feature，如果替代的feature与原feature切割的方式和结果是类似的，那么就表明二者是相似的，就把该替代的feature也存储下来。当预测时遇到原feature缺失的情况，就用替代feature进行分支判断和选择。</p><p><img src="http://img.blog.csdn.net/20170721090058551?" alt="这里写图片描述"></p><h3 id="Decision-Tree-in-Action"><a href="#Decision-Tree-in-Action" class="headerlink" title="Decision Tree in Action"></a>Decision Tree in Action</h3><p>最后我们来举个例子看看C&amp;RT算法究竟是如何进行计算的。例如下图二维平面上分布着许多正负样本，我们使用C&amp;RT算法来对其进行决策树的分类。</p><p><img src="http://img.blog.csdn.net/20170721220210716?" alt="这里写图片描述"></p><p>第一步：</p><p><img src="http://img.blog.csdn.net/20170721220326437?" alt="这里写图片描述"></p><p>第二步：</p><p><img src="http://img.blog.csdn.net/20170721220405953?" alt="这里写图片描述"></p><p>第三步：</p><p><img src="http://img.blog.csdn.net/20170721220449192?" alt="这里写图片描述"></p><p>第四步：</p><p><img src="http://img.blog.csdn.net/20170721220537242?" alt="这里写图片描述"></p><p>在进行第四步切割之后，我们发现每个分支都已经非常纯净了，没有办法继续往下切割。此时表明已经满足了迭代终止条件，这时候就可以回传base hypothesis，构成sub tree，然后每个sub tree再往上整合形成tree，最后形成我们需要的完全决策树。如果将边界添加上去，可得到下图：</p><p><img src="http://img.blog.csdn.net/20170721221314795?" alt="这里写图片描述"></p><p>得到C&amp;RT算法的切割方式之后，我们与AdaBoost-Stump算法进行比较：</p><p><img src="http://img.blog.csdn.net/20170721221458536?" alt="这里写图片描述"></p><p>我们之前就介绍过，AdaBoost-Stump算法的切割线是横跨整个平面的；而C&amp;RT算法的切割线是基于某个条件的，所以一般不会横跨整个平面。比较起来，虽然C&amp;RT和AdaBoost-Stump都采用decision stump方式进行切割，但是二者在细节上还是有所区别。</p><p>再看一个数据集分布比较复杂的例子，C&amp;RT和AdaBoost-Stump的切割方式对比效果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170721222051208?" alt="这里写图片描述"></p><p>通常来说，由于C&amp;RT是基于条件进行切割的，所以C&amp;RT比AdaBoost-Stump分类切割更有效率。总结一下，C&amp;RT决策树有以下特点：</p><p><img src="http://img.blog.csdn.net/20170721222446843?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Decision Tree。首先将decision tree hypothesis对应到不同分支下的矩$g_t(x)$。然后再介绍决策树算法是如何通过递归的形式建立起来。接着详细研究了决策树C&amp;RT算法对应的数学模型和算法架构流程。最后通过一个实际的例子来演示决策树C&amp;RT算法是如何一步一步进行分类的。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170719082956412?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记8 -- Adaptive Boosting</title>
    <link href="https://redstonewill.github.io/2018/03/18/25/"/>
    <id>https://redstonewill.github.io/2018/03/18/25/</id>
    <published>2018-03-18T06:01:09.000Z</published>
    <updated>2018-03-18T06:02:40.916Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170713163950772?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要开始介绍Aggregation Models，目的是将不同的hypothesis得到的$g_t$集合起来，利用集体智慧得到更好的预测模型G。首先我们介绍了Blending，blending是将已存在的所有$g_t$结合起来，可以是uniformly，linearly，或者non-linearly组合形式。然后，我们讨论了在没有那么多$g_t$的情况下，使用bootstrap方式，从已有数据集中得到新的类似的数据集，从而得到不同的$g_t$。这种做法称为bagging。本节课将继续从这些概念出发，介绍一种新的演算法。</p><h3 id="Motivation-of-Boosting"><a href="#Motivation-of-Boosting" class="headerlink" title="Motivation of Boosting"></a>Motivation of Boosting</h3><p>我们先来看一个简单的识别苹果的例子，老师展示20张图片，让6岁孩子们通过观察，判断其中哪些图片的内容是苹果。从判断的过程中推导如何解决二元分类问题的方法。</p><p>显然这是一个监督式学习，20张图片包括它的标签都是已知的。首先，学生Michael回答说：所有的苹果应该是圆形的。根据Michael的判断，对应到20张图片中去，大部分苹果能被识别出来，但也有错误。其中错误包括有的苹果不是圆形，而且圆形的水果也不一定是苹果。如下图所示：</p><p><img src="http://img.blog.csdn.net/20170713163950772?" alt="这里写图片描述"></p><p>上图中蓝色区域的图片代表分类错误。显然，只用“苹果是圆形的”这一个条件不能保证分类效果很好。我们把蓝色区域（分类错误的图片）放大，分类正确的图片缩小，这样在接下来的分类中就会更加注重这些错误样本。</p><p>然后，学生Tina观察被放大的错误样本和上一轮被缩小的正确样本，回答说：苹果应该是红色的。根据Tina的判断，得到的结果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170713171109291?" alt="这里写图片描述"></p><p>上图中蓝色区域的图片一样代表分类错误，即根据这个苹果是红色的条件，使得青苹果和草莓、西红柿都出现了判断错误。那么结果就是把这些分类错误的样本放大化，其它正确的样本缩小化。同样，这样在接下来的分类中就会更加注重这些错误样本。</p><p>接着，学生Joey经过观察又说：苹果也可能是绿色的。根据Joey的判断，得到的结果如下图所示：</p><p><img src="http://img.blog.csdn.net/20170713214123596?" alt="这里写图片描述"></p><p>上图中蓝色区域的图片一样代表分类错误，根据苹果是绿色的条件，使得图中蓝色区域都出现了判断错误。同样把这些分类错误的样本放大化，其它正确的样本缩小化，在下一轮判断继续对其修正。</p><p>后来，学生Jessica又发现：上面有梗的才是苹果。得到如下结果：</p><p><img src="http://img.blog.csdn.net/20170713214711978?" alt="这里写图片描述"></p><p>经过这几个同学的推论，苹果被定义为：圆的，红色的，也可能是绿色的，上面有梗。从一个一个的推导过程中，我们似乎得到一个较为准确的苹果的定义。虽然可能不是非常准确，但是要比单一的条件要好得多。也就是说把所有学生对苹果的定义融合起来，最终得到一个比较好的对苹果的总体定义。这种做法就是我们本节课将要讨论的演算法。这些学生代表的就是简单的hypotheses $g_t$，将所有$g_t$融合，得到很好的预测模型G。例如，二维平面上简单的hypotheses（水平线和垂直线），这些简单$g_t$最终组成的较复杂的分类线能够较好地将正负样本完全分开，即得到了好的预测模型。</p><p><img src="http://img.blog.csdn.net/20170713215718130?" alt="这里写图片描述"></p><p>所以，上个苹果的例子中，不同的学生代表不同的hypotheses $g_t$；最终得到的苹果总体定义就代表hypothesis G；而老师就代表演算法A，指导学生的注意力集中到关键的例子中（错误样本），从而得到更好的苹果定义。其中的数学原理，我们下一部分详细介绍。</p><p><img src="http://img.blog.csdn.net/20170713215800108?" alt="这里写图片描述"></p><h3 id="Diversity-by-Re-weighting"><a href="#Diversity-by-Re-weighting" class="headerlink" title="Diversity by Re-weighting"></a>Diversity by Re-weighting</h3><p>在介绍这个演算法之前，我们先来讲一下上节课就介绍过的bagging。Bagging的核心是bootstrapping，通过对原始数据集D不断进行bootstrap的抽样动作，得到与D类似的数据集$\hat{D}_t$，每组$\hat{D}_t$都能得到相应的$g_t$，从而进行aggregation的操作。现在，假如包含四个样本的D经过bootstrap，得到新的$\hat{D}_t$如下：</p><p><img src="http://img.blog.csdn.net/20170714080644583?" alt="这里写图片描述"></p><p>那么，对于新的$\hat{D}<em>t$，把它交给base algorithm，找出$E</em>{in}$最小时对应的$g_t$，如下图右边所示。</p><p>$$E_{in}^{0/1}(h)=\frac14\sum_{n=1}^4[y\neq h(x)]$$</p><p>由于$\hat{D}_t$完全是D经过bootstrap得到的，其中样本$(x_1,y_1)$出现2次，$(x_2,y_2)$出现1次，$(x_3,y_3)$出现0次，$(x_4,y_4)$出现1次。引入一个参数$u_i$来表示原D中第i个样本在$\hat{D}_t$中出现的次数，如下图左边所示。</p><p>$$E_{in}^u(h)=\frac14\sum_{n=1}^4u_n^{(t)}\cdot [y_n\neq h(x)]$$</p><p><img src="http://img.blog.csdn.net/20170714081506965?" alt="这里写图片描述"></p><p>参数u相当于是权重因子，当$\hat{D}_t$中第i个样本出现的次数越多的时候，那么对应的$u_i$越大，表示在error function中对该样本的惩罚越多。所以，从另外一个角度来看bagging，它其实就是通过bootstrap的方式，来得到这些$u_i$值，作为犯错样本的权重因子，再用base algorithn最小化包含$u_i$的error function，得到不同的$g_t$。这个error function被称为bootstrap-weighted error。</p><p>这种算法叫做Weightd Base Algorithm，目的就是最小化bootstrap-weighted error。</p><p><img src="http://img.blog.csdn.net/20170714083230109?" alt="这里写图片描述"></p><p>其实，这种weightd base algorithm我们之前就介绍过类似的算法形式。例如在soft-margin SVM中，我们引入允许犯错的项，同样可以将每个点的error乘以权重因子$u_n$。加上该项前的参数C，经过QP，最终得到$0\leq \alpha_n\leq Cu_n$，有别于之前介绍的$0\leq \alpha_n\leq C$。这里的$u_n$相当于每个犯错的样本的惩罚因子，并会反映到$\alpha_n$的范围限定上。</p><p>同样在logistic regression中，同样可以对每个犯错误的样本乘以相应的$u_n$，作为惩罚因子。$u_n$表示该错误点出现的次数，$u_n$越大，则对应的惩罚因子越大，则在最小化error时就应该更加重视这些点。</p><p><img src="http://img.blog.csdn.net/20170714085048224?" alt="这里写图片描述"></p><p>其实这种example-weighted learning，我们在机器学习基石课程第8次笔记中就介绍过class-weighted的思想。二者道理是相通的。</p><p>知道了u的概念后，我们知道不同的u组合经过base algorithm得到不同的$g_t$。那么如何选取u，使得到的$g_t$之间有很大的不同呢？之所以要让所有的$g_t$差别很大，是因为上节课aggregation中，我们介绍过$g_t$越不一样，其aggregation的效果越好，即每个人的意见越不相同，越能运用集体的智慧，得到好的预测模型。</p><p>为了得到不同的$g_t$，我们先来看看$g_t$和$g_{t+1}$是怎么得到的：</p><p><img src="http://img.blog.csdn.net/20170714131902089?" alt="这里写图片描述"></p><p>如上所示，$g_t$是由$u_n^{t}$得到的，$g_{t+1}$是由$u_n^{(t+1)}$得到的。如果$g_t$这个模型在使用$u_n^{(t+1)}$的时候得到的error很大，即预测效果非常不好，那就表示由$u_n^{(t+1)}$计算的$g_{t+1}$会与$g_t$有很大不同。而$g_{t+1}$与$g_t$差异性大正是我们希望看到的。</p><p>怎么做呢？方法是利用$g_t$在使用$u_n^{(t+1)}$的时候表现很差的条件，越差越好。如果在$g_t$作用下，$u_n^{(t+1)}$中的表现（即error）近似为0.5的时候，表明$g_t$对$u_n^{(t+1)}$的预测分类没有什么作用，就像抛硬币一样，是随机选择的。这样的做法就能最大限度地保证$g_{t+1}$会与$g_t$有较大的差异性。其数学表达式如下所示：</p><p><img src="http://img.blog.csdn.net/20170714134048908?" alt="这里写图片描述"></p><p>乍看上面这个式子，似乎不好求解。但是，我们对它做一些等价处理，其中分式中分子可以看成$g_t$作用下犯错误的点，而分母可以看成犯错的点和没有犯错误的点的集合，即所有样本点。其中犯错误的点和没有犯错误的点分别用橘色方块和绿色圆圈表示：</p><p><img src="http://img.blog.csdn.net/20170714135429196?" alt="这里写图片描述"></p><p>要让分式等于0.5，显然只要将犯错误的点和没有犯错误的点的数量调成一样就可以了。也就是说，在$g_t$作用下，让犯错的$u_n^{(t+1)}$数量和没有犯错的$u_n^{(t+1)}$数量一致就行（包含权重$u_n^{t+1}$）。一种简单的方法就是利用放大和缩小的思想（本节课开始引入识别苹果的例子中提到的放大图片和缩小图片就是这个目的），将犯错误的$u_n^{t}$和没有犯错误的$u_n^{t}$做相应的乘积操作，使得二者值变成相等。例如$u_n^{t}$ of incorrect为1126，$u_n^{t}$ of correct为6211，要让$u_n^{(t+1)}$中错误比例正好是0.5，可以这样做，对于incorrect $u_n^{(t+1)}$：</p><p>$$u_n^{(t+1)}\leftarrow u_n^{(t)}\cdot 6211$$</p><p>对于correct $u_n^{(t+1)}$：</p><p>$$u_n^{(t+1)}\leftarrow u_n^{(t)}\cdot 1126$$</p><p>或者利用犯错的比例来做，令weighted incorrect rate和weighted correct rate分别设为$\frac{1126}{7337}$和$\frac{6211}{7337}$。一般求解方式是令犯错率为$\epsilon_t$，在计算$u_n^{(t+1)}$的时候，$u_n^{t}$分别乘以$(1-\epsilon_t)$和$\epsilon_t$。</p><p><img src="http://img.blog.csdn.net/20170714141601750?" alt="这里写图片描述"></p><h3 id="Adaptive-Boosting-Algorithm"><a href="#Adaptive-Boosting-Algorithm" class="headerlink" title="Adaptive Boosting Algorithm"></a>Adaptive Boosting Algorithm</h3><p>上一部分，我们介绍了在计算$u_n^{(t+1)}$的时候，$u_n^{t}$分别乘以$(1-\epsilon_t)$和$\epsilon_t$。下面将构造一个新的尺度因子：</p><p>$$\diamond t=\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$$</p><p>那么引入这个新的尺度因子之后，对于错误的$u_n^{t}$，将它乘以$\diamond t$；对于正确的$u_n^{t}$，将它除以$\diamond t$。这种操作跟之前介绍的分别乘以$(1-\epsilon_t)$和$\epsilon_t$的效果是一样的。之所以引入$\diamond t$是因为它告诉我们更多的物理意义。因为如果$\epsilon_t\leq\frac12$，得到$\diamond t\geq1$，那么接下来错误的$u_n^{t}$与$\diamond t$的乘积就相当于把错误点放大了，而正确的$u_n^{t}$与$\diamond t$的相除就相当于把正确点缩小了。这种scale up incorrect和scale down correct的做法与本节课开始介绍的学生识别苹果的例子中放大错误的图片和缩小正确的图片是一个原理，让学生能够将注意力更多地放在犯错误的点上。通过这种scaling-up incorrect的操作，能够保证得到不同于$g_t$的$g_{t+1}$。</p><p><img src="http://img.blog.csdn.net/20170714152321580?" alt="这里写图片描述"></p><p>值得注意的是上述的结论是建立在$\epsilon_t\leq\frac12$的基础上，如果$\epsilon_t\geq\frac12$，那么就做相反的推论即可。关于$\epsilon_t\geq\frac12$的情况，我们稍后会进行说明。</p><p>从这个概念出发，我们可以得到一个初步的演算法。其核心步骤是每次迭代时，利用$\diamond t=\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$把$u_t$更新为$u_{t+1}$。具体迭代步骤如下：</p><p><img src="http://img.blog.csdn.net/20170717080641948?" alt="这里写图片描述"></p><p>但是，上述步骤还有两个问题没有解决，第一个问题是初始的$u^{(1)}$应为多少呢？一般来说，为了保证第一次$E_{in}$最小的话，设$u^{(1)}=\frac1N$即可。这样最开始的$g_1$就能由此推导。第二个问题，最终的G(x)应该怎么求？是将所有的g(t)合并uniform在一起吗？一般来说并不是这样直接uniform求解，因为$g_{t+1}$是通过$g_t$得来的，二者在$E_{in}$上的表现差别比较大。所以，一般是对所有的g(t)进行linear或者non-linear组合来得到G(t)。</p><p><img src="http://img.blog.csdn.net/20170717081850838?" alt="这里写图片描述"></p><p>接下来的内容，我们将对上面的第二个问题进行探讨，研究一种算法，将所有的g(t)进行linear组合。方法是计算$g(t)$的同时，就能计算得到其线性组合系数$\alpha_t$，即aggregate linearly on the fly。这种算法使最终求得$g_{t+1}$的时候，所有$g_t$的线性组合系数$\alpha$也求得了，不用再重新计算$\alpha$了。这种Linear Aggregation on the Fly算法流程为：</p><p><img src="http://img.blog.csdn.net/20170717083113667?" alt="这里写图片描述"></p><p>如何在每次迭代的时候计算$\alpha_t$呢？我们知道$\alpha_t$与$\epsilon_t$是相关的：$\epsilon_t$越小，对应的$\alpha_t$应该越大，$\epsilon_t$越大，对应的$\alpha_t$应该越小。又因为$\diamond t$与$\epsilon_t$是正相关的，所以，$\alpha_t$应该是$\diamond t$的单调函数。我们构造$\alpha_t$为：</p><p>$$\alpha_t=ln(\diamond t)$$</p><p>$\alpha_t$这样取值是有物理意义的，例如当$\epsilon_t=\frac12$时，error很大，跟掷骰子这样的随机过程没什么两样，此时对应的$\diamond t=1$，$\alpha_t=0$，即此$g_t$对G没有什么贡献，权重应该设为零。而当$\epsilon_t=0$时，没有error，表示该$g_t$预测非常准，此时对应的$\diamond t=\infty$，$\alpha_t=\infty$，即此$g_t$对G贡献非常大，权重应该设为无穷大。</p><p><img src="http://img.blog.csdn.net/20170717085137932?" alt="这里写图片描述"></p><p>这种算法被称为Adaptive Boosting。它由三部分构成：base learning algorithm A，re-weighting factor $\diamond t$和linear aggregation $\alpha_t$。这三部分分别对应于我们在本节课开始介绍的例子中的Student，Teacher和Class。</p><p><img src="http://img.blog.csdn.net/20170717085756381?" alt="这里写图片描述"></p><p>综上所述，完整的adaptive boosting（AdaBoost）Algorithm流程如下：</p><p><img src="http://img.blog.csdn.net/20170717112519198?" alt="这里写图片描述"></p><p>从我们之前介绍过的VC bound角度来看，AdaBoost算法理论上满足：</p><p><img src="http://img.blog.csdn.net/20170717112851795?" alt="这里写图片描述"></p><p>上式中，$E_{out}(G)$的上界由两部分组成，一项是$E_{in}(G)$，另一项是模型复杂度O(*)。模型复杂度中$d_{vc}(H)$是$g_t$的VC Dimension，T是迭代次数，可以证明G的$d_{vc}$服从$O(d_{vc}(H)\cdot Tlog\ T)$。</p><p>对这个VC bound中的第一项$E_{in}(G)$来说，有一个很好的性质：如果满足$\epsilon_t\leq \epsilon&lt;\frac12$，则经过$T=O(log\ N)$次迭代之后，$E_{in}(G)$能减小到等于零的程度。而当N很大的时候，其中第二项也能变得很小。因为这两项都能变得很小，那么整个$E_{out}(G)$就能被限定在一个有限的上界中。</p><p>其实，这种性质也正是AdaBoost算法的精髓所在。只要每次的$\epsilon_t\leq \epsilon&lt;\frac12$，即所选择的矩g比乱猜的表现好一点点，那么经过每次迭代之后，矩g的表现都会比原来更好一些，逐渐变强，最终得到$E_{in}=0$且$E_{out}$很小。</p><p><img src="http://img.blog.csdn.net/20170717132338143?" alt="这里写图片描述"></p><h3 id="Adaptive-Boosting-in-Action"><a href="#Adaptive-Boosting-in-Action" class="headerlink" title="Adaptive Boosting in Action"></a>Adaptive Boosting in Action</h3><p>上一小节我们已经介绍了选择一个“弱弱”的算法A（$\epsilon_t\leq \epsilon&lt;\frac12$，比乱猜好就行），就能经过多次迭代得到$E_{in}=0$。我们称这种形式为decision stump模型。下面介绍一个例子，来看看AdaBoost是如何使用decision stump解决实际问题的。</p><p>如下图所示，二维平面上分布一些正负样本点，利用decision stump来做切割。</p><p><img src="http://img.blog.csdn.net/20170717134058765?" alt="这里写图片描述"></p><p>第一步：</p><p><img src="http://img.blog.csdn.net/20170717134139058?" alt="这里写图片描述"></p><p>第二步：</p><p><img src="http://img.blog.csdn.net/20170717134354783?" alt="这里写图片描述"></p><p>第三步：</p><p><img src="http://img.blog.csdn.net/20170717134445972?" alt="这里写图片描述"></p><p>第四步：</p><p><img src="http://img.blog.csdn.net/20170717134548872?" alt="这里写图片描述"></p><p>第五步：</p><p><img src="http://img.blog.csdn.net/20170717134642556?" alt="这里写图片描述"></p><p>可以看到，经过5次迭代之后，所有的正负点已经被完全分开了，则最终得到的分类线为：</p><p><img src="http://img.blog.csdn.net/20170717134743082?" alt="这里写图片描述"></p><p>另外一个例子，对于一个相对比较复杂的数据集，如下图所示。它的分界线从视觉上看应该是一个sin波的形式。如果我们再使用AdaBoost算法，通过decision stump来做切割。在迭代切割100次后，得到的分界线如下所示。</p><p><img src="http://img.blog.csdn.net/20170717135425498?" alt="这里写图片描述"></p><p>可以看出，AdaBoost-Stump这种非线性模型得到的分界线对正负样本有较好的分离效果。</p><p>课程中还介绍了一个AdaBoost-Stump在人脸识别方面的应用：</p><p><img src="http://img.blog.csdn.net/20170717140200102?" alt="这里写图片描述"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了Adaptive Boosting。首先通过讲一个老师教小学生识别苹果的例子，来引入Boosting的思想，即把许多“弱弱”的hypotheses合并起来，变成很强的预测模型。然后重点介绍这种算法如何实现，关键在于每次迭代时，给予样本不同的系数u，宗旨是放大错误样本，缩小正确样本，得到不同的小矩g。并且在每次迭代时根据错误$\epsilon$值的大小，给予不同$g_t$不同的权重。最终由不同的$g_t$进行组合得到整体的预测模型G。实际证明，Adaptive Boosting能够得到有效的预测模型。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170713163950772?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>台湾大学林轩田机器学习技法课程学习笔记7 -- Blending and Bagging</title>
    <link href="https://redstonewill.github.io/2018/03/18/24/"/>
    <id>https://redstonewill.github.io/2018/03/18/24/</id>
    <published>2018-03-18T05:58:22.000Z</published>
    <updated>2018-03-18T06:00:20.499Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.blog.csdn.net/20170711084001106?imageView/2/w/500/q/100" alt="这里写图片描述"><br><a id="more"></a></p><blockquote><p>我的CSDN博客地址：<a href="http://blog.csdn.net/red_stone1" target="_blank" rel="noopener">红色石头的专栏</a><br>我的知乎主页：<a href="https://www.zhihu.com/people/red_stone_wl" target="_blank" rel="noopener">红色石头</a><br>我的微博：<a href="https://weibo.com/6479023696/profile?topnav=1&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">RedstoneWill的微博</a><br>我的GitHub：<a href="https://github.com/RedstoneWill" target="_blank" rel="noopener">RedstoneWill的GitHub</a><br>我的微信公众号：红色石头的机器学习之路（ID：redstonewill）<br>欢迎大家关注我！共同学习，共同进步！</p></blockquote><p>上节课我们主要介绍了Support Vector Regression，将kernel model引入到regression中。首先，通过将ridge regression和representer theorem结合起来，得到kernel ridge regression。但是其解是dense的，即不部分不为零。为了得到sparse解，我们将regularized tube error和Lagrange dual结合起来，利用SVM dual的推导方法，得到support vector regression的sparse解。本系列1-6节课主要介绍Kernel Models及其应用，从本节课开始，讲介绍Aggregation Models，即如何将不同的hypothesis和features结合起来，让模型更好。本节课将介绍其中的两个方法，一个是Blending，一个是Bagging。</p><h3 id="Motivation-of-Aggregation"><a href="#Motivation-of-Aggregation" class="headerlink" title="Motivation of Aggregation"></a>Motivation of Aggregation</h3><p>首先举个例子来说明为什么要使用Aggregation。假如你有T个朋友，每个朋友向你预测推荐明天某支股票会涨还是会跌，对应的建议分别是$g_1,g_2,\cdots,g_T$，那么你该选择哪个朋友的建议呢？即最终选择对股票预测的$g_t(x)$是什么样的？</p><p>第一种方法是从T个朋友中选择一个最受信任，对股票预测能力最强的人，直接听从他的建议就好。这是一种普遍的做法，对应的就是validation思想，即选择犯错误最小的模型。第二种方法，如果每个朋友在股票预测方面都是比较厉害的，都有各自的专长，那么就同时考虑T个朋友的建议，将所有结果做个投票，一人一票，最终决定出对该支股票的预测。这种方法对应的是uniformly思想。第三种方法，如果每个朋友水平不一，有的比较厉害，投票比重应该更大一些，有的比较差，投票比重应该更小一些。那么，仍然对T个朋友进行投票，只是每个人的投票权重不同。这种方法对应的是non-uniformly的思想。第四种方法与第三种方法类似，但是权重不是固定的，根据不同的条件，给予不同的权重。比如如果是传统行业的股票，那么给这方面比较厉害的朋友较高的投票权重，如果是服务行业，那么就给这方面比较厉害的朋友较高的投票权重。以上所述的这四种方法都是将不同人不同意见融合起来的方式，接下来我们就要讨论如何将这些做法对应到机器学习中去。Aggregation的思想与这个例子是类似的，即把多个hypothesis结合起来，得到更好的预测效果。</p><p><img src="http://img.blog.csdn.net/20170711084001106?" alt="这里写图片描述"></p><p>将刚刚举的例子的各种方法用数学化的语言和机器学习符号归纳表示出来，其中G(x)表示最终选择的模型。</p><p>第一种方法对应的模型：</p><p>$$G(x)=g_{t_*}(x)\ with\ t_*=argmin_{t\in{1,2,\cdots,T}}\ E_{val}(g_t^-)$$</p><p>第二种方法对应的模型：</p><p>$$G(x)=sign(\sum_{t=1}^T1\cdot g_t(x))$$</p><p>第三种方法对应的模型：</p><p>$$G(x)=sign(\sum_{t=1}^T\alpha_t\cdot g_t(x))\ with\ \alpha_t\geq0$$</p><p>第四种方法对应的模型：</p><p>$$G(x)=sign(\sum_{t=1}^Tq_t(x)\cdot g_t(x))\ with\ q_t(x)\geq0$$</p><p><img src="http://img.blog.csdn.net/20170711090151610?" alt="这里写图片描述"></p><p>注意这里提到的第一种方法是通过验证集来选择最佳模型，不能使用$E_{in}(g_t)$来代替$E_{val}(g_t^-)$。经过Validation，选择最小的$E_{val}$，保证$E_{out}$最小，从而将对应的模型作为最佳的选择。</p><p>但是第一种方法只是从众多可能的hypothesis中选择最好的模型，并不能发挥集体的智慧。而Aggregation的思想是博采众长，将可能的hypothesis优势集合起来，将集体智慧融合起来，使预测模型达到更好的效果。</p><p>下面先来看一个例子，通过这个例子说明为什么Aggregation能work得更好。</p><p><img src="http://img.blog.csdn.net/20170711133648721?" alt="这里写图片描述"></p><p>如上图所示，平面上分布着一些待分类的点。如果要求只能用一条水平的线或者垂直的线进行分类，那不论怎么选取直线，都达不到最佳的分类效果。这实际上就是上面介绍的第一种方法：validation。但是，如果可以使用集体智慧，比如一条水平线和两条垂直线组合而成的图中折线形式，就可以将所有的点完全分开，得到了最优化的预测模型。</p><p>这个例子表明，通过将不同的hypotheses均匀地结合起来，得到了比单一hypothesis更好的预测模型。这就是aggregation的优势所在，它提高了预测模型的power，起到了特征转换（feature transform）的效果。</p><p><img src="http://img.blog.csdn.net/20170711135405460?" alt="这里写图片描述"></p><p>我们再从另外一方面来看，同样是平面上分布着一些待分类的点，使用PLA算法，可以得到很多满足条件的分类线，如下图所示：</p><p><img src="http://img.blog.csdn.net/20170711140155940?" alt="这里写图片描述"></p><p>这无数条PLA选择出来的直线对应的hypothesis都是满足分类要求的。但是我们最想得到的分类直线是中间那条距离所有点都比较远的黑色直线，这与之前SVM目标是一致的。如果我们将所有可能的hypothesis结合起来，以投票的方式进行组合选择，最终会发现投票得到的分类线就是中间和黑色那条。这从哲学的角度来说，就是对各种效果较好的可能性进行组合，得到的结果一般是中庸的、最合适的，即对应图中那条黑色直线。所以，aggregation也起到了正则化（regularization）的效果，让预测模型更具有代表性。</p><p><img src="http://img.blog.csdn.net/20170711141311004?" alt="这里写图片描述"></p><p>基于以上的两个例子，我们得到了aggregation的两个优势：feature transform和regularization。我们之前在机器学习基石课程中就介绍过，feature transform和regularization是对立的，还把它们分别比作踩油门和踩刹车。如果进行feature transform，那么regularization的效果通常很差，反之亦然。也就是说，单一模型通常只能倾向于feature transform和regularization之一，在两者之间做个权衡。但是aggregation却能将feature transform和regularization各自的优势结合起来，好比把油门和刹车都控制得很好，从而得到不错的预测模型。</p><h3 id="Uniform-Blending"><a href="#Uniform-Blending" class="headerlink" title="Uniform Blending"></a>Uniform Blending</h3><p>那对于我们已经选择的性能较好的一些矩$g_t$，如何将它们进行整合、合并，来得到最佳的预测模型呢？这个过程称为blending。</p><p>最常用的一种方法是uniform blending，应用于classification分类问题，做法是将每一个可能的矩赋予权重1，进行投票，得到的G(x)表示为：</p><p>$$g(x)=sign(\sum_{t=1}^T1\cdot g_t(x)$$</p><p>这种方法对应三种情况：第一种情况是每个候选的矩$g_t$都完全一样，这跟选其中任意一个$g_t$效果相同；第二种情况是每个候选的矩$g_t$都有一些差别，这是最常遇到的，大都可以通过投票的形式使多数意见修正少数意见，从而得到很好的模型，如下图所示；第三种情况是多分类问题，选择投票数最多的那一类即可。</p><p><img src="http://img.blog.csdn.net/20170711211033149?" alt="这里写图片描述"></p><p>如果是regression回归问题，uniform blending的做法很简单，就是将所有的矩$g_t$求平均值：</p><p>$$G(x)=\frac1T\sum_{t=1}^Tg_t(x)$$</p><p>uniform blending for regression对应两种情况：第一种情况是每个候选的矩$g_t$都完全一样，这跟选其中任意一个$g_t$效果相同；第二种情况是每个候选的矩$g_t$都有一些差别，有的$g_t&gt;f(x)$，有的$g_t&lt;f(x)$，此时求平均值的操作可能会消去这种大于和小于的影响，从而得到更好的回归模型。因此，从直觉上来说，求平均值的操作更加稳定，更加准确。</p><p><img src="http://img.blog.csdn.net/20170711212419704?" alt="这里写图片描述"></p><p>对于uniform blending，一般要求每个候选的矩$g_t$都有一些差别。这样，通过不同矩$g_t$的组合和集体智慧，都能得到比单一矩$g_t$更好的模型。</p><p>刚才我们提到了uniform blending for regression中，计算$g_t$的平均值可能比单一的$g_t$更稳定，更准确。下面进行简单的推导和证明。</p><p><img src="http://img.blog.csdn.net/20170711214143674?" alt="这里写图片描述"></p><p>推导过程中注意$G(t)=avg(g_t)$。经过推导，我们发现$avg((g_t(x)-f(x))^2)$与$(G-f)^2$之间差了$avg((g_t-G)^2)$项，且是大于零的。从而得到$g_t$与目标函数f的差值要比G与f的差值大。</p><p>刚才是对单一的x进行证明，如果从期望角度，对整个x分布进行上述公式的整理，得到：</p><p><img src="http://img.blog.csdn.net/20170711215426911?" alt="这里写图片描述"></p><p>从结果上来看，$avg(E_{out}(g_t))\geq E_{out}(G)$，从而证明了从平均上来说，计算$g_t$的平均值G(t)要比单一的$g_t$更接近目标函数f，regression效果更好。</p><p>我们已经知道G是数目为T的$g_t$的平均值。令包含N个数据的样本D独立同分布于$P^N$，每次从新的$D_t$中学习得到新的$g_t$，在对$g_t$求平均得到G，当做无限多次，即T趋向于无穷大的时候：</p><p>$$\overline{g}=lim_{T\rightarrow \infty}\ G=lim_{T\rightarrow \infty}\ \frac1T\sum_{t=1}^Tg_t=\epsilon_DA(D)$$</p><p><img src="http://img.blog.csdn.net/20170712080425995?" alt="这里写图片描述"></p><p>当T趋于无穷大的时候，$G=\overline{g}$，则有如下等式成立：</p><p><img src="http://img.blog.csdn.net/20170712080756020?" alt="这里写图片描述"></p><p>上述等式中左边表示演算法误差的期望值；右边第二项表示不同$g_t$的平均误差共识，用偏差bias表示；右边第一项表示不同$g_t$与共识的差距是多少，反映$g_t$之间的偏差，用方差variance表示。也就是说，一个演算法的平均表现可以被拆成两项，一个是所有$g_t$的共识，一个是不同$g_t$之间的差距是多少，即bias和variance。而uniform blending的操作时求平均的过程，这样就削减弱化了上式第一项variance的值，从而演算法的表现就更好了，能得到更加稳定的表现。</p><h3 id="Linear-and-Any-Blending"><a href="#Linear-and-Any-Blending" class="headerlink" title="Linear and Any Blending"></a>Linear and Any Blending</h3><p>上一部分讲的是uniform blending，即每个$g_t$所占的权重都是1，求平均的思想。下面我们将介绍linear blending，每个$g_t$赋予的权重$\alpha_t$并不相同，其中$\alpha_t\geq0$。我们最终得到的预测结果等于所有$g_t$的线性组合。</p><p><img src="http://img.blog.csdn.net/20170712082617337?" alt="这里写图片描述"></p><p>如何确定$\alpha_t$的值，方法是利用误差最小化的思想，找出最佳的$\alpha_t$，使$E_{in}(\alpha)$取最小值。例如对于linear blending for regression，$E_{in}(\alpha)$可以写成下图左边形式，其中$\alpha_t$是带求解参数，$g_t(x_n)$是每个矩得到的预测值，由已知矩得到。这种形式很类似于下图右边的形式，即加上特征转换$\phi_i(x_n)$的linear regression模型。两个式子中的$g_t(x_n)$对应于$\phi_i(x_n)$，唯一不同的就是linear blending for regression中$\alpha_t\geq0$，而linear regression中$w_i$没有限制。</p><p><img src="http://img.blog.csdn.net/20170712085402079?" alt="这里写图片描述"></p><p>这种求解$\alpha_t$的方法就像是使用two-level learning，类似于我们之前介绍的probabilistic SVM。这里，我们先计算$g_t(x_n)$，再进行linear regression得到$\alpha_t$值。总的来说，linear blending由三个部分组成：LinModel，hypotheses as transform，constraints。其中值得注意的一点就是，计算过程中可以把$g_t$当成feature transform，求解过程就跟之前没有什么不同，除了$\alpha\geq0$的条件限制。</p><p><img src="http://img.blog.csdn.net/20170712134910640?" alt="这里写图片描述"></p><p>我们来看一下linear blending中的constraint $\alpha_t\geq0$。这个条件是否一定要成立呢？如果$\alpha_t&lt;0$，会带来什么后果呢？其实$\alpha_t&lt;0$并不会影响分类效果，只需要将正类看成负类，负类当成正类即可。例如分类问题，判断该点是正类对应的$\alpha_t&lt;0$，则它就表示该点是负类，且对应的$-\alpha_t&gt;0$。如果我们说这个样本是正类的概率是-99%，意思也就是说该样本是负类的概率是99%。$\alpha_t\geq0$和$\alpha_t&lt;0$的效果是等同的一致的。所以，我们可以把$\alpha_t\geq0$这个条件舍去，这样linear blending就可以使用常规方法求解。</p><p><img src="http://img.blog.csdn.net/20170712141217975?" alt="这里写图片描述"></p><p>Linear Blending中使用的$g_t$是通过模型选择而得到的，利用validation，从$D_{train}$中得到$g_1^-,g_2^-,\cdots,g_T^-$。然后将$D_{train}$中每个数据点经过各个矩的计算得到的值，代入到相应的linear blending计算公式中，迭代优化得到对应$\alpha$值。最终，再利用所有样本数据，得到新的$g_t$代替$g_t^-$，则G(t)就是$g_t$的线性组合而不是$g_t^-$，系数是$\alpha_t$。</p><p><img src="http://img.blog.csdn.net/20170712144039981?" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20170712144146977?" alt="这里写图片描述"></p><p>除了linear blending之外，还可以使用任意形式的blending。linear blending中，G(t)是g(t)的线性组合；any blending中，G(t)可以是g(t)的任何函数形式（非线性）。这种形式的blending也叫做Stacking。any blending的优点是模型复杂度提高，更容易获得更好的预测模型；缺点是复杂模型也容易带来过拟合的危险。所以，在使用any blending的过程中要时刻注意避免过拟合发生，通过采用regularization的方法，让模型具有更好的泛化能力。</p><h3 id="Bagging-Bootstrap-Aggregation"><a href="#Bagging-Bootstrap-Aggregation" class="headerlink" title="Bagging(Bootstrap Aggregation)"></a>Bagging(Bootstrap Aggregation)</h3><p>总结一些上面讲的内容，blending的做法就是将已经得到的矩$g_t$进行aggregate的操作。具体的aggregation形式包括：uniform，non-uniforn和conditional。</p><p><img src="http://img.blog.csdn.net/20170712150343056?" alt="这里写图片描述"></p><p>现在考虑一个问题：如何得到不同的$g_t$呢？可以选取不同模型H；可以设置不同的参数，例如$\eta$、迭代次数n等；可以由算法的随机性得到，例如PLA、随机种子等；可以选择不同的数据样本等。这些方法都可能得到不同的$g_t$。</p><p><img src="http://img.blog.csdn.net/20170712151234676?" alt="这里写图片描述"></p><p>那如何利用已有的一份数据集来构造出不同的$g_t$呢？首先，我们回顾一下之前介绍的bias-variance，即一个演算法的平均表现可以被拆成两项，一个是所有$g_t$的共识（bias），一个是不同$g_t$之间的差距是多少（variance）。其中每个$g_t$都是需要新的数据集的。只有一份数据集的情况下，如何构造新的数据集？</p><p><img src="http://img.blog.csdn.net/20170712151854730?" alt="这里写图片描述"></p><p>其中，$\overline{g}$是在矩个数T趋向于无穷大的时候，不同的$g_t$计算平均得到的值。这里我们为了得到$\overline{g}$，做两个近似条件：</p><ul><li><p><strong>有限的T；</strong></p></li><li><p><strong>由已有数据集D构造出$D_t~P^N$，独立同分布</strong></p></li></ul><p>第一个条件没有问题，第二个近似条件的做法就是bootstrapping。bootstrapping是统计学的一个工具，思想就是从已有数据集D中模拟出其他类似的样本$D_t$。</p><p><img src="http://img.blog.csdn.net/20170712152918350?" alt="这里写图片描述"></p><p>bootstrapping的做法是，假设有N笔资料，先从中选出一个样本，再放回去，再选择一个样本，再放回去，共重复N次。这样我们就得到了一个新的N笔资料，这个新的$\breve{D_t}$中可能包含原D里的重复样本点，也可能没有原D里的某些样本，$\breve{D_t}$与D类似但又不完全相同。值得一提的是，抽取-放回的操作不一定非要是N，次数可以任意设定。例如原始样本有10000个，我们可以抽取-放回3000次，得到包含3000个样本的$\breve{D_t}$也是完全可以的。利用bootstrap进行aggragation的操作就被称为bagging。</p><p><img src="http://img.blog.csdn.net/20170712154142461?" alt="这里写图片描述"></p><p>下面举个实际中Bagging Pocket算法的例子。如下图所示，先通过bootstrapping得到25个不同样本集，再使用pocket算法得到25个不同的$g_t$，每个pocket算法迭代1000次。最后，再利用blending，将所有的$g_t$融合起来，得到最终的分类线，如图中黑线所示。可以看出，虽然bootstrapping会得到差别很大的分类线（灰线），但是经过blending后，得到的分类线效果是不错的，则bagging通常能得到最佳的分类模型。</p><p><img src="http://img.blog.csdn.net/20170712155019738?" alt="这里写图片描述"></p><p>值得注意的是，只有当演算法对数据样本分布比较敏感的情况下，才有比较好的表现。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了blending和bagging的方法，它们都属于aggregation，即将不同的$g_t$合并起来，利用集体的智慧得到更加优化的G(t)。Blending通常分为三种情况：Uniform Blending，Linear Blending和Any Blending。其中，uniform blending采样最简单的“一人一票”的方法，linear blending和any blending都采用标准的two-level learning方法，类似于特征转换的操作，来得到不同$g_t$的线性组合或非线性组合。最后，我们介绍了如何利用bagging（bootstrap aggregation），从已有数据集D中模拟出其他类似的样本$D_t$，而得到不同的$g_t$，再合并起来，优化预测模型。</p><p><strong><em>注明：</em></strong></p><p>文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170711084001106?imageView/2/w/500/q/100&quot; alt=&quot;这里写图片描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田机器学习技法" scheme="https://redstonewill.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://redstonewill.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="林轩田" scheme="https://redstonewill.github.io/tags/%E6%9E%97%E8%BD%A9%E7%94%B0/"/>
    
      <category term="笔记" scheme="https://redstonewill.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="技法" scheme="https://redstonewill.github.io/tags/%E6%8A%80%E6%B3%95/"/>
    
  </entry>
  
</feed>
